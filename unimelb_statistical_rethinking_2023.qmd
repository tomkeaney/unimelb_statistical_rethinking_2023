---
title: "Statistical rethinking 2023 (brms examples)"
subtitle: "Melbourne biosciences notes"
---

# Load the packages required to run code

```{r}
library(tidyverse) # for tidy coding
library(brms) # for fitting stan models
library(patchwork) # for combining plots
library(ggdag) # drawing dags
library(tidybayes) # for bayesian data visualisation
library(bayesplot) # more bayes data vis
library(MetBrewer) # colours 
library(ggrepel) # for nice text on ggplots
library(loo) # for information criteria
```


$~$

# Lecture 1: The Golem of Prague
* * * *

$~$

**Causal inference**: the attempt to understand the scientific causal model using data.

- It is prediction - _Knowing a cause means being able to predict the consequences of an intervention_

- It is a kind of imputation - _Knowing a cause means being able to construct unobserved counterfactual outcomes_

- _What if I do this_ types of questions.

Causal inference is intimately related to the **description of populations** and the **design of research questions**. This is because all three depend upon causal models/knowledge.

Describing populations depends upon causal inference because nealry always description depends upon the sample of the population wich you are using to predict population charactetiscs.

$~$

## DAGs (Directed acyclic graphs)

$~$

- Heuristic causal models that clarify scientific thinking.

- We can use these to produce appropriate statistical models.

- These help us think about the science before we think about the data.

- Helps with questions like "what variables should we include in an analysis?"

- An integral part of the course that will come up over and over again.

An example:

_Lets make a function to speed up the DAG making process. We'll use this a lot_

```{r}

# Lets make a function to speed up the DAG making process. We'll use this a lot

gg_simple_dag <- function(d) {
  
  d %>% 
    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_dag_point(color = met.brewer("Hiroshige")[4]) +
    geom_dag_text(color = met.brewer("Hiroshige")[7]) +
    geom_dag_edges() + 
    theme_dag()
}

dagify( X ~ A + C,
        C ~ A + B,
        Y ~ X + C + B,  
        coords = tibble(name = c("A", "C", "B", "X", "Y"),
                         x = c(1, 2, 3, 1, 3),
                         y = c(2, 2, 2, 1, 1))) %>% 
  gg_simple_dag()
```

$~$

## Golems

$~$

Statistical models are akin to Golems - clay robots brought to life by magic to follow specific tasks. The trouble is, they follow tasks extremely literally and are blind to their creators intent, so even those born out of the purest of intentions can cause great harm. Models are another form of robot; if we apply models within the wrong context they will not help us at all.

Ecological models are rarely designed to falsify null-hypotheses. This is because there are many possible non-null models or put another way there are no true null models in many systems. This is problematic because null models underlie how the majority of biologists do statistics!

A more appropriate question is to ask how multiple process models that we can identify are different.

> Should falsify the explanatory model, not the null model. Make predictions and try and falify those. _Karl Popper_

$~$

## Owls

$~$

Satistical modelling explanations often provide a brief introduction, then leave out many of the important details e.g. step 1: draw two circles, step 2: draw the rest of the fking owl.

**We shall have a specific workflow for fitting models that we will document**

Drawing the owl, or the scientific workflow can be broken down into 5 steps:

1. Theoretical estimand: what are you trying to do in your study in the first place?

2. Some scientific causal model should be identified: step 1 will be precisely defined in the context of a causal model.

3. Use 1 and 2 to build a statistical model.

4. Simulate the scientific casual model to validate that the statistical model from step 3 yields the theoretical estimand i.e. check that our stats model works.

5. Analyse the real data. Note that the real data are only introduced now.

$~$

# Lecture 2: The garden of forking data
* * * *

$~$

## Globe tossing

To estimate the proportion of water on planet earth, we collect data by tossing a globe 10 times and record whether our left index finger lands on land or water.

Our results are shown in the code below

```{r}
globe_toss_data <- tibble(toss = c("l", "w", "l", "l", "w", "w", "w", "l", "w", "w")) 

globe_toss_data
```

Remember the owl workflow:

1. Define generative model of sample.

2. Design estimand.

3. Use 1 and 2 to build a statistical model.

4. Test (3) using (1).

5. Analyse sample and summarise.

$~$

**Step 1**

Some true proportion of water, $p$.

We can measure this indirectly using:

- $N$: the number of globe tosses

- $W$: the number of water observations

- $L$: the number of land observations

```{r}
#Put dag here

#N affects L and W but not the other way around

# P also influences W and L
```

**Bayesian data analysis**

For each possible explanation of the data, count all the ways data can happen. Explanations with more ways to produce the data are more plausible (this is entropy).

Toss globe, probability $p$ of observing $W$, $1 - p$ of $L$.

Each toss is random because it is chaotic (there are forces that could be theoretically measured i.e. velocity, exact starting orientation, but we are not equipped to measure these in real time, so the process appears random).

Each toss is independent of one another.

What are the relative number of ways we could get the data we actually got, given the process that generates all the possible datasets that could've been born from the process?

$~$

### A 4 sided globe (dice)

In Bayesian analysis: enumerate all possible outcomes. 


```{r}
# code land as 0 and water as 1 and create possibility data
# create the dataframe (tibble)

d <- tibble(p_1 = 0,
            p_2 = rep(1:0, times = c(1, 3)),
            p_3 = rep(1:0, times = c(2, 2)),
            p_4 = rep(1:0, times = c(3, 1)),
            p_5 = 1)

d %>% 
  gather() %>% 
  mutate(x = rep(1:4, times = 5),
         possibility = rep(1:5, each = 4)) %>% 
  
  ggplot(aes(x = x, y = possibility, 
             fill = value %>% as.character())) +
  geom_point(shape = 21, size = 9) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(.75, 4.25),
                  ylim = c(.75, 5.25)) +
  theme_minimal() +
  theme(legend.position = "none",
        text = element_text(size = 18))    

```


The data: the dice is rolled three times: water, land, water.

Consider all possible answers and how they would occur.

e.g. if we hypothesise that 25% of the earth is covered by water, what are all the possible ways to produce our sample?

```{r}

# create a tibble of all the possibilities per marble draw, with columns position (where to appear on later figures x axis), draw (what number draw is it? and how many for each number? where to appear on figures y axis) and fill (colour of ball for figure)

d <- tibble(position = c((1:4^1) / 4^0, 
                         (1:4^2) / 4^1, 
                         (1:4^3) / 4^2),
            draw     = rep(1:3, times = c(4^1, 4^2, 4^3)),
            fill     = rep(c("b", "w"), times = c(1, 3)) %>% 
              rep(., times = c(4^0 + 4^1 + 4^2)))

# we wish to make a path diagram, for which we will need connecting lines, create two more tibbles for these

lines_1 <- tibble(x    = rep((1:4), each = 4),
                  xend = ((1:4^2) / 4),
                  y    = 1,
                  yend = 2)

lines_2 <- tibble(x    = rep(((1:4^2) / 4), each = 4),
                  xend = (1:4^3) / (4^2),
                  y    = 2,
                  yend = 3)

# We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that.

d <- d %>% 
  mutate(denominator = ifelse(draw == 1, .5,
                              ifelse(draw == 2, .5 / 4,
                                     .5 / 4^2))) %>% 
  mutate(position    = position - denominator)

lines_1 <- lines_1 %>% 
  mutate(x    = x - .5,
         xend = xend - .5 / 4^1)

lines_2 <- lines_2 %>% 
  mutate(x    = x - .5 / 4^1,
         xend = xend - .5 / 4^2)

# create the plot, using geom_segment to add the lines - note coord_polar() which gives th eplot a globe-like effect. scale_x_continuous and the y equivalent have been used to remove axis lables and titles

d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data  = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               size  = 1/3) +
  geom_segment(data  = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend),
               size  = 1/3) +
  geom_point(aes(fill = fill),
             shape = 21, size = 4) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +
  theme_minimal() +
  theme(panel.grid      = element_blank(),
        legend.position = "none") +
  coord_polar()

```


Prune the number of possible outcomes down to those that are consistent with the data.

e.g. there are 3 paths consistent with our dice rolling experiment for the given data and the given hypothesis.

```{r}
lines_1 <-
  lines_1 %>% 
  mutate(remain = c(rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4 * 3)))

lines_2 <-
  lines_2 %>% 
  mutate(remain = c(rep(0,   times = 4),
                    rep(1:0, times = c(1, 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 12 * 4)))

d <-
  d %>% 
  mutate(remain = c(rep(1:0, times = c(1, 3)),
                    rep(0:1, times = c(1, 3)),
                    rep(0,   times = 4 * 4),
                    rep(1:0, times = c(1, 3)) %>% 
                      rep(., times = 3),
                    rep(0,   times = 12 * 4))) 

# finally, the plot:
d %>% 
  ggplot(aes(x = position, y = draw)) +
  geom_segment(data  = lines_1,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               size  = 1/3) +
  geom_segment(data  = lines_2,
               aes(x = x, xend = xend,
                   y = y, yend = yend,
                   alpha = remain %>% as.character()),
               size  = 1/3) +
  geom_point(aes(fill = fill, alpha = remain %>% as.character()),
             shape = 21, size = 4) +
  # it's the alpha parameter that makes elements semitransparent
  scale_alpha_manual(values = c(1/10, 1)) +
  scale_fill_manual(values  = c("navy", "white")) +
  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +
  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +
  theme_minimal() +
  theme(panel.grid      = element_blank(),
        legend.position = "none") +
  coord_polar()
```


Is 3 ways to produce the sample big or small, to find out, compare with other possibilities.

e.g. lets make another conjecture - all land or all water - we have a land and water observation so there are zero ways that the all land/water hypotheses are consistent with the data.


```{r}
# if we make two custom functions, here, it will simplify the code within `mutate()`, below
n_water <- function(x){
  rowSums(x == "W")
}

n_land <- function(x){
  rowSums(x == "L")
}

t <-
  # for the first four columns, `p_` indexes position
  tibble(p_1 = rep(c("L", "W"), times = c(1, 4)),
         p_2 = rep(c("L", "W"), times = c(2, 3)),
         p_3 = rep(c("L", "W"), times = c(3, 2)),
         p_4 = rep(c("L", "W"), times = c(4, 1))) %>% 
  mutate(`roll 1: water`  = n_water(.),
         `roll 2: land` = n_land(.),
         `roll 3: water`  = n_water(.)) %>% 
  mutate(`ways to produce` = `roll 1: water` * `roll 2: land` * `roll 3: water`)

t %>% 
  knitr::kable()
```

Unglamorous basis of applied probability: _Things that can happen more ways are more plausible._

This is Bayesian inference. Given a set of assumptions (hypotheses) the number of ways the numbers could have occurred accoridng to those assumptions (hypotheses) is the posterior probability distribution.

But we don't really have enough evidence to be confident in a prediction. So lets roll the dice again. In bayes world a process called **Bayesian updating** exists, that saves you from running the draws again, in favour of just updating previous counts (or data). Bayesian updating is simple multiplication e.g. we roll another water, for the 3 water hypothesis there are 3 paths for this to occur so we multiply 9 x 3, resulting in 27 possible paths consistent with the new dataset.

Eventually though, the garden gets really big. This is where your computer comes in and it starts to make more sense to work with probabilities rather than counts.

```{r}

W <- sum(globe_toss_data == "w")
L <- sum(globe_toss_data == "l")
p <- c(0, 0.25, 0.5, 0.75, 1) # proportions W
ways <- sapply(p, function(q) (q*4)^W * ((1 - q)*4)^L)
prob <- ways/sum(ways)

posterior <- cbind(p, ways, prob) %>% 
  as_tibble() %>% 
  mutate(p = as.character(p))

posterior %>% 
ggplot(aes(x = p, y = prob)) +
  geom_col() +
  labs(x = "proportion water", y = "probability") +
  theme_minimal()
```

$~$

### Test before you est

1. Code a generative simulation

2. Code an estimator

3. Test (2) with (1)

Build a simulation

```{r}
sim_globe <- function(p = 0.7, N =9) {
  sample(c("W", "L"), size = N, prob = c(p, 1-p), replace = TRUE)
}

# W and L are the possible observations

# N is number of tosses

# prob is the probabilities of water and land occurring
```

The simulation does this:

```{r}
sim_globe()
```

Now lets test it on extreme settings

e.g. all water

```{r}

sim_globe(p=1, N = 11)
    
```

Looks good

We can also test how close the proportion of water produced in the simulation is to the specified p

```{r}
sum(sim_globe(p=0.5, N = 1e4) == "W") / 1e4
```

Also looks good.

So based upon our generative model:

Ways for $p$ to produce $W, L = (4p)^W * (4 - 4p)^L $

```{r}
# function to compute posterior distirbution

compute_posterior <- function(the_sample, poss = c(0, 0.25, 0.5, 0.75, 1)) {
  W <- sum(the_sample == "W") # the number of W observed
  L <- sum(the_sample == "L") # the number of L observed
  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)
  post <- ways/sum(ways)
  #bars <- sapply(post, function(q) make_bar(q))
  tibble(poss, ways, post = round(post, 3))#, bars)
}

```

We can then simulate the experiment many times with `sim_globe`

```{r}
compute_posterior(sim_globe())
```

This allows us to check that our model is doing what we think it is.

$~$

## An infinite number of possibilties

Globes aren't dice. There are an infinite number of possible proportions of the earth covered in water.

To get actual infinity we can turn to math.

The relative number of ways that nay value of $p$ could produce any sample of W and L observations. This is a well know distribution called the **binomial distribution** 

$$p^W(1-p)^L $$

The only trick is normalising to make it a probability. A little calculus is needed (this produces the **beta distirbution**:

$$Pr(W, L|P) = \frac{(W + L)!}{W!L!}p^{W}(1 - p)^{L}$$

Note the normalising constant $\frac{(W + L)!}{W!L!}$

We can use the binomial sampling formula to give us the number of paths through the garden of forking data for this particular problem. That is given some value of P (akin to some number of blue marbles in the bag), the number of ways to see W and L can be worked out using the expression above.

We can use R to calculate this, assuming 6 globe tosses that landed on water out of 10 possible tosses and that P = 0.7:

```{r}
dbinom(6, 10, 0.7)
```

This is the relative number of ways that 6 out of 10 can happen given a value of 0.7 for p. For this to be meaningful, we need to work out a probability value for many other values of P. This gives our probability distribution.

Lets plot how bayesian updating works, as we add observations

```{r}
# add the cumulative number of trials and successes for water to the dataframe.

globe_toss_data <- globe_toss_data %>% mutate(n_trials = 1:10, 
                                              n_success = cumsum(toss == "w"))

# Struggling to follow this code, awesome figure produced though

sequence_length <- 50

globe_toss_data %>%
  expand(nesting(n_trials, toss, n_success),
         p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>%
  group_by(p_water) %>%
  mutate(lagged_n_trials = lag(n_trials, k = 1),
         lagged_n_success = lag(n_success, k =1)) %>%
  ungroup() %>%
  mutate(prior = ifelse(n_trials == 1, .5,
                        dbinom(x = lagged_n_success,
                               size = lagged_n_trials,
                               prob = p_water)),
         likelihood = dbinom(x = n_success,
                             size = n_trials,
                             prob = p_water),
         strip = str_c("n = ", n_trials)
         ) %>%
  # the next three lines allow us to normalize the prior and the likelihood, 
  # putting them both in a probability metric
  group_by(n_trials) %>%
  mutate(prior = prior / sum(prior),
         likelihood = likelihood / sum(likelihood)) %>%
  # plot time
  ggplot(aes(x = p_water)) +
  geom_line(aes(y = prior), linetype = 2) +
  geom_line(aes(y = likelihood)) +
  scale_x_continuous("proportion water", breaks = c(0, 0.5, 1)) +
  scale_y_continuous("plausibility", breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~n_trials, scales = "free_y")
```

Posterior is continually updated as data points are added.

- Each posterior is simply a multiplication of a set of diagonal lines, like that shown in the first panel. Whether the slope of the line is pos or neg depends on whether the observation is land or water. 

- Every posterior is a prior for the next observation, but the data order doesn't make a difference, in that the posterior will always be the same for a given dataset. **BUT** this assumption can be violated if each observation is not independent.

- Sample size is embodied within the shape of the posterior. Already been accounted for.

**Some big things to grasp about bayes**

1. No minimum sample size, because we have something called a **prior**. Note that estimation sucks with small samples, but that's good! The model doesn't draw too much from too little.

2. Shape of distribution embodies sample size

3. There are no point estimates e.g. means or medians. Everything is a distribution. You will never need to bootstrap again.

4. There is no one true interval - they just communicate the shape of the posterior distribution. No magical number e.g. 95%.

$~$

## From posterior to prediction (brms time)

Here is a nice point to introduce `brms` the main modelling package that I use.

Let's fit a globe tossing model, where we observe 24 water observations from 36 tosses.

```{r}
b1.1 <-
  brm(data = list(w = 24), 
      family = binomial(link = "identity"),
      w | trials(36) ~ 0 + Intercept,
      #prior(beta(1, 1), class = b, lb = 0, ub = 1),
      seed = 2,
      file = "fits/b02.01")

b1.1
```

There’s a lot going on in that output. For now, focus on the ‘Intercept’ line. The intercept of a typical regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial.

To use the posterior, we can sample from it using the `as_draws_df` function. 

```{r}
as_draws_df(b1.1) %>% 
  mutate(n = "n = 36") %>%
  
  ggplot(aes(x = b_Intercept)) +
  geom_density(fill = "black") +
  scale_x_continuous("proportion water", limits = c(0, 1)) +
  theme(panel.grid = element_blank(),
        text = element_text(size = 16)) +
  facet_wrap(~ n)
```


Another way to do this is to use the `fitted` function 

```{r}

f <-
  fitted(b1.1, 
         summary = F,
         scale = "linear") %>% 
  data.frame() %>% 
  set_names("proportion") %>% 
  as_tibble()

f
```

Plot the distribution

```{r}
f %>% 
  ggplot(aes(x = proportion)) +
  geom_density(fill = "grey50", color = "grey50") +
  annotate(geom = "text", x = .08, y = 2.5,
           label = "Posterior probability") +
  scale_x_continuous("probability of water",
                     breaks = c(0, .5, 1),
                     limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

Strikingly similar (well, exactly the same).

We can use this distribution of probabilities to predict histograms of water counts.

```{r}
# the simulation
set.seed(3) # so we get the same result every time

f <-
  f %>% 
  mutate(w = rbinom(n(), size = 36,  prob = proportion))

# the plot
f %>% 
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "grey92", size = 1/10) +
  scale_x_continuous("number of water samples", breaks = 0:40 * 4) +
  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 450)) +
  ggtitle("Posterior predictive distribution") +
  coord_cartesian(xlim = c(8, 36)) +
  theme_minimal()
```

$~$

# Lecture 3: Geocentric models
* * * * 

$~$

It is entirely possible for a completely false model to make very accurate predictions. 

Geocentrism is Mcelreath's example: the idea that the planets orbit the earth and the model behind it can very accurately predict where Mars will be in the nights sky, including when it will be in retrograde (appearing to orbit in the opposite direction).

However, just because a model allows us to make accurate predictions does not mean it is correct. We now know that the planets orbit the sun, and this more correct model can also explain mar's apparent retrograde movement.

The relevance of this anecdote is that prediction without explanation is thwart with danger, but at the same time models like the geocentric can be very helpful.

**Linear regression** is similar.

- They are often descriptively accurate but mechanistically wrong.

- Few things in nature are perfectly linear as is assumed.

- They are useful but we must remember their limits

$~$

## What are linear models

$~$

- Simple statistical golems that model the mean and the variance of a variable. That's it.

- The mean is sum weighted sum of other variables. As those variables change, the mean and variance changes.

- Anovas, Ancovas, t-tests are all linear models.

**The normal distribution**

- Counts up all the ways the observations can happen given a set of assumptions.

- Given some mean and variance the normal distribution gives you the relative number of ways the data can appear.

The normal distribution is the norm because:

- It is very common in nature.

- It's easy to calculate 

- Very conservative assumptions (spreads probability out more than any other distribution, reducing the risk of mistake, at the expense of accuracy)

To understand why the normal distribution is so common consider a soccer pitch mid-line, and individuals flipping coins that dictate whether they should step to the left or the right.

Many individuals remain close to the line, while a few move towards the extremes. Simply, there are more ways to get a difference of 0 than there is any other result. This creates a bell curve and summarises processes to mean and variance. The path of one individual is shaded black in the figure below.

```{r}
# lets simulate this scenario

set.seed(4)

pos <-
  replicate(100, runif(16, -1, 1)) %>% # this is the sim
  as_tibble() %>%
  rbind(0, .) %>% # add a row of zeros above simulation results
  mutate(step = 0:16) %>% # creates a step column
  gather(key, value, -step) %>% # convert data to long format
  mutate(person = rep(1:100, each = 17)) %>% # person IDs added
  # the next lines allow us to make cumulative sums within each person
  group_by(person) %>%
  mutate(position = cumsum(value)) %>%
  ungroup() # allows more data manipulation

ggplot(data = pos,
       aes(x = step, y = position, group = person)) +
  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +
  geom_line(aes(colour = person < 2, alpha = person < 2)) +
  scale_colour_manual(values = c("skyblue3", "black")) +
  scale_alpha_manual(values = c(1/5, 1)) +
  scale_x_continuous("step number", breaks = c(0, 4, 8, 12, 16)) +
  theme_minimal() +
  theme(legend.position = "none")

```

The density plot for this scenario at step 16:

```{r}

# first find the sd

sd <-
  pos %>%
  filter(step == 16) %>%
           summarise(sd = sd(position))

pos %>%
  filter(step == 16) %>%
  ggplot(aes(x = position)) +
  stat_function(fun = dnorm,
                args = list(mean = 0, sd = 2.18),
                linetype = 2) +
  geom_density(colour = "transparent", fill = "dodgerblue", alpha = 0.5) +
  coord_cartesian(xlim = c(-6, 6)) +
  labs(title = "16 steps",
       y = "density")


```


Why normal? The generative perspective:

- Nature makes bell curves whenever it adds things together.

- This dampens fluctuations - large fluctuations are cancelled out by small fluctuations.

- Symmetry arises from the addition process.

Inferential perspective:

- If all you know are the mean and the variance, then the least surprising distribution is gaussian.

>Note: if you just want mean or variance, a variable does not have to be normally distributed for the normal distribution to be useful 

> Oh and go with the FLOW. This is hard, you won't understand everything, but keep flowing forward

$~$

## Weekly owl reminder

1. State a clear question

2. Sketch your causal assumptions

3. Use the sketch to define generative model

4. Use generative model to build estimator 

5. **Profit** (real model)

$~$

## Time for some data: Kalahari heights

$~$

To get us started with linear regression, lets load in the Kalahari dataset from McElreath's `rethinking` package.

```{r}
library(rethinking)
data(Howell1)
Kalahari_data <- as_tibble(Howell1)
```
 
Now lets detach `rethinking`, we need to do this so `brms` always works

```{r}
rm(Howell1)
detach(package:rethinking, unload = T)
library(brms)
```

Right lets have a quick look at the data

```{r}
Kalahari_data
```

And we can check out how each variable is distributed like so:

```{r}
Kalahari_data %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = c("height", "weight", "age", "male"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10) +
  facet_wrap(~ name, scales = "free", ncol = 1)
```

**The owl**

**1. In this lecture we are going to focus on describing the association between height and weight in adults.**

**2. Scientific model: how does height affect weight?**

Height influences weight, but not the other way around. Height is causal.

$W = f(H)$ this means that weight is some function of height.

```{r}
dagify( W ~ H + U,  
        coords = tibble(name = c("H", "W", "U"),
                         x = c(1, 2, 3),
                         y = c(1, 1, 1))) %>% 
  gg_simple_dag()
```

Note the $U$ in the DAG - an unobserved influence on weight.

### Build a generative model

$W = \beta H + U$

Lets simulate the data

```{r}
sim_weight <- function(H, b, sd){
  U <- rnorm(length(H), 0, sd)
  W <- b*H + U
  return(W)
}

```

Run the simulation and plot. We'll need values for heights, a $\beta$ value and some value of sd for weight in kgs 

```{r}
sim_data <- tibble(H = runif(200, min = 130, max = 170)) %>% 
  mutate(W = sim_weight(H, b = 0.5, sd = 5))

# plot

sim_data %>% 
  ggplot(aes(x = H, y = W)) +
   geom_point(color = "salmon", shape = 1.5, stroke =2.5, size = 3, alpha = 0.9) +
  theme_classic() +
  labs(x = "Height (cm)", y = "Weight (kgs)") +
  theme(text = element_text(size = 16))
```

**Describing our model**

$W_{i} = \beta H_{i} + U_i$ is our equation for expected weight

$U_{i} = Normal(0,\sigma)$ is the Gaussian error with sd $\sigma$

$H_{i} =$ Uniform$(130, 170)$ means that all values are equally likely for height between 130-170

$i$ is an index and here represents individuals in the dataset

`=` indicates a deterministic relationship

`~` indicates that something is "distributed as"

$~$

### Build estimator

A linear model:

$E(W_i|H_i) = \alpha + \beta H_{i}$

$\alpha$ =  the intercept

$\beta$ = the slope

Our estimator:

$W_{i}$ `~` $Normal(u_{i},\sigma)$

$u_{i}$ `~` $\alpha + \beta H_{i}$ 

In words: $W$ is distributed normally with mean that is a linear function of H

### Priors

We can specify these to be very helpful. We simply want to design priors to stop the model hallucinating impossible outcomes e.g. negative weights.

Priors should express scientific knowledge, but softly. This is because the real process in nature is different to what we imagined and there needs to be room for this.

Some basic things we know about weight:

1. When height is zero, weight should be zero. 

- $\alpha$ ~ Normal(0, 10) will achieve this

2. Weight increases with height in humans on average. So $\beta$ should be positive

3. Weight in kgs is less than height in cms, so $\beta$ should be less than 1

- $\beta$ ~ Uniform(0, 1) will achieve this

4. $\sigma$ must be positive

- $\sigma$ ~ Uniform(0, 10) 

Lets plot these priors

```{r}

n_lines <- 50

tibble(n = 1:n_lines,
       a = rnorm(n_lines, 0, 10),
       b = runif(n_lines, 0, 1)) %>% 
  expand(nesting(n, a, b), height = 130:170) %>% 
  mutate(weight = a + b * (height)) %>%
  
  # plot
  ggplot(aes(x = height, y = weight, group = n)) +
  geom_line(alpha = 1/1.5, linewidth = 1, colour = met.brewer("Hiroshige")[2]) +
  scale_x_continuous(expand = c(0, 0)) +
  coord_cartesian(ylim = c(0, 100),
                  xlim = c(130, 170)) +
  theme_classic()
```

Woah, ok the slope looks ok, but the intercept is wild.

This is because we set a very high sd value for $\alpha$

We can fix this, but for a problem like this, the data will overwhelm the prior.

$~$

### Back to the owl: brms time

Lets validate our model

Let simulate again with our `sim_weight()` function

```{r}
sim_data_100 <- 
  tibble(H = runif(100, min = 130, max = 170)) %>% 
  mutate(W = sim_weight(H, b = 0.5, sd = 5))
```

Fit the model

```{r}
weight_synthetic_model <-
  brm(W ~ 1 + H,
      family = gaussian,
      data = sim_data_100,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(uniform(0, 1), class = b, lb = 0, ub = 1),
                prior(uniform(0, 10), class = sigma, lb = 0, ub = 10)),
      chains = 4, cores = 4, iter = 6000, warmup = 2000, seed = 1,
      file = "fits/weight_synthetic_model")
```

Get the model summary

```{r}
weight_synthetic_model
```

Right so, H is close to 0.5 and sigma is very close to 4.91. These aren't perfect because we have a smallish sample and relatively wild priors.

$~$

**Step 5. Profit** (with the real data)

Time to fit the actual model

```{r}
Kalahari_adults <-
  Kalahari_data %>% 
  filter(age > 18)

weight_kalahari_model <-
  brm(weight ~ 1 + height,
      family = gaussian,
      data = Kalahari_adults,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(uniform(0, 1), class = b, lb = 0, ub = 1),
                prior(uniform(0, 10), class = sigma, lb = 0, ub = 10)),
      chains = 4, cores = 4, iter = 6000, warmup = 2000, seed = 1,
      file = "fits/weight_kalahari_model")

weight_kalahari_model

```

$~$

### OBEY THE LAW

$~$

Law 1. Parameters are not independent of one another and cannot always be interpreted independently. They all act simultaneously on predictors (think about this from the context of the `fitted()` function).

Instead we can push out posterior predictions from the model and describe/interpret those.

1. Plot the sample

2. Plot the posterior mean

3. Plot the uncertainty of the mean

4. Plot the uncertainty of predictions

```{r}
 
# use fitted to get posterior predictions

height_seq <- 
  tibble(height = 135:180) %>% 
  mutate(height_standard = height - mean(Kalahari_adults$height))

# add uncertainty intervals

mu_summary <-
  fitted(weight_kalahari_model,
         newdata = height_seq) %>%
  as_tibble() %>%
  bind_cols(height_seq)

# add prediction intervals

pred_weight <-
  predict(weight_kalahari_model,
          newdata = height_seq) %>%
  as_tibble() %>%
  bind_cols(height_seq)

# make plot 

Kalahari_adults %>%
  ggplot(aes(x = height)) +
   geom_ribbon(data = pred_weight, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "salmon", alpha = 0.5) +
  geom_point(aes(y = weight), color = "salmon", shape = 1.5, stroke =2, size = 1.5, alpha = 0.9) +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "salmon", color = "black", alpha = 0.7, size = 1/2) +
  coord_cartesian(xlim = range(Kalahari_adults$height)) +
  labs(x = "Height (cm)", y = "Weight (kgs)") +
  theme_classic() +
  theme(panel.grid = element_blank())

```

This plot shows 1) the raw data, 2) the predicted relationship between height and weight with 3) 95% uncertainty intervals for the mean and 4) 95% prediction intervals for where data points are predicted to fall within.

`Predict()` reports prediction intervals, which are simulations that are the joint consequence of both the mean and sigma, unlike the results of `fitted()`, which only reflect the mean.

$~$

# Lecture 4: Categories and Curves
* * * *

$~$

## Categories

Want to stratify by categories in our data. This means fitting a separate regression line for each category.

Lets return to the Kalahari data. Now we'll add in a categorical variable - the sex of the individual.

```{r}
Kalahari_adults %>%
  ggplot(aes(x = height, colour = as.factor(male))) +
  geom_point(aes(y = weight), shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +
  coord_cartesian(xlim = range(Kalahari_adults$height)) +
  scale_colour_manual(values = c("salmon", "darkcyan")) +
  labs(x = "Height (cm)", y = "Weight (kgs)") +
  theme_classic() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Think scientifically first:

How are height, sex and weight causally associated?

How are height, sex and weight statistically related?

Lets build a DAG:

- First, we know that height causally effects weight. You can change your own weight without changing your height. However, as you get taller there is more of you, thus it is reasonable to expect height to affect weight.

- We also know that sex influences height, but it is silly to say that height influences sex.

- Third, if there is any influence we expect sex to influence weight not the other way around. Therefore weight may be influenced by both height and sex.

Lets draw this DAG

```{r}
dagify(W ~ S + H,
       H ~ S,
       labels = c("W" = "Weight", 
                  "H" = "Height",
                  "S" = "Sex")) %>% 
  gg_simple_dag()
```

This is a mediation graph. Note that effects do not stop at one variable, instead they continue on through the path. In that way sex has both a direct and indirect effect on weight. The indirect effect may be through height, which is 'contaminated' by sex.

Statistically:

$H = f_{H}(S)$

$W = f_{W}(H, S)$

Following our workflow lets simulate a more complex dataset, that this time includes separate sexes.

To keep in line with the Kalahari data we'll code `females = 1` and `males = 2`

```{r}
sim_HW <- function(S, b, a){
  N <- length(S)
  H <- if_else(S == 1, 150, 160) + rnorm(N, 0, 5)
  W <- a[S] + b[S]*H + rnorm(N, 0, 5)
  tibble(S, H, W)
}
```

Give it some data and run `sim_HW()`

```{r}
S <- rethinking::rbern(100) + 1

(synthetic_data <- sim_HW(S, b = c(0.5, 0.6), a = c (0, 0)))
```

**Define the questions we're going to ask**:

Different questions lead to a need for different stats models.

Q: Causal effect of H on W?

Q: Causal effect of S on W?

Q: Direct effect of S on W?

Each require different components of the DAG.

**Drawing the categorical OWL**:

There are several ways to code categorical variables

1. Dummy or indicator variables
    
- Series of 0 1 variables that stand in for categories
    
2. Index variables

- Assign an index value to each category
    
- Better for specifying priors
    
- Extend effortlessly to multi-level models
    
- What we will use

$~$
    
**Q: What is the causal effect of S on W?**  

$~$
    
Using index variables

Estimating average weight:

$W_{i} = Normal(\mu_{i}, \sigma)$

$\mu_{i} = \alpha S[i]$ where $S[i]$ is the sex of the i-th person 

S = 1 indicates female, S = 2 indicates male

$\alpha = [\alpha_{i}, \alpha_{2}]$ this means there are two intercepts, one for each sex

**Priors**

$\alpha_{j} = Normal(60, 10)$

All this says is that there is more than one value of \alpha (indicated by the subscript j) and that we want the prior to be the same for each of the values. Values correspond to the categories.

We'll let the sample update the prior and tell us if sexual dimorphism exists

Right ready to model. Not yet! More simulation. This might seem like overkill but it will help so much to get into this habit.

We'll construct a female and male sample and look at the average difference. We'll only change sex.

We'll find the difference between the sexes in our simulation (remember we coded a stronger effect of height on male weight than on female weight):

```{r}
# female sample

S <- rep(1, 100)

simF <- sim_HW(S, b = c(0.5, 0.6), a = c(0, 0))

S <- rep(2, 100)

simM <- sim_HW(S, b = c(0.5, 0.6), a = c(0, 0))

# effect of Sex (male - female)

mean(simM$W - simF$W)

```

Ok a difference of ~21kgs

Now lets test the estimator.

Note that we have specified a 0 intercept. In `brms` this will result in an output that produces a separate intercept for each categorical variable.

```{r}
synthetic_kalahari_sex_model <-
  brm(data = synthetic_data,
      W ~ 0 + as.factor(S),
      prior = c(prior(normal(60, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 1,
      file = "fits/synthetic_kalahari_sex")

synthetic_kalahari_sex_model
```

Again we find a difference between the sexes ~21kgs. Looks like our model tests what we want to test.

Now lets use the real data

```{r}
# make the index variable to match McElreath's dataset

Kalahari_adults <-
  Kalahari_adults %>% 
  mutate(Sex = if_else(male == "1", 2, 1),
         Sex = as.factor(Sex))

kalahari_sex_model <-
  brm(data = Kalahari_adults,
      weight ~ 0 + Sex,
      prior = c(prior(normal(60, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 1,
      file = "fits/_kalahari_sex_model")
```

Find the contrasts and plot the average differences between women and men

```{r}
draws <- as_draws_df(kalahari_sex_model)

as_draws_df(kalahari_sex_model) %>% 
  mutate(`Mean weight contrast` = b_Sex2 - b_Sex1) %>% 
  rename(Female = b_Sex1,
         Male = b_Sex2) %>% 
  pivot_longer(cols = c("Female", "Male", "Mean weight contrast")) %>% 
  
  ggplot(aes(x = value, y = 0)) +
    stat_halfeye(.width = 0.95,
                 normalize = "panels") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("Posterior mean weights (kgs)") +
  theme_bw() +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free")
    
```

What about the posterior predictive distribution, not just the mean?

```{r}
Female_dist <- rnorm(1000, draws$b_Sex1, draws$sigma) %>% 
  as_tibble() %>% 
  rename(Female = value)

Male_dist <- rnorm(1000, draws$b_Sex2, draws$sigma) %>% 
  as_tibble() %>% 
  rename(Male = value)

plot_1 <-
  cbind(Female_dist, Male_dist) %>% 
  pivot_longer(cols = c("Female", "Male")) %>% 
    
  ggplot(aes(x = value, group = name, colour = name, fill = name)) +
  geom_density(alpha = 0.8) +
  scale_colour_manual(values = c("salmon", "darkcyan")) +
  scale_fill_manual(values = c("salmon", "darkcyan")) +
  xlab("Posterior predicted weights (kgs)") +
  ylab("Density") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
 

plot_2 <- 
cbind(Female_dist, Male_dist) %>%
  as_tibble() %>% 
  mutate(predictive_diff = Male - Female) %>% 
  
  ggplot(aes(x = predictive_diff, colour = "orange", fill = "orange")) +
  stat_slab(aes(fill = after_stat(x > 0), colour = after_stat(x > 0)), alpha = 0.8) +
  xlab("Posterior predictive weight difference (kgs)") +
  ylab("Density") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = "none")

plot_1 / plot_2
```

The takeaway here is that there are lots of women that are heavier than men, even though the difference between the means is large.

We need to calculate the contrast, or difference between the categories (as we have shown above).

- It is never legitimate to compare overlap in parameters

- What you do is compute the distribution of the difference (as shown above)

$~$

**Q: What is the direct causal effect of S on W?**

$~$

Now we must add Height into the model

$W_{i} = Normal(\mu_{i}, \sigma)$

$\mu_{i} = \alpha_{S[i]} + \beta_{S[i]}(H_{i} - \overline{H})$

Now we have two intercepts and two slopes!

**Centering**

- Centering H makes it so that $\alpha$ is the average weight of a person with average height

- Easy to fit priors for $alpha$

- Linear regressions build lines that pass through this point (the grand mean)

```{r}
Kalahari_adults <-
  Kalahari_adults %>%
   mutate(height_standard = height - mean(height))
```

Lets model

```{r}
Kalahari_h_S_model <-
  brm(data = Kalahari_adults,
      weight ~ 0 + Sex + height_standard,
      prior = c(#prior(normal(60, 10), class = a),
                prior(lognormal(0, 1), class = b, lb = 0),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 1,
      file = "fits/Kalahari_h_S_model")
```

I'm not sure how to make McElreath's plot here - he brilliantly finds the effect of sex at 50 different heights then plots. The best I can do is the overall effect of sex on weight after accounting for sex.

The takeaway however, is that nearly all of the causal effect of sex acts through height.

```{r}
draws_2 <- as_draws_df(Kalahari_h_S_model) %>% 
  mutate(weight_contrast = b_Sex2 - b_Sex1)

Female_dist_2 <- rnorm(1000, draws_2$b_Sex1, draws_2$sigma) %>% 
  as_tibble() %>% 
  rename(Female = value)

Male_dist_2 <- rnorm(1000, draws_2$b_Sex2, draws_2$sigma) %>% 
  as_tibble() %>% 
  rename(Male = value)

plot_1 <-
  cbind(Female_dist_2, Male_dist_2) %>% 
  pivot_longer(cols = c("Female", "Male")) %>% 
    
  ggplot(aes(x = value, group = name, colour = name, fill = name)) +
  geom_density(alpha = 0.8) +
  scale_colour_manual(values = c("salmon", "darkcyan")) +
  scale_fill_manual(values = c("salmon", "darkcyan")) +
  xlab("Posterior predicted weights (kgs)\nafter accounting for the effect of height") +
  ylab("Density") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
 

plot_2 <- 
cbind(Female_dist, Male_dist) %>%
  as_tibble() %>% 
  mutate(predictive_diff = Male - Female) %>% 
  
  ggplot(aes(x = predictive_diff, colour = "orange", fill = "orange")) +
  geom_density(alpha = 0.8) +
  xlab("Posterior predictive weight difference (kgs)\nafter accounting for the effect of height") +
  ylab("Density") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = "none")

plot_1 / plot_2
  
```

$~$

## Curves with lines

$~$

Many causal relationships are obviously not linear.

Linear models can fit curves quite easily, but be wary that this is geocentric (not mechanistic).

For example lets examine the full Kalahari dataset, which includes children.

```{r}
Kalahari_data %>%
  ggplot(aes(x = height)) +
  geom_point(aes(y = weight), colour = "salmon", shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +
  coord_cartesian(xlim = range(Kalahari_data$height)) +
  labs(x = "Height (cm)", y = "Weight (kgs)") +
  theme_classic() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

Two popular strategies

- Polynomials

- Splines and generalised additive models (nearly always better)

$~$

### Polynomials

$~$

$\mu_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x^2_{i}$

Note the $x^{2}$, you can use higher and higher order terms and the model becomes more flexible.

**Issues**

- There is unhelpful asyymetry to polynomials. For example, for a second order term, the model believes the data creates a parabola and it's very hard to convince it otherwise.

- The uncertainty at the edges of the data can be huge, making extrapolation and prediction very difficult.

- There is no local smoothing so any data point can massively change the shape of a curve.

For a second order term, the model believes the data creates a parabola and it's very hard to convince it otherwise.

Fit the models with second and third order terms


```{r}
# First it is worth standardising height
Kalahari_data <-
Kalahari_data %>%
  mutate(height_s = (height - mean(height)) / sd(height)) %>% 
  mutate(height_s2 = height_s^2,
         height_s3 = height_s^3)

# fit quadratic model

quadratic_kalahari <- 
  brm(data = Kalahari_data, 
      family = gaussian,
      weight ~ 1 + height_s + height_s2,
      prior = c(prior(normal(60, 10), class = Intercept),
                prior(lognormal(0, 1), class = b, coef = "height_s"),
                prior(normal(0, 1), class = b, coef = "height_s2"),
                prior(uniform(0, 50), class = sigma)),
      iter = 30000, warmup = 29000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/quadratic_kalahari")

# get predictions

height_seq <- 
  tibble(height_s = seq(from = -4, to = 4, length.out = 30)) %>% 
  mutate(height_s2 = height_s^2)

fitd_quad <-
  fitted(quadratic_kalahari, 
         newdata = height_seq) %>%
  data.frame() %>%
  bind_cols(height_seq)

pred_quad <-
  predict(quadratic_kalahari, 
          newdata = height_seq) %>%
  data.frame() %>%
  bind_cols(height_seq) 

p2 <-
  ggplot(data = Kalahari_data, 
       aes(x = height_s)) +
  geom_ribbon(data = pred_quad, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_quad,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = weight),
             color = "salmon", shape = 1.5, stroke =2, size = 2.5, alpha = 0.33) +
  labs(y = "weight",
       subtitle = "quadratic") +
  coord_cartesian(xlim = range(-4, 4),
                  ylim = range(0, 100)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

# fit cubic model

cubic_kalahari <- 
  brm(data = Kalahari_data, 
      family = gaussian,
      weight ~ 1 + height_s + height_s2 + height_s3,
      prior = c(prior(normal(60, 10), class = Intercept),
                prior(lognormal(0, 1), class = b, coef = "height_s"),
                prior(normal(0, 1), class = b, coef = "height_s2"),
                prior(normal(0, 1), class = b, coef = "height_s3"),
                prior(uniform(0, 50), class = sigma)),
      iter = 40000, warmup = 39000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/cubic_kalahari")

# get predictions

height_seq <- 
  height_seq %>% 
  mutate(height_s3 = height_s^3)

fitd_cub <-
  fitted(cubic_kalahari, 
         newdata = height_seq) %>%
  as_tibble() %>%
  bind_cols(height_seq)

pred_cub <-
  predict(cubic_kalahari, 
          newdata = height_seq) %>%
  as_tibble() %>%
  bind_cols(height_seq) 

p3 <-
  ggplot(data = Kalahari_data, 
       aes(x = height_s)) +
  geom_ribbon(data = pred_cub, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_cub,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, size = 1/2) +
  geom_point(aes(y = weight),
             color = "salmon", shape = 1.5, stroke =2, size = 2.5, alpha = 0.33) +
  labs(y = "weight",
       subtitle = "cubic") +
  coord_cartesian(xlim = range(-4, 4),
                  ylim = range(0, 100)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p2 + p3
```

Note our use of the coef argument within our prior statements. Since $\beta_{1}$ and $\beta_{2}$ are both parameters of class = b within the `brms` set-up, we need to use the `coef` argument in cases when we want their priors to differ.

At the left extreme, the model predicts that weight will increase as height decreases. This is because we have told the model the data is curved.

$~$

### Splines

$~$

A big family of functions designed to do local smoothing.

This means that only the points within a region determine the shape of the function in that region. In polynomials the whole shape is affected by each data point. Results in splines being far more flexible than polynomials.

Once again, they have no mechanistic reality.

**What is a spline?**

Used in drafting for architecture. They create wriggly lines.

B-splines: regressions on synthetic variables

$\mu_{i} = \alpha + w_{1}B_{i},1 + w_{2}B_{i},2 + w_{3}B_{i},3 +$

$w$ = the weight parameter - like a slope, determine the importance of the different variables for the predicted mean. These can overlap somewhat.

$B$ = the basis function - you make this - only have positive values in narrow regions of x-axis. They turn on weights in isolated regions of the x-axis.

$~$

# Lecture 5: Elemental Confounds
* * * *

$~$

Correlation in general is not surprising. In large data sets, every pair of variables has a statistically discernible non-zero correlation. But since most correlations do not indicate causal relationships, we need tools for distinguishing mere association from evidence of causation. This is why so much effort is devoted to multiple regression, using more than one predictor variable to simultaneously model an outcome.

Waffle house causes divorce?

```{r}
data(WaffleDivorce, package = "rethinking") # this loads WaffleDivorce as a dataframe without loading rehtinking

# Standarise as discussed below. We can use the rethinking package

Waffle_data <-
  WaffleDivorce %>% 
  mutate(d = rethinking::standardize(Divorce),
         m = rethinking::standardize(Marriage),
         a = rethinking::standardize(MedianAgeMarriage))

# time to plot

Waffle_data %>%
  ggplot(aes(x = WaffleHouses/Population, y = Divorce)) +
  stat_smooth(method = "lm", fullrange = T, size = 1/2,
              color = "firebrick4", fill = "firebrick", alpha = 1/5) +
  geom_point(size = 3.5, color = "firebrick4", alpha = 1/2) +
  geom_text_repel(data = Waffle_data %>% filter(Loc %in% c("ME", "OK", "AR", "AL", "GA", "SC", "NJ")),  
                  aes(label = Loc), 
                  size = 3, seed = 1042) +  # this makes it reproducible
  scale_x_continuous("Waffle Houses per million", limits = c(0, 55)) +
  ylab("Divorce rate") +
  theme_bw() +
  theme(panel.grid = element_blank())

```

There is a statistical association! But surely this isn't casual. We'll get back to this later.

$~$

## The four elemental confounds

$~$

Confounds: some feature of the sample and how we use it that misleads us. 

There are four major confounds:

1. The fork

2. The Pipe

3. The Collider

4. The Descendant

```{r}


d1 <- 
  dagify(X ~ Z,
         Y ~ Z,
         coords = tibble(name = c("X", "Y", "Z"),
                         x = c(1, 3, 2),
                         y = c(2, 2, 1)))

d2 <- 
  dagify(Z ~ X,
         Y ~ Z,
         coords = tibble(name = c("X", "Y", "Z"),
                         x = c(1, 3, 2),
                         y = c(2, 1, 1.5)))

d3 <- 
  dagify(Z ~ X + Y,
         coords = tibble(name = c("X", "Y", "Z"),
                         x = c(1, 3, 2),
                         y = c(1, 1, 2)))

d4 <- 
  dagify(Z ~ X + Y,
         D ~ Z,
         coords = tibble(name = c("X", "Y", "Z", "D"),
                         x = c(1, 3, 2, 2),
                         y = c(1, 1, 2, 1.05)))

p1 <- gg_simple_dag(d1) + labs(subtitle = "The Fork")
p2 <- gg_simple_dag(d2) + labs(subtitle = "The Pipe")
p3 <- gg_simple_dag(d3) + labs(subtitle = "The Collider")
p4 <- gg_simple_dag(d4) + labs(subtitle = "The Descendant")

(p1 | p2 | p3 | p4) &
  theme(plot.subtitle = element_text(hjust = 0.5)) &
  plot_annotation(title = "The four elemental confounds")
```

$~$

## The Fork

$~$

X and Y are associated because they share a common cause. However, this association disappears once we account for Z. That means that Y is independent of X once we account for Z.

```{r}
p1
```

Note that once we stratify for Z, the noise affecting X and Y can become independent.

Let's simulate:

```{r}
n <- 1000

tibble(Z = rbernoulli(n, 0.5)) %>% 
  mutate(Z = if_else(Z == "TRUE", 1, 0),
         X = rbernoulli(n, (1 - Z) * 0.1 + Z * 0.9),
         Y = rbernoulli(n, (1 - Z)* 0.1 + Z * 0.9),
         X = if_else(X == "TRUE", 1, 0),
         Y = if_else(Y == "TRUE", 1, 0)) %>% 
  count(X, Y)
```

See how this works? Our simulation specifies no relation between X and Y except through Z, but they are super positively correlated.

Now for a continuous example

```{r}

n <- 300

fork_sim_data <- 
  tibble(Z = rbernoulli(n)) %>% 
  mutate(Z = if_else(Z == "TRUE", 1, 0),
         X = rnorm(n, 2*Z - 1),
         Y = rnorm(n, 2 * Z -1)) 

fork_sim_data %>%
  ggplot(aes(X, Y)) +
  geom_point(aes(colour = as.factor(Z)), shape = 1.5, stroke =2, size = 2.5, alpha = 0.8) +
  geom_smooth(aes(colour = as.factor(Z)), method = "lm", se = FALSE, linewidth = 1.5) +
  geom_smooth(colour = "black", method = "lm", se = FALSE) +
  scale_colour_manual(values = c(met.brewer(name = "Hokusai3")[1], met.brewer(name = "Hokusai3")[3])) +
  theme_classic() +
  theme(panel.grid = element_blank(),
        text = element_text(size = 16))
```

A data problem: **divorce rate** and **marriage rate**.

```{r}
Waffle_data %>%
  ggplot(aes(x = Marriage, y = Divorce)) +
  stat_smooth(method = "lm", fullrange = T, size = 1/2,
              color = "firebrick4", fill = "firebrick", alpha = 1/5) +
  geom_point(size = 3.5, color = "firebrick4", alpha = 1/2) +
  #scale_x_continuous("Marriage rate", limits = c(22, 30)) +
  ylab("Divorce rate") +
  xlab("Marriage rate") +
  theme_bw() +
  theme(panel.grid = element_blank())
```


These two variables are correlated but is marriage rate causal of divorce rate?

Hold on, but there are other things we need to consider.

What else could affect divorce? Age at marriage certainly might. Lets plot:

```{r}
Waffle_data %>%
  ggplot(aes(x = MedianAgeMarriage, y = Divorce)) +
  stat_smooth(method = "lm", fullrange = T, size = 1/2,
              color = "firebrick4", fill = "firebrick", alpha = 1/5) +
  geom_point(size = 3, color = "firebrick4", alpha = 1/2) +
  scale_x_continuous("Median age at marriage", limits = c(22, 30)) +
  ylab("Divorce rate") +
  theme_bw() +
  theme(panel.grid = element_blank())
```

Lets account for this and build the DAG, considering that age of marriage could plausibly lead to more marriage.

To estimate the direct effect of marriage, **you must break the fork** - that is stratify by the common cause - age at marriage.

```{r}
dag_coords <-
  tibble(name = c("A", "M", "D"),
         x    = c(1, 3, 2),
         y    = c(2, 2, 1))

p1 <-
  dagify(M ~ A,
         D ~ A + M,
         coords = dag_coords) %>%
  
  gg_simple_dag()

p1
```

2. Scientific model

What does **stratify** mean:

- It creates sub-populations - in a linear regression it does this:

$D_{i} = Normal(\mu_{i}, \sigma)$

$\mu_{i} = \alpha + \beta_{M}M_{i} + \beta_{A}A_{i}$

That is, make the marriage variable intercept conditional upon age of marriage. To do this simply include it as a term.

We can also stratify using interactions. But how we should do it depends on your casual situation.

3. Statistical model

$D_{i} = Normal(\mu_{i}, \sigma)$

$\mu_{i} = \alpha + \beta_{M}M_{i} + \beta_{A}A_{i}$

$\alpha = Normal(?, ?)$

$\beta_{M} = Normal(?, ?)$

$\beta_{A} = Normal(?, ?)$

$\sigma = Exponential(?)$

Before we talk about the exponential prior lets standardise the data (done above but explained here)

**Why standardise**?

To standardise subtract the mean and divide each value by the standard deviation. Variables become Z scores - values represent SDs from the mean. 0 is the mean.

This makes computation more efficient.

Priors are easier to choose because we have some understanding about effect size.

Lets do some prior simulation, first with weak priors:

$\alpha = Normal(0, 10)$

$\beta_{M} = Normal(0, 10)$

$\beta_{A} = Normal(0, 10)$

$\sigma = Exponential(1)$

These priors are **so broad they are essentially flat. This is a bad idea**. The reason is because almost all the probability mass is for insanely strong pos or neg relationships.

The plot below shows the **stupidly steep slopes** these priors predict.

```{r}
# fit a model with bad priors

Waffle_model_bad_priors <- 
  brm(data = Waffle_data, 
      family = gaussian,
      d ~ 1 + a,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      sample_prior = T,
      file = "fits/Waffle_model_bad_priors")

set.seed(5)

prior_draws(Waffle_model_bad_priors) %>% 
  slice_sample(n = 50) %>% 
  rownames_to_column("draw") %>% 
  expand(nesting(draw, Intercept, b),
         a = c(-2, 2)) %>% 
  mutate(d = Intercept + b * a) %>% 
  
  ggplot(aes(x = a, y = d)) +
  geom_line(aes(group = draw),
            color = "firebrick", alpha = .4) +
  labs(x = "Median age marriage (std)",
       y = "Divorce rate (std)") +
  coord_cartesian(ylim = c(-2, 2)) +
  theme_bw() +
  theme(panel.grid = element_blank()) 
```

Better priors

$\alpha = Normal(0, 0.2)$

$\beta_{M} = Normal(0, 0.5)$

$\beta_{A} = Normal(0, 0.5)$

$\sigma = Exponential(1)$

Re-sample

```{r}
Marriage_age_model <- 
  brm(data = Waffle_data, 
      family = gaussian,
      d ~ 1 + a,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      sample_prior = T,
      file = "fits/Marriage_age_model")

Marriage_age_model
```

Lets have a look at that plot again with our updated priors

```{r}
prior_draws(Marriage_age_model) %>% 
  slice_sample(n = 50) %>% 
  rownames_to_column("draw") %>% 
  expand(nesting(draw, Intercept, b),
         a = c(-2, 2)) %>% 
  mutate(d = Intercept + b * a) %>% 
  
  ggplot(aes(x = a, y = d)) +
  geom_line(aes(group = draw),
            color = "firebrick", alpha = .4) +
  labs(x = "Median age marriage (std)",
       y = "Divorce rate (std)") +
  coord_cartesian(ylim = c(-2, 2)) +
  theme_bw() +
  theme(panel.grid = element_blank()) 
```

Much better!

Lets test for the causal effect of marriage, while closing the fork

```{r}
marriage_model <- 
  brm(data = Waffle_data, 
      family = gaussian,
      d ~ 1 + m + a,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/marriage_model")

marriage_model

```

Ok the exponential distribution:

- Constrained to be positive

- Only info is average displacement from 0, hence the one value, also called the rate. 

- A value of 1 means roughly 1 SD from 0

- Great for sigma

Lets look at the coefficients:

```{r}
mcmc_plot(marriage_model)
```

There is a very small effect of marriage, but we aren't sure if this very small effect is positive or negative. Essentially no causal effect on divorce.

Age of marriage has a large negative effect though.

$~$

**Gold standard: simulating an intervention from the data**

```{r}
post <- as_draws_df(marriage_model)

n <- 4000
A <- sample(Waffle_data$a, size = n, replace = T)

# simulate divorce for M = 0 
bind_cols(
post %>% 
  mutate(m_0 = rnorm(n, b_Intercept + b_m * 0 + b_a * A, sigma)) %>% 
  select(m_0),

# now 1 sd above the mean

post %>% 
  mutate(m_1 = rnorm(n, b_Intercept + b_m * 1 + b_a * A, sigma)) %>% 
  select(m_1)
) %>% 
  mutate(diff = m_1 - m_0) %>% 
  
  ggplot(aes(x = diff)) +
  geom_density(fill = met.brewer("Hokusai2")[1]) +
    labs(x = "Effect of 1 sd increase in M on D") +
  #coord_cartesian(ylim = c(-2, 2)) +
  theme_bw() +
  theme(panel.grid = element_blank()) 
  
```

$~$

## The Pipe

$~$

Structurally different to the fork, but statistically handled similarly.

In this case Z is a mediator variable. That is, the influence of X on Y is transmitted through Z.

```{r}
p2

```

Simulate to understand. Look carefully, **we don't code any effect of X on Y or vice-versa**.

```{r}
n <- 1000

tibble(X = rbernoulli(n, 0.5)) %>% 
  mutate(X = if_else(X == "TRUE", 1, 0),
         Z = rbernoulli(n, (1 - X) * 0.1 + X * 0.9),
         Y = rbernoulli(n, (1 - Z)* 0.1 + Z * 0.9),
         Z = if_else(X == "TRUE", 1, 0),
         Y = if_else(Y == "TRUE", 1, 0)) %>% 
  count(X, Y)
```

Yet there appears to be an effect! This is because X effects Z which affects Y.

An example: a plant growth experiment

Create the data

```{r}
# how many plants would you like?
n <- 100

# h0 is the starting height
# treatment is an antifungal
# fungus is fungus presence
# h1 is end of experiment plant height

set.seed(71)

plant_experiment <- 
  tibble(h0        = rnorm(n, mean = 10, sd = 2), 
         treatment = rep(0:1, each = n / 2),
         fungus    = rbinom(n, size = 1, prob = .5 - treatment * 0.4),
         h1        = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)) %>% 
  mutate(treatment = as.factor(treatment),
         fungus = as.factor(fungus))

```

There are 100 plants troubled by fungal growth. Half the plants receive anti-fungal treatments. We measure growth and presence of fungus.

1. The estimand is the effect of anti-fungal treatment on growth.

2. Scientific model:

```{r}
# define our coordinates
dag_coords <-
  tibble(name = c("H0", "T", "F", "H1"),
         x    = c(1, 5, 4, 3),
         y    = c(2, 2, 1.5, 1))

# save our DAG
dag <-
  dagify(F ~ T,
         H1 ~ H0 + F,
         coords = dag_coords)

# plot 
dag %>%
 gg_simple_dag()
```

This is a harder problem than it first appears

The correct stats analysis here is to ignore the fungal status. This is because this will block the pipe! We remove the effect of the desired part of the experiment.

See the model output

```{r}
fungal_model <- 
  brm(data = plant_experiment, 
      family = gaussian,
      h1 ~ 0 + h0 + treatment + fungus,
      prior = c(prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/fungal_model")

fungal_model
```

The treatment works through fungal growth, so if we account for fungal growth, of course we will find no effect. So the message is not to stratify by an effect of the treatment you're interested in.

To prove this, lets fit the more logical model:

```{r}
fungal_model_causal <- 
  brm(data = plant_experiment, 
      family = gaussian,
      h1 ~ 0 + h0 + treatment,
      prior = c(prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/fungal_model_causal")

fungal_model_causal
```

You should be very wary of including consequences of your treatment other than the desired outcome variable because of the pipe.

$~$

## The Collider

$~$


```{r}
p3
```


X and Y are not associated but both influence Z.

If you stratify by Z, then X and Y become associated! How??

If we learn the value of Z we have to learn something about X and Y. Still confused?


An example: **Selection bias in grants**

- There are 200 grant applications

- Rated on trustworthiness and newsworthiness

- These are uncorrelated variables

Load in the data and plot the relationship

```{r}
set.seed(1914)
n <- 200  # number of grant proposals
p <- 0.1  # proportion to select

grants <-
  # uncorrelated newsworthiness and trustworthiness
  tibble(newsworthiness  = rnorm(n, mean = 0, sd = 1),
         trustworthiness = rnorm(n, mean = 0, sd = 1)) %>% 
  # total_score
  mutate(total_score = newsworthiness + trustworthiness) %>% 
  # select top 10% of combined scores
  mutate(selected = ifelse(total_score >= quantile(total_score, 1 - p), TRUE, FALSE))
```

```{r}

# we'll need this for the annotation
text <-
  tibble(newsworthiness  = c(2, 1), 
         trustworthiness = c(2.25, -2.5),
         selected        = c(TRUE, FALSE),
         label           = c("selected", "rejected"))

grants %>% 
  ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +
  geom_point(aes(shape = selected), alpha = 3/4, stroke = 1.4) +
  geom_text(data = text,
            aes(label = label)) +
  geom_smooth(data = . %>% filter(selected == TRUE),
              method = "lm", fullrange = T,
              color = "lightblue", se = F, size = 1/2) +
  scale_color_manual(values = c("black", "lightblue")) +
  scale_shape_manual(values = c(1, 19)) +
  scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +
  coord_cartesian(ylim = range(grants$trustworthiness)) +
  theme(legend.position = "none") +
  theme_minimal()
```

After selection there is a strong negative correlation.

Ok, so due to selection the only grants that make it are either high in newsworthiness or trustworthiness (makes sense). From a random distribution few grants will be high in both (this is much rarer than being high in one of the categories), The result is a subset of grants that are good at one thing but mainly bad at the other, which creates the appearance of a negative correlation across the entire sample. The key is to not subset the data (don't stratify)

Example 2: selection bias in restaurants

- Restaurants are successful because they have good food or because they're in a good location.

- Only the bad restaurants in really good locations can survive.

- Creates a negative relationship between food quality and location.

Example 3: selection bias in actors

- Actors can be successful because they are skilled or because they are good looking.

- Really bad actors can survive if they are attractive - they end up being the only less skilled actors that can survive.

- Selection creates a negative relationship between looks and skill.

$~$

**Stats example**: endogenous colliders

If you condition on a collider, you create phantom non-causal associations.

Example: **Does age influence happiness?**

```{r}

dag_coords <-
  tibble(name = c("H", "M", "A"),
         x    = 1:3,
         y    = 1)

dagify(M ~ H + A,
       coords = dag_coords) %>%
  gg_simple_dag()

```

1. The estimand: Age affects happiness

This is possibly confounded by marital status.

Suppose age has no effect on happiness but both affect marital status.

So the collider scenario here is that of the married people, only a very advanced age can overcome unhapiness or a very happy dispoistion can overcome a very young age = a neg relationship

Lets sim this to visualise how

```{r}
marriage_sim <- rethinking::sim_happiness(seed = 1977, N_years = 1000)

marriage_sim %>% 
  mutate(married = factor(married,
                          labels = c("unmarried", "married"))) %>% 
  
  ggplot(aes(x = age, y = happiness, color = married)) +
  geom_point(size = 1.75) +
  scale_color_manual(NULL, values = c("grey85", "forestgreen")) +
  scale_x_continuous(expand = c(.015, .015)) +
  theme(panel.grid = element_blank()) +
  theme_minimal()
```

If the visualisation isn't enough to convince you, lets fit the models.

First with the collider 

```{r}
marriage_sim_2 <-
  marriage_sim %>% 
  mutate(mid = factor(married + 1, labels = c("single", "married"))) %>% 
  # only inlcude those 18 and above
  filter(age > 17) %>% 
  # create a, a standarised variable where 18 = 0 and 65 = 1
  mutate(a = (age - 18) / (65 - 18))

marriage_happiness_model <- 
  brm(data = marriage_sim_2, 
      family = gaussian,
      happiness ~ 0 + mid + a,
      prior = c(prior(normal(0, 1), class = b, coef = midmarried),
                prior(normal(0, 1), class = b, coef = midsingle),
                prior(normal(0, 2), class = b, coef = a),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 6,
      file = "fits/marriage_happiness_model")

marriage_happiness_model
```

As we expected, age appears to be negatively associated with happiness.

Now remove the collider

```{r}
marriage_happiness_model_causal <- 
  brm(data = marriage_sim_2, 
      family = gaussian,
      happiness ~ 0 + a,
      prior = c(prior(normal(0, 2), class = b, coef = a),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 6,
      file = "fits/marriage_happiness_model_causal")

marriage_happiness_model_causal
```

Age has literally no effect on happiness now, as we specified in the simulation.

$~$

## The Descendant

$~$

```{r}
d4 <- 
  dagify(Z ~ X + Y,
         D ~ Z,
         coords = tibble(name = c("X", "Y", "Z", "D"),
                                x = c(1, 3, 2, 2),
                                  y = c(1, 1, 2, 1.05))
         )

p4 <- gg_simple_dag(d4) + labs(subtitle = "The Descendant")

p4
```

The variable A is the descendant of the Z variable.

The consequence is if we condition (stratify) by A it will have the same effect as conditioning by Z. While A does not affect X or Y, it is related to Z so it has a weaker (depending on the A -> association strength) but tangible effect on X/Y.

Think about how common descendants are. Any proxy is a descendant! 

E.g. all fitness measures are proxies and therefore descendants of true fitness.

$~$

# Lecture 6: Do calculus and good and bad controls

$~$

## Unobserved confounds

$~$

Suppose we are interested in the direct effect of grandparents education on their grandchildrens education.

- They will have direct and indirect effects on these kids

Estimand: Direct effect of grandparents G on children C.

But parent education is obviously an issue here.

```{r}
dag_coords <-
  tibble(name = c("G", "P", "C", "U"),
         x    = c(1, 2, 2, 3),
         y    = c(2, 2, 1, 1.5))

dagify(P ~ G + U,
       C ~ P + G + U,
       coords = dag_coords) %>%
  gg_simple_dag()
```

The parental effect is likely confounded with living conditions. However, this might not be the case for grandparents who live in a different area.

Remember what our goal is: estimate the direct causal effect of grandparents.

If the situation was this:

```{r}
dagify(P ~ G,
       C ~ P + G,
       coords = dag_coords) %>%
  gg_simple_dag()
```

Then we would just have to stratify by P to block the pipe and find the direct effect of G.

Unfortunately, U exists, which makes P a collider! So this makes P potentially associated with C because U affects them both. 

We are left with only bad choices. Sometimes this is the way it is.

Why does collider bias happen: continuing with the parental example.

Let's simulate the data, with G having no causal effect on C.

```{r}
# how many grandparent-parent-child triads would you like?
n    <- 200 

b_gp <- 1  # direct effect of G on P
b_gc <- 0  # direct effect of G on C
b_pc <- 1  # direct effect of P on C
b_u  <- 2  # direct effect of U on P and C

# simulate triads
set.seed(1)

d <-
  tibble(u = 2 * rbinom(n, size = 1, prob = .5) - 1,
         g = rnorm(n, mean = 0, sd = 1)) %>% 
  mutate(p = rnorm(n, mean = b_gp * g + b_u * u, sd = 1)) %>% 
  mutate(c = rnorm(n, mean = b_pc * p + b_gc * g + b_u * u, sd = 1))
```

Now we'll fit the model without U - which closes the pipe but opens the collider.

```{r}
parent_collider_model <- 
  brm(data = d, 
      family = gaussian,
      c ~ 0 + Intercept + p + g,
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 6,
      file = "fits/parent_collider_model")

parent_collider_model
```

Lets plot what's happening

```{r}
d %>% 
  mutate(centile = ifelse(p >= quantile(p, prob = .45) & p <= quantile(p, prob = .60), "a", "b"),
         u = factor(u)) %>%
  
  ggplot(aes(x = g, y = c)) +
  geom_point(aes(shape = centile, color = u),
             size = 2.5, stroke = 1/4) +
  stat_smooth(data = . %>% filter(centile == "a"),
              method = "lm", se = F, size = 1/2, color = "black", fullrange = T) +
  scale_shape_manual(values = c(19, 1)) +
  scale_color_manual(values = c("black", "lightblue")) +
  theme(legend.position = "none")
```

Stratifying by P as the regression line shows creates a negative relationship where nothing causal is happening!

Now we'll fit the model with U

```{r}
grandparent_total <- 
  update(parent_collider_model,
         newdata = d,
         formula = c ~ 0 + Intercept + p + g + u,
         seed = 6,
         file = "fits/grandparent_total")

grandparent_total
```

Note the change in the effect of G on C. Remember that we coded it to have 0 effect in our simulation!

**In short**: there are two ways for parents to attain their education: from G or from U. This is the classic setup for a collider.

$~$

## DAG thinking

$~$

In an experiment we aim to cut all the causes of the treatment through randomisation. 

But if we don't randomise, can we overcome this with statistics? **Sometimes**.

First some notation:

$P(Y|do(X)) = P(Y|?)$

Where P(Y) is the distribution of Y

So the above means the distribution of Y conditional upon do(X) equals the distribution of Y conditional on some set of information.

do(X) means to intervene on X - to set it to some particular value.

We can do this with a DAG!

```{r}
dag_coords <-
  tibble(name = c("U", "X", "Y"),
         x    = c(2, 1, 3),
         y    = c(2, 1, 1))

dagify(X ~ U,
       Y ~ X + U,
       coords = dag_coords) %>%
  gg_simple_dag()
```

If we don't know U we can find the distribution of Y, stratified by X and U, averaged over the distribution of U. But how?

Remember that the causal effect of X on Y is not the coefficient relating X to Y. It is actually the distribution of Y when we change X, averaged over the distributions of the control variables (U in this case). This is a marginal effect.

## Do calculus

**The back-door criterion**

A way to analyse graphs to find sets of variables to stratify by

It can also help us design research.

Formally it is a rule to find a set of variables to stratify (condition) by to yield $P(Y|do(X))$

The steps:

1. Identify all paths connecting the treatment (X) to the outcome (Y).

2. Once you have the paths, focus on the paths that enter X. These are back-door paths (non-causal).

3. Find adjustment set (set of variables to stratify by) that closes/blocks all back-door paths.

## Controls

**Control variable**: a variable we introduce to an analysis so that a causal estimate of something else is possible.

Things you should not do:

- Add everything you've measured

- Including all variables that aren't collinear: sometimes both variables play a role in causality 

- Anything measured before treatment is fair game to add - not true


Taking some examples from Cinelli, Forney and Pearl 2021: A crash course in good and bad controls

**Example 1**

```{r}
dag_coords <-
  tibble(name = c("u", "X", "Z", "v", "Y"),
         x    = c(1, 1, 2, 3, 3),
         y    = c(2, 1, 1.6, 2, 1))

dagify(X ~ u,
       Z ~ u + v,
       Y ~ v + X,
       coords = dag_coords) %>%
  gg_simple_dag()
```

Where _u_ and _v_ are unobserved variables.

Let's say that Z denotes friendship status and X and Y the health of two people. _u_ and _v_ are the hobbies of person X and person Y.

If we stratify by Z, that is, include it as a control, then we open a back-door path (it is a collider). This is bad for estimating the causal effect of X on Y.

Note that Z could be a pre-treatment variable (I haven't been taught this but apparently it's a thing).

Do calculus time - list the paths:

1. X -> Y: front door

2. X <- u -> Z <- v -> Y: backdoor. Closed **unless** you include Z. A bad control.

**Example 2**

```{r}
dag_coords <-
  tibble(name = c("X", "Z", "u", "Y"),
         x    = c(1, 2, 2.5, 3),
         y    = c(1, 1, 1.2, 1))

dagify(Z ~ u + X,
       Y ~ Z + u,
       coords = dag_coords) %>%
  gg_simple_dag()
```

Analyse the paths:

X -> Z -> Y: front

X -> Z <- u -> Y: still front

Z is a mediator here, so if the goal to to estimate the effect of X on Y, we should not include Z in the model anyway. There is no back-door path we have to block.

However, it's worse than that, because u - an unexplained variable - creates a fork. Including Z makes the estimate even worse than it would usually. This should always be noted where things might have a common cause e.g. happiness and lifespan or body size and fecundity. 

**Add sim and plot**

```{r}

```


Note that Z would turn out to be significant. This is spurious! Don't do backwards step selection. You need a causal model.

$~$

## Do not touch the collider

$~$

DAG example

**Case control bias**

```{r}
dag_coords <-
  tibble(name = c("X", "Z", "Y"),
         x    = c(1, 2, 2),
         y    = c(1, 2, 1))

dagify(Z ~ Y,
       Y ~ X,
       coords = dag_coords) %>%
  gg_simple_dag()
```

Ok, so this is a simple pipe. But if we want to know the causal effect of X on Y then we run into an issue if we control for Z.

Imagine that X = education, Y = occupation and Z = Income. If we control for income, then this removes variation in Y, leaving less for education to explain - the effect size is underestimated. Now Z is not a cause of Y, but the model has no way of knowing that so it just measures the covariance. We are required to make this distinction.

A common bad control. When you apply selection on the outcome - this ruins scientific influence.

E.g. if you want to know the effect of education on occupation then you should not stratify by income because it narrows the range of cases you compare at each level. 

**Sim this one as well code at 58.34**

$~$

**Precision parasite**

```{r}
dag_coords <-
  tibble(name = c("X", "Z", "Y"),
         x    = c(1, 1, 2),
         y    = c(1, 2, 1))

dagify(X ~ Z,
       Y ~ X,
       coords = dag_coords) %>%
  gg_simple_dag()
```

While Z affects X it is not a back-door path because it does not connect to Y (except via the front door). However, it is still not good to condition on Z.

The parasite does not change the mean estimate, but it creates an estimate with higher variance. We lose precision. 

**Another sim**

$~$

**Bias amplification**

A bad version of the precision parasite.

```{r}
dag_coords <-
  tibble(name = c("X", "Z", "Y", "u"),
         x    = c(1, 1, 2, 1.5),
         y    = c(1, 2, 1, 1.75))

dagify(X ~ Z + u,
       Y ~ X + u,
       coords = dag_coords) %>%
  gg_simple_dag()
```

Remember _u_ is an unmeasured variable. We can't get an unbiased causal inference in this scenario.

If we add Z, the world implodes. We get a really biased estimate, with an inflated effect size.

WHY?

Covariation between X and Y requires variation in their causes. Variation is removed by conditioning - we are accounting for this association then finding the variance explained by the remaining variables. So stratifying on X removes variation in X, which gives more weight to be explained by the confound U = stronger false relationship!

Hard to get my head around but wow.

Lets plot to help understanding.

We'll sim a situation like the above. Z is a binomial variable that causally affects X. X has no effect on Y. However, u affects both X and Y.   

```{r}
n <- 1000
Z <- rethinking::rbern(n)
u <- rnorm(n)
X <- rnorm(n, 7*Z + u)
Y <- rnorm(n, 0*X + u)

tibble(Z = as.factor(Z),
       u = u,
       X = X,
       Y = Y) %>% 
  
  ggplot(aes(x = X, y = Y, colour = Z)) +
  geom_point(aes(shape = Z), stroke =2, size = 1.2, alpha = 0.5) +
  geom_smooth(
    method = "lm", fullrange = T,
    color = "black", se = F, size = 1) +
  geom_smooth(data = . %>% filter(Z == "1"),
              method = "lm", fullrange = T,
              color = "blue", se = F, size = 1) +
  geom_smooth(data = . %>% filter(Z == "0"),
              method = "lm", fullrange = T,
              color = "red", se = F, size = 1) +
  scale_shape_manual(values = c(1, 19)) 
```

There is less variation in each 'stratification'!

$~$


The bias is amplified,

Why?

- X can't vary unless its causes vary 

- If Z doesn't vary then X can't vary. Stratifying by this removes variance in X and in that way Y. This means a larger proportion of the variation in X comes from the confounding fork u.

$~$

## The Table 2 fallacy

$~$

In many fields the typical Table 2 in a manuscript will contain model coefficients.

The thing is, not all the coefficients are causal effects and there is no information that tells you whether they are.

Remember you have designed your model to identify the causal effect of X on Y. It is dangerous to interpret the effects of other variables that you have not designed the model for. The other variables are there to block back-door paths.

```{r}
dag_coords <-
  tibble(name = c("S", "A", "X", "Y"),
         x    = c(1, 1, 2, 3),
         y    = c(3, 1, 2, 2))

dagify(S ~ A,
       X ~ S + A,
       Y ~ S + A + X,
       coords = dag_coords) %>%
  gg_simple_dag()
```

S = smoking

A = age

X = HIV

Y = stroke

We want to know the effect of X on Y - the effect of HIV on stroke risk.

Things to note:

- Age has many effects, including some quite indirect ones

- We need to decontaminate X of the effects of smoking and age.

Back-door criterion:

List paths

1. X -> Y

2. X <- S -> Y

3. X <- A -> Y

4. X <- A -> S -> Y

We need to close paths 2-4. 

We can condition by A and S to effectively do this.

So we could fit a model that looks like this: 

$$Y = X + S + A$$
What if we focus on the effect of S?


```{r}
dagify(S ~ A,
       X ~ S + A,
       Y ~ S + A + X,
       coords = dag_coords) %>%
  gg_simple_dag()
```

S is confounded by A, as there is a back-door path. But we condition on A and X so that back-door path is closed.

BUT

What are we estimating for S? Well we have removed the indirect effect of S that acts through X, as we have closed that pipe. I.e. We are only estimating how smoking directly affects stroke, but not including how smoking acts on stroke through increasing the risk of HIV. This might be small or it might be large, we don't know. The take home is this coefficient measures something different than that of X.

Now lets focus on **Age**.

It acts through everything, so we need not include any of the conditional variables to find its total effect on stroke (both direct and indirect). But we have conditioned on S and X. We have closed these pathways. We only get the direct effect, which may be tiny!

Take-home: don't interpret all coefficients as unconditional effects!

**How to report results**

Either

1. Just report causal coefficients

2. Give an explicit interpretation of each coefficient

$~$

# Lecture 7: Fitting Over and Under
* * * *

$~$

Ockham's razor: the simple solution is often the best. This lecture will focus on why this is.

We need to design efficient estimators: the struggle against data. Causual inference doesn't help us much here.

**Problems with prediction**

To aid our understanding let's use a Hominin dataset. We are interested in the affect of weight (mass) on brain volume. 

```{r}

# create the data and print the tibble

(
  hominin_data <- 
  tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))
  )

# standardise. However, rescale the outcome, brain volume, so that the largest observed value is 1. Why not standardize brain volume as well? Because we want to preserve zero as a reference point: No brain at all. You can’t have negative brain. I don’t think.

hominin_data <-
  hominin_data %>% 
  mutate(mass_std = (mass - mean(mass)) / sd(mass),
         brain_std = brain / max(brain))

# plot the relationship between brain volume and weight


hominin_data %>% 
  ggplot(aes(x = mass, y = brain, label = species)) +
  geom_point(size = 5, colour = met.brewer("Cassatt1")[2]) +
  geom_text_repel(size = 3, colour = "black", family = "Courier", seed = 438, box.padding = 1) +
  labs(subtitle = "Average brain volume by body\nmass for six hominin species",
       x = "Body mass (kg)",
       y = "Brain volume (cc)") +
  xlim(30, 65) +
  theme_classic() +
    theme(text = element_text(family = "Courier"),
          panel.background = element_rect(fill = alpha(met.brewer("Cassatt1", 8)[4], 1/4)))

```


What can we do with statistical models:

1. Find a function that describes these points (fitting)

2. What function explains these points (causal inference)

3. What happens when we change a point's mass (intervention)

4. What is the next observation from the same process (forecasting or **prediction**)

We will focus on point 4 in this lecture.

$~$

## Cross Validation

$~$

- For simple models, more parameters improves fit within sample but may reduce accuracy out of sample

- There is a trade off between flexibility and **overfitting**

We can test how effective our linear regression is at prediction using **Leave-one-out-cross-validation**

The steps:

1. Drop one point

2. Fit line to remaining points

3. Predict dropped point

4. Repeat (1) with next point

5. Score based on the error between actual point and predicted point of the regression

```{r}
hominin_brain_mass_model <- 
  brm(data = hominin_data, 
      family = gaussian,
      brain_std ~ 1 + mass_std,
      prior = c(prior(normal(0.5, 1), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(lognormal(0, 1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 7,
      file = "fits/hominin_brain_mass_model")
```

**Bayesian Cross-Validation**

As usual we use distributions in bayes rather than points. The predictive distribution is uncertain, we don't get a specific point estimate for the prediction. A cross-validation score is a bit harder to get over a distribution, but luckily we can follow the **log pointwise predictive density** equation. I won't display it here because we can depend on the software. Just know that it is referred to as $lppd_{CV}$. Essentially this goes point by point like above, finds the posterior distribution, then averages this distribution over the number of samples (I think).

Some definitions:

**In sample score**: summed deviance of points from the full sample regression

**Out of sample score**: summed deviance of points from each out of sample regression

Let's fit more complex models

```{r}
# quadratic
b7.2 <- 
  update(hominin_brain_mass_model,
         newdata = hominin_data, 
         formula = brain_std ~ 1 + mass_std + I(mass_std^2),
         iter = 2000, warmup = 1000, chains = 4, cores = 4,
         seed = 7,
         file = "fits/b07.02")

# cubic
b7.3 <- 
  update(hominin_brain_mass_model,
         newdata = hominin_data, 
         formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3),
         iter = 2000, warmup = 1000, chains = 4, cores = 4,
         seed = 7,
         control = list(adapt_delta = .9),
         file = "fits/b07.03")


# fourth-order
b7.4 <- 
  update(hominin_brain_mass_model,
         newdata = hominin_data, 
         formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4),
         iter = 2000, warmup = 1000, chains = 4, cores = 4,
         seed = 7,
         control = list(adapt_delta = .995),
         file = "fits/b07.04")

# fifth-order
b7.5 <- 
  update(hominin_brain_mass_model,
         newdata = hominin_data, 
         formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5),
         iter = 2000, warmup = 1000, chains = 4, cores = 4,
         seed = 7,
         control = list(adapt_delta = .99999),
         file = "fits/b07.05")

# make a function for plotting

make_figure7.3 <- function(brms_fit, ylim = range(hominin_data$brain_std)) {
  
  
  # define the new data 
  nd <- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))
  
  # simulate and wrangle
  fitted(brms_fit, newdata = nd, probs = c(.055, .945)) %>% 
    data.frame() %>% 
    bind_cols(nd) %>% 
    
    # plot!  
    ggplot(aes(x = mass_std)) +
    geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),
                    color = met.brewer("Cassatt1")[1], size = 1/2, 
                    fill = alpha(met.brewer("Cassatt1")[2], 1/3)) +
    geom_point(data = hominin_data,
               aes(y = brain_std),
               color = met.brewer("Cassatt1")[2],
               size = 5) +
    labs(x = "body mass (std)",
         y = "brain volume (std)") +
    coord_cartesian(xlim = c(-1.2, 1.5),
                    ylim = ylim) +
    theme_classic() +
    theme(text = element_text(family = "Courier"),
          panel.background = element_rect(fill = alpha(met.brewer("Cassatt1", 8)[4], 1/4)))
  
}

p1 <- make_figure7.3(hominin_brain_mass_model)
p2 <- make_figure7.3(b7.2)
p3 <- make_figure7.3(b7.3)
p4 <- make_figure7.3(b7.4, ylim = c(.25, 1.1))
p5 <- make_figure7.3(b7.5, ylim = c(.1, 1.4))


((p1 | p2) / (p3 | p4) / (p5)) +
  plot_annotation(title = "Figure7.3. Polynomial linear models of increasing\ndegree for the hominin data.")
```

So how does this affect our fit?

The in-sample score improves with model complexity, but the out of sample score gets worse (larger)! For the 4th order polynomial model in sample is insanely good, but the fit varies wildly once we remove a point. The take home is that we can build really complex models that fit the observed data really well, but have little to no predictive capacity for new data points. This is the basic trade-off inherent to modelling: over fitting leads to bad predictions.

But what if we have more data?

With more data we find that there is an intermediate model complexity (the 2nd degree model in this case) that maximises both within and out of sample deviance; that is, it is the best at describing these data. However, this doesn't mean that it explains them, it's just best at pure prediction in the **absence of intervention**. If we want to do this, we need to understand the causal structure.

::: {.callout-tip}
## From McElreath

The overfit polynomial models fit the data extremely well, but they suffer for this within-sample accuracy by making nonsensical out-of-sample predictions. In contrast, **underfitting** produces models that are inaccurate both within and out of sample. They learn too little, failing to recover regular features of the sample. (p. 201)
:::

$~$

To explore the distinctions between overfitting and underfitting, we’ll need to refit the models above several times after serially dropping one of the rows in the data. You can `filter()` by `row_number()` to drop rows in a tidyverse kind of way. For example, we can drop the second row of `hominin_data` like this.

```{r}
hominin_data %>%
  mutate(row = 1:n()) %>% 
  filter(row_number() != 2)
```

Now we'll make a function `brain_loo_lines()` that will refit the model and extract lines information in one step.

```{r}
nd <- tibble(mass_std = seq(from = -2, to = 2, length.out = 100))

brain_loo_lines <- function(brms_fit, row, ...) {
  
  # refit the model
  new_fit <- 
    update(brms_fit,
           newdata = filter(hominin_data, row_number() != row), 
           iter = 2000, warmup = 1000, chains = 4, cores = 4,
           seed = 7,
           refresh = 0,
           ...)
  
  # pull the lines values
  fitted(new_fit, 
         newdata = nd) %>% 
    data.frame() %>% 
    select(Estimate) %>% 
    bind_cols(nd)
  
}

```

Now use `map()` to iterate the function on all LOO dataset versions

```{r}
hominin_fits <-
  tibble(row = 1:7) %>% 
  mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = hominin_brain_mass_model, row = .))) %>% 
  unnest(post)

hominin_polynomial_fits <-
  tibble(row = 1:7) %>% 
  mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = b7.4, 
                                                 row = ., 
                                                 control = list(adapt_delta = .999)))) %>% 
  unnest(post)
```

Plot the results

```{r}
# left
p1 <-
  hominin_fits %>%  
  
  ggplot(aes(x = mass_std)) +
  geom_line(aes(y = Estimate, group = row),
            color = met.brewer("Cassatt1", 8)[2], linewidth = 1/2, alpha = 1/2) +
  geom_point(data = hominin_data,
             aes(y = brain_std),
             color = met.brewer("Cassatt1", 8)[2],
             size = 5) +
  labs(x = "body mass (std)",
       y = "brain volume (std)") +
  coord_cartesian(xlim = range(hominin_data$mass_std),
                  ylim = range(hominin_data$brain_std)) +
  theme_classic() +
  theme(text = element_text(family = "Courier"),
        panel.background = element_rect(fill = alpha(met.brewer("Cassatt1", 8)[4], 1/4)))

# right
p2 <-
  hominin_polynomial_fits %>%  
  
  ggplot(aes(x = mass_std, y = Estimate)) +
  geom_line(aes(group = row),
            color = met.brewer("Cassatt1", 8)[2], linewidth = 1/2, alpha = 1/2) +
  geom_point(data = hominin_data,
             aes(y = brain_std),
             color = met.brewer("Cassatt1", 8)[2],
             size = 5) +
  labs(x = "body mass (std)",
       y = "brain volume (std)") +
  coord_cartesian(xlim = range(hominin_data$mass_std),
                  ylim = c(-0.1, 1.4)) +
  theme_classic() +
  theme(text = element_text(family = "Courier"),
        panel.background = element_rect(fill = alpha(met.brewer("Cassatt1", 8)[4], 1/4)))

# combine
p1 + p2
```

::: {.callout-tip}
## From McElreath

“Notice that the straight lines hardly vary, while the curves fly about wildly. This is a general contrast between underfit and overfit models: sensitivity to the exact composition of the sample used to fit the model” (p. 201).
:::


## Regularisation

$~$

We can design models that are better at prediction in their structure.

We want to identify regular features in the data, this leads to good predictions

We can minimise over fitting with:

**Priors**:

- Sceptical priors have tighter variance, and reduce the flexibility of a model

- Often tighter than we think they need to be

- These improve predictions

- Essentially, tight priors stop the model overreacting to nonsense within the sample

**Choosing the width of a prior**

- For causal inference, use science, then go a touch tighter.

- For pure prediction, we can use cross-validation to tune the prior.

- Many tasks are a mix of both

**Prediction penalty**

For N points, cross-validation requires fitting N models. This quickly becomes computationally expensive, when your dataset gets larger.

Good news! We can compute the prediction penalty from a single model fit. There are two ways:

1. Importance sampling (PSIS)

2. Information criteria (WAIC or LOO)

$~$

## Predictive criteria should not be casued to choose model structure

- They often prefer confounds and colliders

**Example: return of the plant growth experiment**

The DAG looks like this:

```{r}
# define our coordinates
dag_coords <-
  tibble(name = c("H0", "T", "F", "H1"),
         x    = c(1, 5, 4, 3),
         y    = c(2, 2, 1.5, 1))

# save our DAG
dag <-
  dagify(F ~ T,
         H1 ~ H0 + F,
         coords = dag_coords)

# plot 
dag %>%
 gg_simple_dag()
```

And the data:

```{r}
head(plant_experiment)
```

Now lets add the information criterion `loo` (leave one out) to the model object. `loo` is the PSIS metric McElreath spruiks. Then plot the result.

```{r}
fungal_model <- add_criterion(fungal_model, criterion = "loo")
fungal_model_causal <- add_criterion(fungal_model_causal, criterion = "loo")

loo <- loo_compare(fungal_model, fungal_model_causal, criterion = "loo")

print(loo, simplify = F)

```

```{r}
loo[, 7:8] %>% 
  data.frame() %>% 
  rownames_to_column("model_name") %>% 
  mutate(model_name = fct_reorder(model_name, looic, .desc = T)) %>% 
  
  ggplot(aes(x = looic, y = model_name, 
             xmin = looic - se_looic, 
             xmax = looic + se_looic)) +
  geom_pointrange(color = met.brewer("Cassatt1", 8)[2],
                  fill = met.brewer("Cassatt1", 8)[1], shape = 21) +
  labs(x = "PSIS-LOO", y = NULL) +
  theme(axis.ticks.y = element_blank()) +
  theme_classic() +
  theme(text = element_text(family = "Courier"),
        panel.background = element_rect(fill = alpha(met.brewer("Cassatt1", 8)[4], 1/4)))
```

Fungus status is actually a better predictor of plant growth than anti-fungal treatment. Makes sense, the treatment is essentially an imperfect proxy for fungal status. For prediction this is better, but it's terrible for determining the effect of the treatment. See that your preferred model depends entirely upon your aim.

```{r}
p1 <-
  plant_experiment %>% 
  mutate(growth = h1 - h0) %>% 
  ggplot(aes(x = fungus, y = growth, colour = treatment)) +
  geom_jitter(size = 3, shape = 1.5, stroke =2, alpha = 0.75) +
  labs(x = "Presence of fungus",
       y = "Plant growth") + 
  theme_classic()

p2 <-
  plant_experiment %>% 
  mutate(growth = h1 - h0) %>% 
  ggplot(aes(x = treatment, y = growth, colour = fungus)) +
  geom_jitter(size = 3, shape = 1.5, stroke =2, alpha = 0.75) +
  labs(x = "Presence of fungus",
       y = "Plant growth") +
  theme_classic()


p1 / p2

```

$~$

## Importance sampling

$~$

Use a single posterior distribution for N points to sample for each posterior for N-1 points.

Key idea: Point with low probability has a strong influence on posterior distribution. Highly probable points on the other hand do not individually affect the posterior distribution much, because there are other points in close proximity conveying very similar information.

Can use pointwise probabilities to re-weight samples from the posterior.

BUT, importance sampling tends to be unreliable.

The best current one: **Pareto-smoothed importance sampling**

- More stable (lower variance)

- Useful diagnostics (it will tell you when it's bad aka has high variance)

- Identifies important (high leverage) points (outliers)

$~$

## Information criteria

$~$

**Akaike information criteria: AIC**

The first form. For flat priors and large samples:

$AIC = (-2) * lppd + 2k$

Where -2 is a scaling term, lppd is the log pointwise predictive density and 2k is a penalty term for complexity, where k is the number of parameters.

If we want to use good priors, we can't use it. 

$~$

**Widely applicable information criterion: WAIC**

$WAIC(y, \Theta) =  -2(lppd - \sum var_{\Theta} log p(y_{i}|\Theta))$

This is a more complex formula but is has some similarities to the AIC formula.

$\sum var_{\Theta} log p(y_{i}|\Theta)$ is the penalty term now.

$WAIC$ provides a very similar result to PSIS, but without the diagnostics.

$~$

## Outliers and robust regression

$~$

Some points are more important for the posterior distribution than others.

Outliers are observations in the tails of predictive distributions 

If a model has outliers, this indicates the model doesn't expect enough variation. This can lead to poor, damaging predictions.

Say we want to predict hurricane strength. We really don't want to disregard extreme hurricanes, as this would be disastrous.

Don't drop outliers, **it's the models fault, not the data's**. But how do we deal with them?

- It's the model that's wrong, not the data

- First, quantify influence of each point - we can do this with cross validation

- Second, use a mixture model with fatter tails i.e. the student t distribution.

$~$

**Robust regression**

We're back at the divorce rate example, where Idaho and Maine are outliers. Idaho has the lowest median marriage age and a very low divorce rate, while Maine has a very high divorce rate, but an average age of marriage.

```{r}
b5.3 <- 
  brm(data = Waffle_data, 
      family = gaussian,
      d ~ 1 + m + a,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/b05.03")

b5.3 <- add_criterion(b5.3, criterion = "loo")

b5.3 <- add_criterion(b5.3, "waic", file = "fits/b05.03")

Waffle_data %>% 
  mutate(outlier = if_else(Location %in% c("Idaho", "Maine"), "YES", "NO")) %>% 
  ggplot(aes(x = a, y = d, colour = outlier)) +
  geom_point(shape = 1.5, stroke = 2, size = 3) +
  scale_colour_manual(values = c(met.brewer("Cassatt1")[2], met.brewer("Cassatt1")[6])) +
  geom_text_repel(aes(label = Location),
                  data = Waffle_data %>% filter(Location == c("Idaho", "Maine")), size = 5, colour = "black", family = "Courier", seed = 438, box.padding = 0.5) +
  labs(x = "Age at marriage (std)",
       y = "Divorce rate (std)") +
  theme_classic() +
  theme(legend.position = "none")

```

Using PSIS we can find which data points are outliers

```{r}
loo(b5.3)
```

Ok so we have one point that is having a large effect on the posterior. Which is it?

```{r}
loo(b5.3) %>% 
  pareto_k_ids(threshold = 0.5)
```

Point 13, this means row 13 in the tibble. Lets find out which location that corresponds to.

```{r}
Waffle_data %>% 
  as_tibble() %>% 
  slice(13) %>% 
  select(Location:Loc)
```

We can quantify the influence using the PSIS k statistic or the WAIC penalty term (the effective number of parameters)

Let's plot them both 

```{r}

tibble(pareto_k = b5.3$criteria$loo$diagnostics$pareto_k,
       p_waic   = b5.3$criteria$waic$pointwise[, "p_waic"],
       Loc      = pull(Waffle_data, Loc)) %>% 
  
  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == "ID")) +
  geom_vline(xintercept = .5, linetype = 2, color = "black", alpha = 1/2) +
  geom_point(aes(shape = Loc == "ID"), size = 3, stroke = 2, alpha = 0.8) +
  geom_text(data = . %>% filter(p_waic > 0.5),
            aes(x = pareto_k - 0.03, label = Loc),
            hjust = 1) +
  scale_color_manual(values = met.brewer("Cassatt1")[c(2, 5)]) +
  scale_shape_manual(values = c(1, 19)) +
  labs(subtitle = "Gaussian model (b5.3)") +
  theme_classic() +
  theme(legend.position = "none")
```

We can combat outliers with **mixture models**

- Use multiple gaussian distributions with different variances: produces the **student t distribution**

- We have a distribution with thicker tails, meaning that it **expects more extreme points than your standard gaussian model**

```{r}
tibble(x = seq(from = -6, to = 6, by = 0.01)) %>% 
  mutate(Gaussian    = dnorm(x),
         `Student-t` = dstudent_t(df = 2, x)) %>% 
  pivot_longer(-x,
               names_to = "likelihood",
               values_to = "density") %>% 
  mutate(`minus log density` = -log(density)) %>% 
  pivot_longer(contains("density")) %>% 
  
  ggplot(aes(x = x, y = value, group = likelihood, color = likelihood)) +
  geom_line() +
  scale_color_manual(values = c(met.brewer("Cassatt1")[2], "black")) +
  ylim(0, NA) +
  labs(x = "value", y = NULL) +
  theme(strip.background = element_blank()) +
  facet_wrap(~ name, scales = "free_y") +
  theme_classic()
```

Let's try it out and retest those PSIS, WAIC values.

This is an easy model to fit in `brms`

```{r}
b5.3t <- 
  brm(data = Waffle_data, 
      family = student,
      bf(d ~ 1 + m + a, nu = 2),
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 5,
      file = "fits/b05.03t")

b5.3t <- add_criterion(b5.3t, criterion = c("loo", "waic"))

# plot

tibble(pareto_k = b5.3t$criteria$loo$diagnostics$pareto_k,
       p_waic   = b5.3t$criteria$waic$pointwise[, "p_waic"],
       Loc      = pull(Waffle_data, Loc)) %>% 
  
  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == "ID")) +
  geom_point(aes(shape = Loc == "ID"), size = 3, stroke = 2) +
  geom_text(data = . %>% filter(Loc %in% c("ID", "ME")),
            aes(x = pareto_k - 0.01, label = Loc),
            hjust = 1) +
  scale_color_manual(values = met.brewer("Cassatt1")[c(2, 5)]) +
  scale_shape_manual(values = c(1, 19)) +
  labs(subtitle = "Student-t model (b5.3t)") +
  theme_classic() +
  theme(legend.position = "none") 
```

And a final comparison

```{r}
bind_rows(posterior_samples(b5.3),
          posterior_samples(b5.3t)) %>% 
  mutate(fit = rep(c("Gaussian (b5.3)", "Student-t (b5.3t)"), each = n() / 2)) %>% 
  pivot_longer(b_Intercept:sigma) %>% 
  mutate(name = factor(name,
                       levels = c("b_Intercept", "b_a", "b_m", "sigma"),
                       labels = c("alpha", "beta[a]", "beta[m]", "sigma"))) %>% 
  
  ggplot(aes(x = value, y = fit, color = fit)) +
  stat_pointinterval(.width = .95, size = 1) +
  scale_color_manual(values = c(met.brewer("Cassatt1")[1], "black")) +
  labs(x = "posterior", y = NULL) +
  theme_classic() +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        legend.position = "none",
        strip.background = element_rect(fill = alpha(met.brewer("Cassatt1")[2], 1/4), color = "transparent"),
        strip.text = element_text(size = 12)) +
  facet_wrap(~ name, ncol = 1, labeller = label_parsed)
```

Overall, the coefficients are very similar between the two models.

**Robust regression summary**

- Lots of unobserved heterogeneity that comes from a mixture of Gaussian distributions

- Thicker tails makes the model less surprised/influenced by extreme observations

- Degrees of freedom determines how much weight to put in the tails. No good way of doing this other than trial and error...

- Student-t regression is a nice default (perhaps better than Gaussian)
