[
  {
    "objectID": "simulating_and_modelling.html",
    "href": "simulating_and_modelling.html",
    "title": "Simulating data and modelling it in brms",
    "section": "",
    "text": "Code\nlibrary(tidyverse) # for tidy style coding\nlibrary(brms) # for bayesian models\nlibrary(tidybayes) # for many helpful functions used to visualise distributions\nlibrary(MetBrewer) # for pretty colours\nlibrary(patchwork) # for combining plots"
  },
  {
    "objectID": "simulating_and_modelling.html#simulating-data",
    "href": "simulating_and_modelling.html#simulating-data",
    "title": "Simulating data and modelling it in brms",
    "section": "Simulating data",
    "text": "Simulating data\n\n\nCode\nsim_growth_data_small <-\n  expand_grid(\n    Days = rep.int(1:100, 1),\n    Sex = c(\"Female\", \"Male\")) %>% \n  arrange(Sex) %>% \n  mutate(Mass = if_else(Sex == \"Female\",\n                        Days * 0.2 + rnorm(100, 50, 5),\n                        Days * 0.3 + rnorm(100, 50, 5)))\n\nsim_growth_data_large <-\n  expand_grid(\n    Days = rep.int(1:100, 10),\n    Sex = c(\"Female\", \"Male\")) %>% \n  arrange(Days) %>% \n  mutate(Mass = if_else(Sex == \"Female\",\n                        Days * 0.2 + rnorm(1000, 50, 5),\n                        Days * 0.3 + rnorm(1000, 50, 5)))\n\n\nPlot them\n\n\nCode\np1 <-\n  sim_growth_data_small %>%\n  ggplot(aes(x = Days)) +\n  geom_point(aes(y = Mass, colour = Sex), shape = 1.5, stroke =2, size = 2.5, alpha = 0.9) +\n  scale_colour_manual(values = c(met.brewer(name = \"Hokusai3\")[1], met.brewer(name = \"Hokusai3\")[3])) +\n  labs(x = NULL, y = \"Mass (grams)\", subtitle = \"One observation each day\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16))\n\np2 <-\n  sim_growth_data_large %>%\n  ggplot(aes(x = Days)) +\n  geom_point(aes(y = Mass, colour = Sex), shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +\n  scale_colour_manual(values = c(met.brewer(name = \"Hokusai3\")[1], met.brewer(name = \"Hokusai3\")[3])) +\n  labs(x = \"Days since hatching\", y = \"Mass (grams)\", subtitle = \"Ten observations each day\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16))\n\np1 / p2"
  },
  {
    "objectID": "simulating_and_modelling.html#model-with-brms",
    "href": "simulating_and_modelling.html#model-with-brms",
    "title": "Simulating data and modelling it in brms",
    "section": "Model with brms",
    "text": "Model with brms\nThe core inputs to make the model run are formula, family and data.\nYou can code them like this:\nbrm(Mass ~ 1 + Days, family = gaussian, data = sim_growth_data_small)\nNext are your priors\nThe get_prior function is very useful here. Let’s try it out:\n\n\nCode\nget_prior(Mass ~ 1 + Days, family = gaussian, data = sim_growth_data_small)\n\n\n                    prior     class coef group resp dpar nlpar lb ub\n                   (flat)         b                                 \n                   (flat)         b Days                            \n student_t(3, 62.9, 10.4) Intercept                                 \n    student_t(3, 0, 10.4)     sigma                             0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThis shows the brms defaults for all the priors that are neccessary to run this model. If you don’t supply your own prior, brms will use its defaults.\nSo we need priors for b, Intercept and sigma\nWhat do thes mean?\n\nb is the effect that Days has on Mass. We know that things generally get bigger after hatching, so this must be positive.\nIntercept is the value for mass when Days = 0. This also must be positive.\nsigma is the variation in mass. This also must be > 0.\n\nTo check out what a prior looks like I use this quick bit of code\n\n\nCode\nhist(rnorm(n = 1000, mean = 0, sd = 1))\n\n\n\n\n\nNow some modelling nitty gritty\nWe need to tell brms how many iterations to run the model for, how many of these iterations we wish to use as warmup, how many chains to run and how many of your computers cores you want to use.\nFinally, because these models can be slow (this one will be fast but it’s good practice), you can use the file option to save the model output in your working directory and automatically load it whenever you rerun the code.\nHere is the full model:\n\n\nCode\nOur_mass_model_small <-\n  brm(Mass ~ 1 + Days,\n    family = gaussian,\n    data = sim_growth_data_small,\n    prior = c(prior(normal(50, 10), class = Intercept),\n              prior(lognormal(-1, 1), class = b, lb = 0),\n              prior(exponential(1), class = sigma)),\n    iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1,\n    file = \"fits/Our_mass_model_small_2\")\n\n\nYou can view the model output easily\n\n\nCode\nOur_mass_model_small\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Mass ~ 1 + Days \n   Data: sim_growth_data_small (Number of observations: 200) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    50.17      0.82    48.50    51.81 1.00     7722     5774\nDays          0.24      0.01     0.21     0.27 1.00     7771     5542\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.75      0.28     5.23     6.34 1.00     7593     5623\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRe-run on the bigger dataset\n\n\nCode\nOur_mass_model_large <-\n  brm(Mass ~ Days,\n    family = gaussian,\n    data = sim_growth_data_large,\n    prior = c(prior(normal(50, 10), class = Intercept),\n              prior(lognormal(-1, 1), class = b, lb = 0),\n              prior(exponential(1), class = sigma)),\n    iter = 4000, warmup = 2000, chains = 4, cores = 4,\n    file = \"fits/Our_mass_model_large\")\n\n\nAnd view output\n\n\nCode\nOur_mass_model_large\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Mass ~ Days \n   Data: sim_growth_data_large (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    49.80      0.27    49.28    50.33 1.00     8877     6133\nDays          0.25      0.00     0.24     0.26 1.00     9296     5883\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.97      0.10     5.79     6.16 1.00     8499     5963\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "simulating_and_modelling.html#plot-the-posterior",
    "href": "simulating_and_modelling.html#plot-the-posterior",
    "title": "Simulating data and modelling it in brms",
    "section": "Plot the posterior",
    "text": "Plot the posterior\nNow we want to plot the model predictions - that is, for a given Day, what is the predicted mass of an individual?\nWe can use two techniques, the easiest of which uses fitted\nFirst we need to create a new dataset with the combinations we want to predict from the model\n\n\nCode\nnew_data <- sim_growth_data_small %>% distinct(Days)\n\n\nNow lets fit the model predictions\n\n\nCode\nModel_predictions <-\n  fitted(Our_mass_model_small, newdata = new_data) %>% \n  bind_cols(new_data)\n\n\nLet’s plot these\n\n\nCode\np3 <-\n  ggplot(data = sim_growth_data_small, \n         aes(x = Days)) +\n  geom_point(aes(y = Mass),\n             color = met.brewer(name = \"Hokusai3\")[4], shape = 1.5, stroke =2, size = 2.5, alpha = 0.9) +\n  geom_smooth(data = Model_predictions,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  labs(y = \"Mass\") +\n  theme_classic() +\n  #coord_cartesian(xlim = range(-4, 4),\n  #               ylim = range(Kalahari_data$weight)) +\n  theme(text = element_text(size = 16),\n        panel.grid = element_blank())\n\np3\n\n\n\n\n\nWhat if we wanted to plot the distribution of the intercept?\nWe could get the prediction from fitted when Days = 0 or we can use the as_draws_df function\n\n\nCode\nOur_mass_model_small %>% \n  as_draws_df()\n\n\n# A draws_df: 2000 iterations, 4 chains, and 5 variables\n   b_Intercept b_Days sigma lprior lp__\n1           51   0.22   6.0   -9.6 -645\n2           52   0.22   5.9   -9.5 -645\n3           52   0.21   5.4   -9.0 -647\n4           48   0.28   6.1   -9.7 -647\n5           51   0.24   5.8   -9.5 -645\n6           50   0.24   5.2   -8.7 -645\n7           50   0.25   6.1   -9.7 -644\n8           51   0.23   5.6   -9.3 -645\n9           51   0.23   5.5   -9.1 -645\n10          49   0.24   6.0   -9.5 -646\n# ... with 7990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nThis gives us 2000 values for all the parameters estimated by the model - when plotted together these give us posterior distributions. Let’s do so for the Intercept.\n\n\nCode\np4 <- \n  Our_mass_model_small %>% \n  as_draws_df() %>% \n\n  ggplot(aes(x = b_Intercept)) +\n    stat_halfeye(fill = met.brewer(\"Hokusai3\")[2], .width = c(0.66, 0.95), alpha = 1,\n               point_interval = \"mean_qi\", point_fill = \"white\", \n               shape = 21, point_size = 4, stroke = 1.5, scale = 0.8) +\n  labs(x= \"Mass at day zero (the intercept)\", y = NULL) +\n  theme_classic() + \n  coord_cartesian(xlim = c(45, 55)) +\n  theme(panel.background = element_rect(fill='transparent'), #transparent panel bg\n        plot.background = element_rect(fill='transparent', color=NA), #transparent plot bg\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\", #transparent legend panel\n        text = element_text(size=16))\n\np4\n\n\n\n\n\nFinally, we can combine the plot to make them look nice\n\n\nCode\np3 + p4"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html",
    "href": "unimelb_statistical_rethinking_2023.html",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "",
    "text": "Code\nlibrary(tidyverse) # for tidy coding\nlibrary(brms) # for fitting stan models\nlibrary(patchwork) # for combining plots\nlibrary(ggdag) # drawing dags\nlibrary(tidybayes) # for bayesian data visualisation\nlibrary(bayesplot) # more bayes data vis\nlibrary(MetBrewer) # colours \nlibrary(ggrepel) # for nice text on ggplots\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#dags-directed-acyclic-graphs",
    "href": "unimelb_statistical_rethinking_2023.html#dags-directed-acyclic-graphs",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "DAGs (Directed acyclic graphs)",
    "text": "DAGs (Directed acyclic graphs)\n\\(~\\)\n\nHeuristic causal models that clarify scientific thinking.\nWe can use these to produce appropriate statistical models.\nThese help us think about the science before we think about the data.\nHelps with questions like “what variables should we include in an analysis?”\nAn integral part of the course that will come up over and over again.\n\nAn example:\nLets make a function to speed up the DAG making process. We’ll use this a lot\n\n\nCode\n# Lets make a function to speed up the DAG making process. We'll use this a lot\n\ngg_simple_dag <- function(d) {\n  \n  d %>% \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(color = met.brewer(\"Hiroshige\")[4]) +\n    geom_dag_text(color = met.brewer(\"Hiroshige\")[7]) +\n    geom_dag_edges() + \n    theme_dag()\n}\n\ndagify( X ~ A + C,\n        C ~ A + B,\n        Y ~ X + C + B,  \n        coords = tibble(name = c(\"A\", \"C\", \"B\", \"X\", \"Y\"),\n                         x = c(1, 2, 3, 1, 3),\n                         y = c(2, 2, 2, 1, 1))) %>% \n  gg_simple_dag()\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#golems",
    "href": "unimelb_statistical_rethinking_2023.html#golems",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Golems",
    "text": "Golems\n\\(~\\)\nStatistical models are akin to Golems - clay robots brought to life by magic to follow specific tasks. The trouble is, they follow tasks extremely literally and are blind to their creators intent, so even those born out of the purest of intentions can cause great harm. Models are another form of robot; if we apply models within the wrong context they will not help us at all.\nEcological models are rarely designed to falsify null-hypotheses. This is because there are many possible non-null models or put another way there are no true null models in many systems. This is problematic because null models underlie how the majority of biologists do statistics!\nA more appropriate question is to ask how multiple process models that we can identify are different.\n\nShould falsify the explanatory model, not the null model. Make predictions and try and falify those. Karl Popper\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#owls",
    "href": "unimelb_statistical_rethinking_2023.html#owls",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Owls",
    "text": "Owls\n\\(~\\)\nSatistical modelling explanations often provide a brief introduction, then leave out many of the important details e.g. step 1: draw two circles, step 2: draw the rest of the fking owl.\nWe shall have a specific workflow for fitting models that we will document\nDrawing the owl, or the scientific workflow can be broken down into 5 steps:\n\nTheoretical estimand: what are you trying to do in your study in the first place?\nSome scientific causal model should be identified: step 1 will be precisely defined in the context of a causal model.\nUse 1 and 2 to build a statistical model.\nSimulate the scientific casual model to validate that the statistical model from step 3 yields the theoretical estimand i.e. check that our stats model works.\nAnalyse the real data. Note that the real data are only introduced now.\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#globe-tossing",
    "href": "unimelb_statistical_rethinking_2023.html#globe-tossing",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Globe tossing",
    "text": "Globe tossing\nTo estimate the proportion of water on planet earth, we collect data by tossing a globe 10 times and record whether our left index finger lands on land or water.\nOur results are shown in the code below\n\n\nCode\nglobe_toss_data <- tibble(toss = c(\"l\", \"w\", \"l\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"w\")) \n\nglobe_toss_data\n\n\n# A tibble: 10 × 1\n   toss \n   <chr>\n 1 l    \n 2 w    \n 3 l    \n 4 l    \n 5 w    \n 6 w    \n 7 w    \n 8 l    \n 9 w    \n10 w    \n\n\nRemember the owl workflow:\n\nDefine generative model of sample.\nDesign estimand.\nUse 1 and 2 to build a statistical model.\nTest (3) using (1).\nAnalyse sample and summarise.\n\n\\(~\\)\nStep 1\nSome true proportion of water, \\(p\\).\nWe can measure this indirectly using:\n\n\\(N\\): the number of globe tosses\n\\(W\\): the number of water observations\n\\(L\\): the number of land observations\n\n\n\nCode\n#Put dag here\n\n#N affects L and W but not the other way around\n\n# P also influences W and L\n\n\nBayesian data analysis\nFor each possible explanation of the data, count all the ways data can happen. Explanations with more ways to produce the data are more plausible (this is entropy).\nToss globe, probability \\(p\\) of observing \\(W\\), \\(1 - p\\) of \\(L\\).\nEach toss is random because it is chaotic (there are forces that could be theoretically measured i.e. velocity, exact starting orientation, but we are not equipped to measure these in real time, so the process appears random).\nEach toss is independent of one another.\nWhat are the relative number of ways we could get the data we actually got, given the process that generates all the possible datasets that could’ve been born from the process?\n\\(~\\)\n\nA 4 sided globe (dice)\nIn Bayesian analysis: enumerate all possible outcomes.\n\n\nCode\n# code land as 0 and water as 1 and create possibility data\n# create the dataframe (tibble)\n\nd <- tibble(p_1 = 0,\n            p_2 = rep(1:0, times = c(1, 3)),\n            p_3 = rep(1:0, times = c(2, 2)),\n            p_4 = rep(1:0, times = c(3, 1)),\n            p_5 = 1)\n\nd %>% \n  gather() %>% \n  mutate(x = rep(1:4, times = 5),\n         possibility = rep(1:5, each = 4)) %>% \n  \n  ggplot(aes(x = x, y = possibility, \n             fill = value %>% as.character())) +\n  geom_point(shape = 21, size = 9) +\n  scale_fill_manual(values = c(\"white\", \"navy\")) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(.75, 4.25),\n                  ylim = c(.75, 5.25)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        text = element_text(size = 18))    \n\n\n\n\n\nThe data: the dice is rolled three times: water, land, water.\nConsider all possible answers and how they would occur.\ne.g. if we hypothesise that 25% of the earth is covered by water, what are all the possible ways to produce our sample?\n\n\nCode\n# create a tibble of all the possibilities per marble draw, with columns position (where to appear on later figures x axis), draw (what number draw is it? and how many for each number? where to appear on figures y axis) and fill (colour of ball for figure)\n\nd <- tibble(position = c((1:4^1) / 4^0, \n                         (1:4^2) / 4^1, \n                         (1:4^3) / 4^2),\n            draw     = rep(1:3, times = c(4^1, 4^2, 4^3)),\n            fill     = rep(c(\"b\", \"w\"), times = c(1, 3)) %>% \n              rep(., times = c(4^0 + 4^1 + 4^2)))\n\n# we wish to make a path diagram, for which we will need connecting lines, create two more tibbles for these\n\nlines_1 <- tibble(x    = rep((1:4), each = 4),\n                  xend = ((1:4^2) / 4),\n                  y    = 1,\n                  yend = 2)\n\nlines_2 <- tibble(x    = rep(((1:4^2) / 4), each = 4),\n                  xend = (1:4^3) / (4^2),\n                  y    = 2,\n                  yend = 3)\n\n# We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that.\n\nd <- d %>% \n  mutate(denominator = ifelse(draw == 1, .5,\n                              ifelse(draw == 2, .5 / 4,\n                                     .5 / 4^2))) %>% \n  mutate(position    = position - denominator)\n\nlines_1 <- lines_1 %>% \n  mutate(x    = x - .5,\n         xend = xend - .5 / 4^1)\n\nlines_2 <- lines_2 %>% \n  mutate(x    = x - .5 / 4^1,\n         xend = xend - .5 / 4^2)\n\n# create the plot, using geom_segment to add the lines - note coord_polar() which gives th eplot a globe-like effect. scale_x_continuous and the y equivalent have been used to remove axis lables and titles\n\nd %>% \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data  = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               size  = 1/3) +\n  geom_segment(data  = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               size  = 1/3) +\n  geom_point(aes(fill = fill),\n             shape = 21, size = 4) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +\n  theme_minimal() +\n  theme(panel.grid      = element_blank(),\n        legend.position = \"none\") +\n  coord_polar()\n\n\n\n\n\nPrune the number of possible outcomes down to those that are consistent with the data.\ne.g. there are 3 paths consistent with our dice rolling experiment for the given data and the given hypothesis.\n\n\nCode\nlines_1 <-\n  lines_1 %>% \n  mutate(remain = c(rep(0:1, times = c(1, 3)),\n                    rep(0,   times = 4 * 3)))\n\nlines_2 <-\n  lines_2 %>% \n  mutate(remain = c(rep(0,   times = 4),\n                    rep(1:0, times = c(1, 3)) %>% \n                      rep(., times = 3),\n                    rep(0,   times = 12 * 4)))\n\nd <-\n  d %>% \n  mutate(remain = c(rep(1:0, times = c(1, 3)),\n                    rep(0:1, times = c(1, 3)),\n                    rep(0,   times = 4 * 4),\n                    rep(1:0, times = c(1, 3)) %>% \n                      rep(., times = 3),\n                    rep(0,   times = 12 * 4))) \n\n# finally, the plot:\nd %>% \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data  = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain %>% as.character()),\n               size  = 1/3) +\n  geom_segment(data  = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain %>% as.character()),\n               size  = 1/3) +\n  geom_point(aes(fill = fill, alpha = remain %>% as.character()),\n             shape = 21, size = 4) +\n  # it's the alpha parameter that makes elements semitransparent\n  scale_alpha_manual(values = c(1/10, 1)) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +\n  theme_minimal() +\n  theme(panel.grid      = element_blank(),\n        legend.position = \"none\") +\n  coord_polar()\n\n\n\n\n\nIs 3 ways to produce the sample big or small, to find out, compare with other possibilities.\ne.g. lets make another conjecture - all land or all water - we have a land and water observation so there are zero ways that the all land/water hypotheses are consistent with the data.\n\n\nCode\n# if we make two custom functions, here, it will simplify the code within `mutate()`, below\nn_water <- function(x){\n  rowSums(x == \"W\")\n}\n\nn_land <- function(x){\n  rowSums(x == \"L\")\n}\n\nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"L\", \"W\"), times = c(1, 4)),\n         p_2 = rep(c(\"L\", \"W\"), times = c(2, 3)),\n         p_3 = rep(c(\"L\", \"W\"), times = c(3, 2)),\n         p_4 = rep(c(\"L\", \"W\"), times = c(4, 1))) %>% \n  mutate(`roll 1: water`  = n_water(.),\n         `roll 2: land` = n_land(.),\n         `roll 3: water`  = n_water(.)) %>% \n  mutate(`ways to produce` = `roll 1: water` * `roll 2: land` * `roll 3: water`)\n\nt %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_1\np_2\np_3\np_4\nroll 1: water\nroll 2: land\nroll 3: water\nways to produce\n\n\n\n\nL\nL\nL\nL\n0\n4\n0\n0\n\n\nW\nL\nL\nL\n1\n3\n1\n3\n\n\nW\nW\nL\nL\n2\n2\n2\n8\n\n\nW\nW\nW\nL\n3\n1\n3\n9\n\n\nW\nW\nW\nW\n4\n0\n4\n0\n\n\n\n\n\nUnglamorous basis of applied probability: Things that can happen more ways are more plausible.\nThis is Bayesian inference. Given a set of assumptions (hypotheses) the number of ways the numbers could have occurred accoridng to those assumptions (hypotheses) is the posterior probability distribution.\nBut we don’t really have enough evidence to be confident in a prediction. So lets roll the dice again. In bayes world a process called Bayesian updating exists, that saves you from running the draws again, in favour of just updating previous counts (or data). Bayesian updating is simple multiplication e.g. we roll another water, for the 3 water hypothesis there are 3 paths for this to occur so we multiply 9 x 3, resulting in 27 possible paths consistent with the new dataset.\nEventually though, the garden gets really big. This is where your computer comes in and it starts to make more sense to work with probabilities rather than counts.\n\n\nCode\nW <- sum(globe_toss_data == \"w\")\nL <- sum(globe_toss_data == \"l\")\np <- c(0, 0.25, 0.5, 0.75, 1) # proportions W\nways <- sapply(p, function(q) (q*4)^W * ((1 - q)*4)^L)\nprob <- ways/sum(ways)\n\nposterior <- cbind(p, ways, prob) %>% \n  as_tibble() %>% \n  mutate(p = as.character(p))\n\nposterior %>% \nggplot(aes(x = p, y = prob)) +\n  geom_col() +\n  labs(x = \"proportion water\", y = \"probability\") +\n  theme_minimal()\n\n\n\n\n\n\\(~\\)\n\n\nTest before you est\n\nCode a generative simulation\nCode an estimator\nTest (2) with (1)\n\nBuild a simulation\n\n\nCode\nsim_globe <- function(p = 0.7, N =9) {\n  sample(c(\"W\", \"L\"), size = N, prob = c(p, 1-p), replace = TRUE)\n}\n\n# W and L are the possible observations\n\n# N is number of tosses\n\n# prob is the probabilities of water and land occurring\n\n\nThe simulation does this:\n\n\nCode\nsim_globe()\n\n\n[1] \"W\" \"L\" \"W\" \"W\" \"W\" \"W\" \"L\" \"W\" \"W\"\n\n\nNow lets test it on extreme settings\ne.g. all water\n\n\nCode\nsim_globe(p=1, N = 11)\n\n\n [1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\n\nLooks good\nWe can also test how close the proportion of water produced in the simulation is to the specified p\n\n\nCode\nsum(sim_globe(p=0.5, N = 1e4) == \"W\") / 1e4\n\n\n[1] 0.4864\n\n\nAlso looks good.\nSo based upon our generative model:\nWays for \\(p\\) to produce $W, L = (4p)^W * (4 - 4p)^L $\n\n\nCode\n# function to compute posterior distirbution\n\ncompute_posterior <- function(the_sample, poss = c(0, 0.25, 0.5, 0.75, 1)) {\n  W <- sum(the_sample == \"W\") # the number of W observed\n  L <- sum(the_sample == \"L\") # the number of L observed\n  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post <- ways/sum(ways)\n  #bars <- sapply(post, function(q) make_bar(q))\n  tibble(poss, ways, post = round(post, 3))#, bars)\n}\n\n\nWe can then simulate the experiment many times with sim_globe\n\n\nCode\ncompute_posterior(sim_globe())\n\n\n# A tibble: 5 × 3\n   poss  ways  post\n  <dbl> <dbl> <dbl>\n1  0        0 0    \n2  0.25    27 0.021\n3  0.5    512 0.404\n4  0.75   729 0.575\n5  1        0 0    \n\n\nThis allows us to check that our model is doing what we think it is.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#an-infinite-number-of-possibilties",
    "href": "unimelb_statistical_rethinking_2023.html#an-infinite-number-of-possibilties",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "An infinite number of possibilties",
    "text": "An infinite number of possibilties\nGlobes aren’t dice. There are an infinite number of possible proportions of the earth covered in water.\nTo get actual infinity we can turn to math.\nThe relative number of ways that nay value of \\(p\\) could produce any sample of W and L observations. This is a well know distribution called the binomial distribution\n\\[p^W(1-p)^L \\]\nThe only trick is normalising to make it a probability. A little calculus is needed (this produces the beta distirbution:\n\\[Pr(W, L|P) = \\frac{(W + L)!}{W!L!}p^{W}(1 - p)^{L}\\]\nNote the normalising constant \\(\\frac{(W + L)!}{W!L!}\\)\nWe can use the binomial sampling formula to give us the number of paths through the garden of forking data for this particular problem. That is given some value of P (akin to some number of blue marbles in the bag), the number of ways to see W and L can be worked out using the expression above.\nWe can use R to calculate this, assuming 6 globe tosses that landed on water out of 10 possible tosses and that P = 0.7:\n\n\nCode\ndbinom(6, 10, 0.7)\n\n\n[1] 0.2001209\n\n\nThis is the relative number of ways that 6 out of 10 can happen given a value of 0.7 for p. For this to be meaningful, we need to work out a probability value for many other values of P. This gives our probability distribution.\nLets plot how bayesian updating works, as we add observations\n\n\nCode\n# add the cumulative number of trials and successes for water to the dataframe.\n\nglobe_toss_data <- globe_toss_data %>% mutate(n_trials = 1:10, \n                                              n_success = cumsum(toss == \"w\"))\n\n# Struggling to follow this code, awesome figure produced though\n\nsequence_length <- 50\n\nglobe_toss_data %>%\n  expand(nesting(n_trials, toss, n_success),\n         p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>%\n  group_by(p_water) %>%\n  mutate(lagged_n_trials = lag(n_trials, k = 1),\n         lagged_n_success = lag(n_success, k =1)) %>%\n  ungroup() %>%\n  mutate(prior = ifelse(n_trials == 1, .5,\n                        dbinom(x = lagged_n_success,\n                               size = lagged_n_trials,\n                               prob = p_water)),\n         likelihood = dbinom(x = n_success,\n                             size = n_trials,\n                             prob = p_water),\n         strip = str_c(\"n = \", n_trials)\n         ) %>%\n  # the next three lines allow us to normalize the prior and the likelihood, \n  # putting them both in a probability metric\n  group_by(n_trials) %>%\n  mutate(prior = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) %>%\n  # plot time\n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior), linetype = 2) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = c(0, 0.5, 1)) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~n_trials, scales = \"free_y\")\n\n\n\n\n\nPosterior is continually updated as data points are added.\n\nEach posterior is simply a multiplication of a set of diagonal lines, like that shown in the first panel. Whether the slope of the line is pos or neg depends on whether the observation is land or water.\nEvery posterior is a prior for the next observation, but the data order doesn’t make a difference, in that the posterior will always be the same for a given dataset. BUT this assumption can be violated if each observation is not independent.\nSample size is embodied within the shape of the posterior. Already been accounted for.\n\nSome big things to grasp about bayes\n\nNo minimum sample size, because we have something called a prior. Note that estimation sucks with small samples, but that’s good! The model doesn’t draw too much from too little.\nShape of distribution embodies sample size\nThere are no point estimates e.g. means or medians. Everything is a distribution. You will never need to bootstrap again.\nThere is no one true interval - they just communicate the shape of the posterior distribution. No magical number e.g. 95%.\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#from-posterior-to-prediction-brms-time",
    "href": "unimelb_statistical_rethinking_2023.html#from-posterior-to-prediction-brms-time",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "From posterior to prediction (brms time)",
    "text": "From posterior to prediction (brms time)\nHere is a nice point to introduce brms the main modelling package that I use.\nLet’s fit a globe tossing model, where we observe 24 water observations from 36 tosses.\n\n\nCode\nb1.1 <-\n  brm(data = list(w = 24), \n      family = binomial(link = \"identity\"),\n      w | trials(36) ~ 0 + Intercept,\n      #prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      seed = 2,\n      file = \"fits/b02.01\")\n\nb1.1\n\n\n Family: binomial \n  Links: mu = identity \nFormula: w | trials(36) ~ 0 + Intercept \n   Data: list(w = 24) (Number of observations: 1) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.66      0.08     0.51     0.80 1.00     1681     1947\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere’s a lot going on in that output. For now, focus on the ‘Intercept’ line. The intercept of a typical regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial.\nTo use the posterior, we can sample from it using the as_draws_df function.\n\n\nCode\nas_draws_df(b1.1) %>% \n  mutate(n = \"n = 36\") %>%\n  \n  ggplot(aes(x = b_Intercept)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion water\", limits = c(0, 1)) +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16)) +\n  facet_wrap(~ n)\n\n\n\n\n\nAnother way to do this is to use the fitted function\n\n\nCode\nf <-\n  fitted(b1.1, \n         summary = F,\n         scale = \"linear\") %>% \n  data.frame() %>% \n  set_names(\"proportion\") %>% \n  as_tibble()\n\nf\n\n\n# A tibble: 4,000 × 1\n   proportion\n        <dbl>\n 1      0.705\n 2      0.562\n 3      0.584\n 4      0.667\n 5      0.694\n 6      0.706\n 7      0.654\n 8      0.684\n 9      0.682\n10      0.658\n# … with 3,990 more rows\n\n\nPlot the distribution\n\n\nCode\nf %>% \n  ggplot(aes(x = proportion)) +\n  geom_density(fill = \"grey50\", color = \"grey50\") +\n  annotate(geom = \"text\", x = .08, y = 2.5,\n           label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                     breaks = c(0, .5, 1),\n                     limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nStrikingly similar (well, exactly the same).\nWe can use this distribution of probabilities to predict histograms of water counts.\n\n\nCode\n# the simulation\nset.seed(3) # so we get the same result every time\n\nf <-\n  f %>% \n  mutate(w = rbinom(n(), size = 36,  prob = proportion))\n\n# the plot\nf %>% \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"number of water samples\", breaks = 0:40 * 4) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 450)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(8, 36)) +\n  theme_minimal()\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#what-are-linear-models",
    "href": "unimelb_statistical_rethinking_2023.html#what-are-linear-models",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "What are linear models",
    "text": "What are linear models\n\\(~\\)\n\nSimple statistical golems that model the mean and the variance of a variable. That’s it.\nThe mean is sum weighted sum of other variables. As those variables change, the mean and variance changes.\nAnovas, Ancovas, t-tests are all linear models.\n\nThe normal distribution\n\nCounts up all the ways the observations can happen given a set of assumptions.\nGiven some mean and variance the normal distribution gives you the relative number of ways the data can appear.\n\nThe normal distribution is the norm because:\n\nIt is very common in nature.\nIt’s easy to calculate\nVery conservative assumptions (spreads probability out more than any other distribution, reducing the risk of mistake, at the expense of accuracy)\n\nTo understand why the normal distribution is so common consider a soccer pitch mid-line, and individuals flipping coins that dictate whether they should step to the left or the right.\nMany individuals remain close to the line, while a few move towards the extremes. Simply, there are more ways to get a difference of 0 than there is any other result. This creates a bell curve and summarises processes to mean and variance. The path of one individual is shaded black in the figure below.\n\n\nCode\n# lets simulate this scenario\n\nset.seed(4)\n\npos <-\n  replicate(100, runif(16, -1, 1)) %>% # this is the sim\n  as_tibble() %>%\n  rbind(0, .) %>% # add a row of zeros above simulation results\n  mutate(step = 0:16) %>% # creates a step column\n  gather(key, value, -step) %>% # convert data to long format\n  mutate(person = rep(1:100, each = 17)) %>% # person IDs added\n  # the next lines allow us to make cumulative sums within each person\n  group_by(person) %>%\n  mutate(position = cumsum(value)) %>%\n  ungroup() # allows more data manipulation\n\nggplot(data = pos,\n       aes(x = step, y = position, group = person)) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_line(aes(colour = person < 2, alpha = person < 2)) +\n  scale_colour_manual(values = c(\"skyblue3\", \"black\")) +\n  scale_alpha_manual(values = c(1/5, 1)) +\n  scale_x_continuous(\"step number\", breaks = c(0, 4, 8, 12, 16)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nThe density plot for this scenario at step 16:\n\n\nCode\n# first find the sd\n\nsd <-\n  pos %>%\n  filter(step == 16) %>%\n           summarise(sd = sd(position))\n\npos %>%\n  filter(step == 16) %>%\n  ggplot(aes(x = position)) +\n  stat_function(fun = dnorm,\n                args = list(mean = 0, sd = 2.18),\n                linetype = 2) +\n  geom_density(colour = \"transparent\", fill = \"dodgerblue\", alpha = 0.5) +\n  coord_cartesian(xlim = c(-6, 6)) +\n  labs(title = \"16 steps\",\n       y = \"density\")\n\n\n\n\n\nWhy normal? The generative perspective:\n\nNature makes bell curves whenever it adds things together.\nThis dampens fluctuations - large fluctuations are cancelled out by small fluctuations.\nSymmetry arises from the addition process.\n\nInferential perspective:\n\nIf all you know are the mean and the variance, then the least surprising distribution is gaussian.\n\n\nNote: if you just want mean or variance, a variable does not have to be normally distributed for the normal distribution to be useful\n\n\nOh and go with the FLOW. This is hard, you won’t understand everything, but keep flowing forward\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#weekly-owl-reminder",
    "href": "unimelb_statistical_rethinking_2023.html#weekly-owl-reminder",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Weekly owl reminder",
    "text": "Weekly owl reminder\n\nState a clear question\nSketch your causal assumptions\nUse the sketch to define generative model\nUse generative model to build estimator\nProfit (real model)\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#time-for-some-data-kalahari-heights",
    "href": "unimelb_statistical_rethinking_2023.html#time-for-some-data-kalahari-heights",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Time for some data: Kalahari heights",
    "text": "Time for some data: Kalahari heights\n\\(~\\)\nTo get us started with linear regression, lets load in the Kalahari dataset from McElreath’s rethinking package.\n\n\nCode\nlibrary(rethinking)\ndata(Howell1)\nKalahari_data <- as_tibble(Howell1)\n\n\nNow lets detach rethinking, we need to do this so brms always works\n\n\nCode\nrm(Howell1)\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\n\nRight lets have a quick look at the data\n\n\nCode\nKalahari_data\n\n\n# A tibble: 544 × 4\n   height weight   age  male\n    <dbl>  <dbl> <dbl> <int>\n 1   152.   47.8    63     1\n 2   140.   36.5    63     0\n 3   137.   31.9    65     0\n 4   157.   53.0    41     1\n 5   145.   41.3    51     0\n 6   164.   63.0    35     1\n 7   149.   38.2    32     0\n 8   169.   55.5    27     1\n 9   148.   34.9    19     0\n10   165.   54.5    54     1\n# … with 534 more rows\n\n\nAnd we can check out how each variable is distributed like so:\n\n\nCode\nKalahari_data %>% \n  pivot_longer(everything()) %>% \n  mutate(name = factor(name, levels = c(\"height\", \"weight\", \"age\", \"male\"))) %>% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10) +\n  facet_wrap(~ name, scales = \"free\", ncol = 1)\n\n\n\n\n\nThe owl\n1. In this lecture we are going to focus on describing the association between height and weight in adults.\n2. Scientific model: how does height affect weight?\nHeight influences weight, but not the other way around. Height is causal.\n\\(W = f(H)\\) this means that weight is some function of height.\n\n\nCode\ndagify( W ~ H + U,  \n        coords = tibble(name = c(\"H\", \"W\", \"U\"),\n                         x = c(1, 2, 3),\n                         y = c(1, 1, 1))) %>% \n  gg_simple_dag()\n\n\n\n\n\nNote the \\(U\\) in the DAG - an unobserved influence on weight.\n\nBuild a generative model\n\\(W = \\beta H + U\\)\nLets simulate the data\n\n\nCode\nsim_weight <- function(H, b, sd){\n  U <- rnorm(length(H), 0, sd)\n  W <- b*H + U\n  return(W)\n}\n\n\nRun the simulation and plot. We’ll need values for heights, a \\(\\beta\\) value and some value of sd for weight in kgs\n\n\nCode\nsim_data <- tibble(H = runif(200, min = 130, max = 170)) %>% \n  mutate(W = sim_weight(H, b = 0.5, sd = 5))\n\n# plot\n\nsim_data %>% \n  ggplot(aes(x = H, y = W)) +\n   geom_point(color = \"salmon\", shape = 1.5, stroke =2.5, size = 3, alpha = 0.9) +\n  theme_classic() +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme(text = element_text(size = 16))\n\n\n\n\n\nDescribing our model\n\\(W_{i} = \\beta H_{i} + U_i\\) is our equation for expected weight\n\\(U_{i} = Normal(0,\\sigma)\\) is the Gaussian error with sd \\(\\sigma\\)\n\\(H_{i} =\\) Uniform\\((130, 170)\\) means that all values are equally likely for height between 130-170\n\\(i\\) is an index and here represents individuals in the dataset\n= indicates a deterministic relationship\n~ indicates that something is “distributed as”\n\\(~\\)\n\n\nBuild estimator\nA linear model:\n\\(E(W_i|H_i) = \\alpha + \\beta H_{i}\\)\n\\(\\alpha\\) = the intercept\n\\(\\beta\\) = the slope\nOur estimator:\n\\(W_{i}\\) ~ \\(Normal(u_{i},\\sigma)\\)\n\\(u_{i}\\) ~ \\(\\alpha + \\beta H_{i}\\)\nIn words: \\(W\\) is distributed normally with mean that is a linear function of H\n\n\nPriors\nWe can specify these to be very helpful. We simply want to design priors to stop the model hallucinating impossible outcomes e.g. negative weights.\nPriors should express scientific knowledge, but softly. This is because the real process in nature is different to what we imagined and there needs to be room for this.\nSome basic things we know about weight:\n\nWhen height is zero, weight should be zero.\n\n\n\\(\\alpha\\) ~ Normal(0, 10) will achieve this\n\n\nWeight increases with height in humans on average. So \\(\\beta\\) should be positive\nWeight in kgs is less than height in cms, so \\(\\beta\\) should be less than 1\n\n\n\\(\\beta\\) ~ Uniform(0, 1) will achieve this\n\n\n\\(\\sigma\\) must be positive\n\n\n\\(\\sigma\\) ~ Uniform(0, 10)\n\nLets plot these priors\n\n\nCode\nn_lines <- 50\n\ntibble(n = 1:n_lines,\n       a = rnorm(n_lines, 0, 10),\n       b = runif(n_lines, 0, 1)) %>% \n  expand(nesting(n, a, b), height = 130:170) %>% \n  mutate(weight = a + b * (height)) %>%\n  \n  # plot\n  ggplot(aes(x = height, y = weight, group = n)) +\n  geom_line(alpha = 1/1.5, linewidth = 1, colour = met.brewer(\"Hiroshige\")[2]) +\n  scale_x_continuous(expand = c(0, 0)) +\n  coord_cartesian(ylim = c(0, 100),\n                  xlim = c(130, 170)) +\n  theme_classic()\n\n\n\n\n\nWoah, ok the slope looks ok, but the intercept is wild.\nThis is because we set a very high sd value for \\(\\alpha\\)\nWe can fix this, but for a problem like this, the data will overwhelm the prior.\n\\(~\\)\n\n\nBack to the owl: brms time\nLets validate our model\nLet simulate again with our sim_weight() function\n\n\nCode\nsim_data_100 <- \n  tibble(H = runif(100, min = 130, max = 170)) %>% \n  mutate(W = sim_weight(H, b = 0.5, sd = 5))\n\n\nFit the model\n\n\nCode\nweight_synthetic_model <-\n  brm(W ~ 1 + H,\n      family = gaussian,\n      data = sim_data_100,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(uniform(0, 1), class = b, lb = 0, ub = 1),\n                prior(uniform(0, 10), class = sigma, lb = 0, ub = 10)),\n      chains = 4, cores = 4, iter = 6000, warmup = 2000, seed = 1,\n      file = \"fits/weight_synthetic_model\")\n\n\nGet the model summary\n\n\nCode\nweight_synthetic_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: W ~ 1 + H \n   Data: sim_data_100 (Number of observations: 100) \n  Draws: 4 chains, each with iter = 6000; warmup = 2000; thin = 1;\n         total post-warmup draws = 16000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    12.28      6.51    -0.70    25.06 1.00    15740    11165\nH             0.42      0.04     0.34     0.51 1.00    15754    11351\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.91      0.35     4.28     5.67 1.00    14738    10608\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRight so, H is close to 0.5 and sigma is very close to 4.91. These aren’t perfect because we have a smallish sample and relatively wild priors.\n\\(~\\)\nStep 5. Profit (with the real data)\nTime to fit the actual model\n\n\nCode\nKalahari_adults <-\n  Kalahari_data %>% \n  filter(age > 18)\n\nweight_kalahari_model <-\n  brm(weight ~ 1 + height,\n      family = gaussian,\n      data = Kalahari_adults,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(uniform(0, 1), class = b, lb = 0, ub = 1),\n                prior(uniform(0, 10), class = sigma, lb = 0, ub = 10)),\n      chains = 4, cores = 4, iter = 6000, warmup = 2000, seed = 1,\n      file = \"fits/weight_kalahari_model\")\n\nweight_kalahari_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: weight ~ 1 + height \n   Data: Kalahari_adults (Number of observations: 346) \n  Draws: 4 chains, each with iter = 6000; warmup = 2000; thin = 1;\n         total post-warmup draws = 16000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -51.66      4.55   -60.54   -42.77 1.00    16472    11740\nheight        0.63      0.03     0.57     0.68 1.00    16478    11912\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.27      0.16     3.97     4.60 1.00    15291    11571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\\(~\\)\n\n\nOBEY THE LAW\n\\(~\\)\nLaw 1. Parameters are not independent of one another and cannot always be interpreted independently. They all act simultaneously on predictors (think about this from the context of the fitted() function).\nInstead we can push out posterior predictions from the model and describe/interpret those.\n\nPlot the sample\nPlot the posterior mean\nPlot the uncertainty of the mean\nPlot the uncertainty of predictions\n\n\n\nCode\n# use fitted to get posterior predictions\n\nheight_seq <- \n  tibble(height = 135:180) %>% \n  mutate(height_standard = height - mean(Kalahari_adults$height))\n\n# add uncertainty intervals\n\nmu_summary <-\n  fitted(weight_kalahari_model,\n         newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq)\n\n# add prediction intervals\n\npred_weight <-\n  predict(weight_kalahari_model,\n          newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq)\n\n# make plot \n\nKalahari_adults %>%\n  ggplot(aes(x = height)) +\n   geom_ribbon(data = pred_weight, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"salmon\", alpha = 0.5) +\n  geom_point(aes(y = weight), color = \"salmon\", shape = 1.5, stroke =2, size = 1.5, alpha = 0.9) +\n  geom_smooth(data = mu_summary,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"salmon\", color = \"black\", alpha = 0.7, size = 1/2) +\n  coord_cartesian(xlim = range(Kalahari_adults$height)) +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme_classic() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nThis plot shows 1) the raw data, 2) the predicted relationship between height and weight with 3) 95% uncertainty intervals for the mean and 4) 95% prediction intervals for where data points are predicted to fall within.\nPredict() reports prediction intervals, which are simulations that are the joint consequence of both the mean and sigma, unlike the results of fitted(), which only reflect the mean.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#categories",
    "href": "unimelb_statistical_rethinking_2023.html#categories",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Categories",
    "text": "Categories\nWant to stratify by categories in our data. This means fitting a separate regression line for each category.\nLets return to the Kalahari data. Now we’ll add in a categorical variable - the sex of the individual.\n\n\nCode\nKalahari_adults %>%\n  ggplot(aes(x = height, colour = as.factor(male))) +\n  geom_point(aes(y = weight), shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +\n  coord_cartesian(xlim = range(Kalahari_adults$height)) +\n  scale_colour_manual(values = c(\"salmon\", \"darkcyan\")) +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\nThink scientifically first:\nHow are height, sex and weight causally associated?\nHow are height, sex and weight statistically related?\nLets build a DAG:\n\nFirst, we know that height causally effects weight. You can change your own weight without changing your height. However, as you get taller there is more of you, thus it is reasonable to expect height to affect weight.\nWe also know that sex influences height, but it is silly to say that height influences sex.\nThird, if there is any influence we expect sex to influence weight not the other way around. Therefore weight may be influenced by both height and sex.\n\nLets draw this DAG\n\n\nCode\ndagify(W ~ S + H,\n       H ~ S,\n       labels = c(\"W\" = \"Weight\", \n                  \"H\" = \"Height\",\n                  \"S\" = \"Sex\")) %>% \n  gg_simple_dag()\n\n\n\n\n\nThis is a mediation graph. Note that effects do not stop at one variable, instead they continue on through the path. In that way sex has both a direct and indirect effect on weight. The indirect effect may be through height, which is ‘contaminated’ by sex.\nStatistically:\n\\(H = f_{H}(S)\\)\n\\(W = f_{W}(H, S)\\)\nFollowing our workflow lets simulate a more complex dataset, that this time includes separate sexes.\nTo keep in line with the Kalahari data we’ll code females = 1 and males = 2\n\n\nCode\nsim_HW <- function(S, b, a){\n  N <- length(S)\n  H <- if_else(S == 1, 150, 160) + rnorm(N, 0, 5)\n  W <- a[S] + b[S]*H + rnorm(N, 0, 5)\n  tibble(S, H, W)\n}\n\n\nGive it some data and run sim_HW()\n\n\nCode\nS <- rethinking::rbern(100) + 1\n\n(synthetic_data <- sim_HW(S, b = c(0.5, 0.6), a = c (0, 0)))\n\n\n# A tibble: 100 × 3\n       S     H     W\n   <dbl> <dbl> <dbl>\n 1     2  161.  98.8\n 2     1  156.  81.0\n 3     1  148.  65.0\n 4     2  161.  98.2\n 5     1  148.  80.3\n 6     1  156.  75.2\n 7     1  142.  71.9\n 8     2  155.  94.0\n 9     2  162.  95.8\n10     2  156.  95.1\n# … with 90 more rows\n\n\nDefine the questions we’re going to ask:\nDifferent questions lead to a need for different stats models.\nQ: Causal effect of H on W?\nQ: Causal effect of S on W?\nQ: Direct effect of S on W?\nEach require different components of the DAG.\nDrawing the categorical OWL:\nThere are several ways to code categorical variables\n\nDummy or indicator variables\n\n\nSeries of 0 1 variables that stand in for categories\n\n\nIndex variables\n\n\nAssign an index value to each category\nBetter for specifying priors\nExtend effortlessly to multi-level models\nWhat we will use\n\n\\(~\\)\nQ: What is the causal effect of S on W?\n\\(~\\)\nUsing index variables\nEstimating average weight:\n\\(W_{i} = Normal(\\mu_{i}, \\sigma)\\)\n\\(\\mu_{i} = \\alpha S[i]\\) where \\(S[i]\\) is the sex of the i-th person\nS = 1 indicates female, S = 2 indicates male\n\\(\\alpha = [\\alpha_{i}, \\alpha_{2}]\\) this means there are two intercepts, one for each sex\nPriors\n\\(\\alpha_{j} = Normal(60, 10)\\)\nAll this says is that there is more than one value of (indicated by the subscript j) and that we want the prior to be the same for each of the values. Values correspond to the categories.\nWe’ll let the sample update the prior and tell us if sexual dimorphism exists\nRight ready to model. Not yet! More simulation. This might seem like overkill but it will help so much to get into this habit.\nWe’ll construct a female and male sample and look at the average difference. We’ll only change sex.\nWe’ll find the difference between the sexes in our simulation (remember we coded a stronger effect of height on male weight than on female weight):\n\n\nCode\n# female sample\n\nS <- rep(1, 100)\n\nsimF <- sim_HW(S, b = c(0.5, 0.6), a = c(0, 0))\n\nS <- rep(2, 100)\n\nsimM <- sim_HW(S, b = c(0.5, 0.6), a = c(0, 0))\n\n# effect of Sex (male - female)\n\nmean(simM$W - simF$W)\n\n\n[1] 19.53385\n\n\nOk a difference of ~21kgs\nNow lets test the estimator.\nNote that we have specified a 0 intercept. In brms this will result in an output that produces a separate intercept for each categorical variable.\n\n\nCode\nsynthetic_kalahari_sex_model <-\n  brm(data = synthetic_data,\n      W ~ 0 + as.factor(S),\n      prior = c(prior(normal(60, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 1,\n      file = \"fits/synthetic_kalahari_sex\")\n\nsynthetic_kalahari_sex_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: W ~ 0 + as.factor(S) \n   Data: synthetic_data (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nas.factorS1    75.69      0.82    74.13    77.31 1.00     4038     2899\nas.factorS2    96.13      0.74    94.69    97.63 1.00     4126     2946\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.50      0.36     4.86     6.29 1.00     4199     2984\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAgain we find a difference between the sexes ~21kgs. Looks like our model tests what we want to test.\nNow lets use the real data\n\n\nCode\n# make the index variable to match McElreath's dataset\n\nKalahari_adults <-\n  Kalahari_adults %>% \n  mutate(Sex = if_else(male == \"1\", 2, 1),\n         Sex = as.factor(Sex))\n\nkalahari_sex_model <-\n  brm(data = Kalahari_adults,\n      weight ~ 0 + Sex,\n      prior = c(prior(normal(60, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 1,\n      file = \"fits/_kalahari_sex_model\")\n\n\nFind the contrasts and plot the average differences between women and men\n\n\nCode\ndraws <- as_draws_df(kalahari_sex_model)\n\nas_draws_df(kalahari_sex_model) %>% \n  mutate(`Mean weight contrast` = b_Sex2 - b_Sex1) %>% \n  rename(Female = b_Sex1,\n         Male = b_Sex2) %>% \n  pivot_longer(cols = c(\"Female\", \"Male\", \"Mean weight contrast\")) %>% \n  \n  ggplot(aes(x = value, y = 0)) +\n    stat_halfeye(.width = 0.95,\n                 normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"Posterior mean weights (kgs)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\nWhat about the posterior predictive distribution, not just the mean?\n\n\nCode\nFemale_dist <- rnorm(1000, draws$b_Sex1, draws$sigma) %>% \n  as_tibble() %>% \n  rename(Female = value)\n\nMale_dist <- rnorm(1000, draws$b_Sex2, draws$sigma) %>% \n  as_tibble() %>% \n  rename(Male = value)\n\nplot_1 <-\n  cbind(Female_dist, Male_dist) %>% \n  pivot_longer(cols = c(\"Female\", \"Male\")) %>% \n    \n  ggplot(aes(x = value, group = name, colour = name, fill = name)) +\n  geom_density(alpha = 0.8) +\n  scale_colour_manual(values = c(\"salmon\", \"darkcyan\")) +\n  scale_fill_manual(values = c(\"salmon\", \"darkcyan\")) +\n  xlab(\"Posterior predicted weights (kgs)\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n \n\nplot_2 <- \ncbind(Female_dist, Male_dist) %>%\n  as_tibble() %>% \n  mutate(predictive_diff = Male - Female) %>% \n  \n  ggplot(aes(x = predictive_diff, colour = \"orange\", fill = \"orange\")) +\n  stat_slab(aes(fill = after_stat(x > 0), colour = after_stat(x > 0)), alpha = 0.8) +\n  xlab(\"Posterior predictive weight difference (kgs)\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\nplot_1 / plot_2\n\n\n\n\n\nThe takeaway here is that there are lots of women that are heavier than men, even though the difference between the means is large.\nWe need to calculate the contrast, or difference between the categories (as we have shown above).\n\nIt is never legitimate to compare overlap in parameters\nWhat you do is compute the distribution of the difference (as shown above)\n\n\\(~\\)\nQ: What is the direct causal effect of S on W?\n\\(~\\)\nNow we must add Height into the model\n\\(W_{i} = Normal(\\mu_{i}, \\sigma)\\)\n\\(\\mu_{i} = \\alpha_{S[i]} + \\beta_{S[i]}(H_{i} - \\overline{H})\\)\nNow we have two intercepts and two slopes!\nCentering\n\nCentering H makes it so that \\(\\alpha\\) is the average weight of a person with average height\nEasy to fit priors for \\(alpha\\)\nLinear regressions build lines that pass through this point (the grand mean)\n\n\n\nCode\nKalahari_adults <-\n  Kalahari_adults %>%\n   mutate(height_standard = height - mean(height))\n\n\nLets model\n\n\nCode\nKalahari_h_S_model <-\n  brm(data = Kalahari_adults,\n      weight ~ 0 + Sex + height_standard,\n      prior = c(#prior(normal(60, 10), class = a),\n                prior(lognormal(0, 1), class = b, lb = 0),\n                prior(exponential(1), class = sigma)),\n      iter = 4000, warmup = 2000, chains = 4, cores = 4,\n      seed = 1,\n      file = \"fits/Kalahari_h_S_model\")\n\n\nI’m not sure how to make McElreath’s plot here - he brilliantly finds the effect of sex at 50 different heights then plots. The best I can do is the overall effect of sex on weight after accounting for sex.\nThe takeaway however, is that nearly all of the causal effect of sex acts through height.\n\n\nCode\ndraws_2 <- as_draws_df(Kalahari_h_S_model) %>% \n  mutate(weight_contrast = b_Sex2 - b_Sex1)\n\nFemale_dist_2 <- rnorm(1000, draws_2$b_Sex1, draws_2$sigma) %>% \n  as_tibble() %>% \n  rename(Female = value)\n\nMale_dist_2 <- rnorm(1000, draws_2$b_Sex2, draws_2$sigma) %>% \n  as_tibble() %>% \n  rename(Male = value)\n\nplot_1 <-\n  cbind(Female_dist_2, Male_dist_2) %>% \n  pivot_longer(cols = c(\"Female\", \"Male\")) %>% \n    \n  ggplot(aes(x = value, group = name, colour = name, fill = name)) +\n  geom_density(alpha = 0.8) +\n  scale_colour_manual(values = c(\"salmon\", \"darkcyan\")) +\n  scale_fill_manual(values = c(\"salmon\", \"darkcyan\")) +\n  xlab(\"Posterior predicted weights (kgs)\\nafter accounting for the effect of height\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n \n\nplot_2 <- \ncbind(Female_dist, Male_dist) %>%\n  as_tibble() %>% \n  mutate(predictive_diff = Male - Female) %>% \n  \n  ggplot(aes(x = predictive_diff, colour = \"orange\", fill = \"orange\")) +\n  geom_density(alpha = 0.8) +\n  xlab(\"Posterior predictive weight difference (kgs)\\nafter accounting for the effect of height\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\nplot_1 / plot_2\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#curves-with-lines",
    "href": "unimelb_statistical_rethinking_2023.html#curves-with-lines",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Curves with lines",
    "text": "Curves with lines\n\\(~\\)\nMany causal relationships are obviously not linear.\nLinear models can fit curves quite easily, but be wary that this is geocentric (not mechanistic).\nFor example lets examine the full Kalahari dataset, which includes children.\n\n\nCode\nKalahari_data %>%\n  ggplot(aes(x = height)) +\n  geom_point(aes(y = weight), colour = \"salmon\", shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +\n  coord_cartesian(xlim = range(Kalahari_data$height)) +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\nTwo popular strategies\n\nPolynomials\nSplines and generalised additive models (nearly always better)\n\n\\(~\\)\n\nPolynomials\n\\(~\\)\n\\(\\mu_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x^2_{i}\\)\nNote the \\(x^{2}\\), you can use higher and higher order terms and the model becomes more flexible.\nIssues\n\nThere is unhelpful asyymetry to polynomials. For example, for a second order term, the model believes the data creates a parabola and it’s very hard to convince it otherwise.\nThe uncertainty at the edges of the data can be huge, making extrapolation and prediction very difficult.\nThere is no local smoothing so any data point can massively change the shape of a curve.\n\nFor a second order term, the model believes the data creates a parabola and it’s very hard to convince it otherwise.\nFit the models with second and third order terms\n\n\nCode\n# First it is worth standardising height\nKalahari_data <-\nKalahari_data %>%\n  mutate(height_s = (height - mean(height)) / sd(height)) %>% \n  mutate(height_s2 = height_s^2,\n         height_s3 = height_s^3)\n\n# fit quadratic model\n\nquadratic_kalahari <- \n  brm(data = Kalahari_data, \n      family = gaussian,\n      weight ~ 1 + height_s + height_s2,\n      prior = c(prior(normal(60, 10), class = Intercept),\n                prior(lognormal(0, 1), class = b, coef = \"height_s\"),\n                prior(normal(0, 1), class = b, coef = \"height_s2\"),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 30000, warmup = 29000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/quadratic_kalahari\")\n\n# get predictions\n\nheight_seq <- \n  tibble(height_s = seq(from = -4, to = 4, length.out = 30)) %>% \n  mutate(height_s2 = height_s^2)\n\nfitd_quad <-\n  fitted(quadratic_kalahari, \n         newdata = height_seq) %>%\n  data.frame() %>%\n  bind_cols(height_seq)\n\npred_quad <-\n  predict(quadratic_kalahari, \n          newdata = height_seq) %>%\n  data.frame() %>%\n  bind_cols(height_seq) \n\np2 <-\n  ggplot(data = Kalahari_data, \n       aes(x = height_s)) +\n  geom_ribbon(data = pred_quad, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_quad,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  geom_point(aes(y = weight),\n             color = \"salmon\", shape = 1.5, stroke =2, size = 2.5, alpha = 0.33) +\n  labs(y = \"weight\",\n       subtitle = \"quadratic\") +\n  coord_cartesian(xlim = range(-4, 4),\n                  ylim = range(0, 100)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n# fit cubic model\n\ncubic_kalahari <- \n  brm(data = Kalahari_data, \n      family = gaussian,\n      weight ~ 1 + height_s + height_s2 + height_s3,\n      prior = c(prior(normal(60, 10), class = Intercept),\n                prior(lognormal(0, 1), class = b, coef = \"height_s\"),\n                prior(normal(0, 1), class = b, coef = \"height_s2\"),\n                prior(normal(0, 1), class = b, coef = \"height_s3\"),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 40000, warmup = 39000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/cubic_kalahari\")\n\n# get predictions\n\nheight_seq <- \n  height_seq %>% \n  mutate(height_s3 = height_s^3)\n\nfitd_cub <-\n  fitted(cubic_kalahari, \n         newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq)\n\npred_cub <-\n  predict(cubic_kalahari, \n          newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq) \n\np3 <-\n  ggplot(data = Kalahari_data, \n       aes(x = height_s)) +\n  geom_ribbon(data = pred_cub, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_cub,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  geom_point(aes(y = weight),\n             color = \"salmon\", shape = 1.5, stroke =2, size = 2.5, alpha = 0.33) +\n  labs(y = \"weight\",\n       subtitle = \"cubic\") +\n  coord_cartesian(xlim = range(-4, 4),\n                  ylim = range(0, 100)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np2 + p3\n\n\n\n\n\nNote our use of the coef argument within our prior statements. Since \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are both parameters of class = b within the brms set-up, we need to use the coef argument in cases when we want their priors to differ.\nAt the left extreme, the model predicts that weight will increase as height decreases. This is because we have told the model the data is curved.\n\\(~\\)\n\n\nSplines\n\\(~\\)\nA big family of functions designed to do local smoothing.\nThis means that only the points within a region determine the shape of the function in that region. In polynomials the whole shape is affected by each data point. Results in splines being far more flexible than polynomials.\nOnce again, they have no mechanistic reality.\nWhat is a spline?\nUsed in drafting for architecture. They create wriggly lines.\nB-splines: regressions on synthetic variables\n\\(\\mu_{i} = \\alpha + w_{1}B_{i},1 + w_{2}B_{i},2 + w_{3}B_{i},3 +\\)\n\\(w\\) = the weight parameter - like a slope, determine the importance of the different variables for the predicted mean. These can overlap somewhat.\n\\(B\\) = the basis function - you make this - only have positive values in narrow regions of x-axis. They turn on weights in isolated regions of the x-axis.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-four-elemental-confounds",
    "href": "unimelb_statistical_rethinking_2023.html#the-four-elemental-confounds",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The four elemental confounds",
    "text": "The four elemental confounds\n\\(~\\)\nConfounds: some feature of the sample and how we use it that misleads us."
  }
]