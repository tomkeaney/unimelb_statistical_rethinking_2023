[
  {
    "objectID": "simulating_and_modelling.html",
    "href": "simulating_and_modelling.html",
    "title": "Simulating data and modelling it in brms",
    "section": "",
    "text": "Code\nlibrary(tidyverse) # for tidy style coding\nlibrary(brms) # for bayesian models\nlibrary(tidybayes) # for many helpful functions used to visualise distributions\nlibrary(MetBrewer) # for pretty colours\nlibrary(patchwork) # for combining plots"
  },
  {
    "objectID": "simulating_and_modelling.html#simulating-data",
    "href": "simulating_and_modelling.html#simulating-data",
    "title": "Simulating data and modelling it in brms",
    "section": "Simulating data",
    "text": "Simulating data\n\n\nCode\nsim_growth_data_small <-\n  expand_grid(\n    Days = rep.int(1:100, 1),\n    Sex = c(\"Female\", \"Male\")) %>% \n  arrange(Sex) %>% \n  mutate(Mass = if_else(Sex == \"Female\",\n                        Days * 0.2 + rnorm(100, 50, 5),\n                        Days * 0.3 + rnorm(100, 50, 5)))\n\nsim_growth_data_large <-\n  expand_grid(\n    Days = rep.int(1:100, 10),\n    Sex = c(\"Female\", \"Male\")) %>% \n  arrange(Days) %>% \n  mutate(Mass = if_else(Sex == \"Female\",\n                        Days * 0.2 + rnorm(1000, 50, 5),\n                        Days * 0.3 + rnorm(1000, 50, 5)))\n\n\nPlot them\n\n\nCode\np1 <-\n  sim_growth_data_small %>%\n  ggplot(aes(x = Days)) +\n  geom_point(aes(y = Mass, colour = Sex), shape = 1.5, stroke =2, size = 2.5, alpha = 0.9) +\n  scale_colour_manual(values = c(met.brewer(name = \"Hokusai3\")[1], met.brewer(name = \"Hokusai3\")[3])) +\n  labs(x = NULL, y = \"Mass (grams)\", subtitle = \"One observation each day\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16))\n\np2 <-\n  sim_growth_data_large %>%\n  ggplot(aes(x = Days)) +\n  geom_point(aes(y = Mass, colour = Sex), shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +\n  scale_colour_manual(values = c(met.brewer(name = \"Hokusai3\")[1], met.brewer(name = \"Hokusai3\")[3])) +\n  labs(x = \"Days since hatching\", y = \"Mass (grams)\", subtitle = \"Ten observations each day\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16))\n\np1 / p2"
  },
  {
    "objectID": "simulating_and_modelling.html#model-with-brms",
    "href": "simulating_and_modelling.html#model-with-brms",
    "title": "Simulating data and modelling it in brms",
    "section": "Model with brms",
    "text": "Model with brms\nThe core inputs to make the model run are formula, family and data.\nYou can code them like this:\nbrm(Mass ~ 1 + Days, family = gaussian, data = sim_growth_data_small)\nNext are your priors\nThe get_prior function is very useful here. Let’s try it out:\n\n\nCode\nget_prior(Mass ~ 1 + Days, family = gaussian, data = sim_growth_data_small)\n\n\n                   prior     class coef group resp dpar nlpar lb ub\n                  (flat)         b                                 \n                  (flat)         b Days                            \n student_t(3, 62.9, 8.8) Intercept                                 \n    student_t(3, 0, 8.8)     sigma                             0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThis shows the brms defaults for all the priors that are neccessary to run this model. If you don’t supply your own prior, brms will use its defaults.\nSo we need priors for b, Intercept and sigma\nWhat do thes mean?\n\nb is the effect that Days has on Mass. We know that things generally get bigger after hatching, so this must be positive.\nIntercept is the value for mass when Days = 0. This also must be positive.\nsigma is the variation in mass. This also must be > 0.\n\nTo check out what a prior looks like I use this quick bit of code\n\n\nCode\nhist(rnorm(n = 1000, mean = 0, sd = 1))\n\n\n\n\n\nNow some modelling nitty gritty\nWe need to tell brms how many iterations to run the model for, how many of these iterations we wish to use as warmup, how many chains to run and how many of your computers cores you want to use.\nFinally, because these models can be slow (this one will be fast but it’s good practice), you can use the file option to save the model output in your working directory and automatically load it whenever you rerun the code.\nHere is the full model:\n\n\nCode\nOur_mass_model_small <-\n  brm(Mass ~ 1 + Days,\n    family = gaussian,\n    data = sim_growth_data_small,\n    prior = c(prior(normal(50, 10), class = Intercept),\n              prior(lognormal(-1, 1), class = b, lb = 0),\n              prior(exponential(1), class = sigma)),\n    iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1,\n    file = \"fits/Our_mass_model_small_2\")\n\n\nYou can view the model output easily\n\n\nCode\nOur_mass_model_small\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Mass ~ 1 + Days \n   Data: sim_growth_data_small (Number of observations: 200) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    50.17      0.82    48.50    51.81 1.00     7722     5774\nDays          0.24      0.01     0.21     0.27 1.00     7771     5542\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.75      0.28     5.23     6.34 1.00     7593     5623\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRe-run on the bigger dataset\n\n\nCode\nOur_mass_model_large <-\n  brm(Mass ~ Days,\n    family = gaussian,\n    data = sim_growth_data_large,\n    prior = c(prior(normal(50, 10), class = Intercept),\n              prior(lognormal(-1, 1), class = b, lb = 0),\n              prior(exponential(1), class = sigma)),\n    iter = 4000, warmup = 2000, chains = 4, cores = 4,\n    file = \"fits/Our_mass_model_large\")\n\n\nAnd view output\n\n\nCode\nOur_mass_model_large\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Mass ~ Days \n   Data: sim_growth_data_large (Number of observations: 2000) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    49.80      0.27    49.28    50.33 1.00     8877     6133\nDays          0.25      0.00     0.24     0.26 1.00     9296     5883\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.97      0.10     5.79     6.16 1.00     8499     5963\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "simulating_and_modelling.html#plot-the-posterior",
    "href": "simulating_and_modelling.html#plot-the-posterior",
    "title": "Simulating data and modelling it in brms",
    "section": "Plot the posterior",
    "text": "Plot the posterior\nNow we want to plot the model predictions - that is, for a given Day, what is the predicted mass of an individual?\nWe can use two techniques, the easiest of which uses fitted\nFirst we need to create a new dataset with the combinations we want to predict from the model\n\n\nCode\nnew_data <- sim_growth_data_small %>% distinct(Days)\n\n\nNow lets fit the model predictions\n\n\nCode\nModel_predictions <-\n  fitted(Our_mass_model_small, newdata = new_data) %>% \n  bind_cols(new_data)\n\n\nLet’s plot these\n\n\nCode\np3 <-\n  ggplot(data = sim_growth_data_small, \n         aes(x = Days)) +\n  geom_point(aes(y = Mass),\n             color = met.brewer(name = \"Hokusai3\")[4], shape = 1.5, stroke =2, size = 2.5, alpha = 0.9) +\n  geom_smooth(data = Model_predictions,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  labs(y = \"Mass\") +\n  theme_classic() +\n  #coord_cartesian(xlim = range(-4, 4),\n  #               ylim = range(Kalahari_data$weight)) +\n  theme(text = element_text(size = 16),\n        panel.grid = element_blank())\n\np3\n\n\n\n\n\nWhat if we wanted to plot the distribution of the intercept?\nWe could get the prediction from fitted when Days = 0 or we can use the as_draws_df function\n\n\nCode\nOur_mass_model_small %>% \n  as_draws_df()\n\n\n# A draws_df: 2000 iterations, 4 chains, and 5 variables\n   b_Intercept b_Days sigma lprior lp__\n1           51   0.22   6.0   -9.6 -645\n2           52   0.22   5.9   -9.5 -645\n3           52   0.21   5.4   -9.0 -647\n4           48   0.28   6.1   -9.7 -647\n5           51   0.24   5.8   -9.5 -645\n6           50   0.24   5.2   -8.7 -645\n7           50   0.25   6.1   -9.7 -644\n8           51   0.23   5.6   -9.3 -645\n9           51   0.23   5.5   -9.1 -645\n10          49   0.24   6.0   -9.5 -646\n# ... with 7990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nThis gives us 2000 values for all the parameters estimated by the model - when plotted together these give us posterior distributions. Let’s do so for the Intercept.\n\n\nCode\np4 <- \n  Our_mass_model_small %>% \n  as_draws_df() %>% \n\n  ggplot(aes(x = b_Intercept)) +\n    stat_halfeye(fill = met.brewer(\"Hokusai3\")[2], .width = c(0.66, 0.95), alpha = 1,\n               point_interval = \"mean_qi\", point_fill = \"white\", \n               shape = 21, point_size = 4, stroke = 1.5, scale = 0.8) +\n  labs(x= \"Mass at day zero (the intercept)\", y = NULL) +\n  theme_classic() + \n  coord_cartesian(xlim = c(45, 55)) +\n  theme(panel.background = element_rect(fill='transparent'), #transparent panel bg\n        plot.background = element_rect(fill='transparent', color=NA), #transparent plot bg\n        panel.grid.minor.x = element_blank(),\n        legend.position = \"none\", #transparent legend panel\n        text = element_text(size=16))\n\np4\n\n\n\n\n\nFinally, we can combine the plot to make them look nice\n\n\nCode\np3 + p4"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html",
    "href": "unimelb_statistical_rethinking_2023.html",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "",
    "text": "Code\nlibrary(tidyverse) # for tidy coding\nlibrary(brms) # for fitting stan models\nlibrary(patchwork) # for combining plots\nlibrary(ggdag) # drawing dags\nlibrary(tidybayes) # for bayesian data visualisation\nlibrary(bayesplot) # more bayes data vis\nlibrary(MetBrewer) # colours \nlibrary(ggrepel) # for nice text on ggplots\nlibrary(loo) # for information criteria\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#dags-directed-acyclic-graphs",
    "href": "unimelb_statistical_rethinking_2023.html#dags-directed-acyclic-graphs",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "DAGs (Directed acyclic graphs)",
    "text": "DAGs (Directed acyclic graphs)\n\\(~\\)\n\nHeuristic causal models that clarify scientific thinking.\nWe can use these to produce appropriate statistical models.\nThese help us think about the science before we think about the data.\nHelps with questions like “what variables should we include in an analysis?”\nAn integral part of the course that will come up over and over again.\n\nAn example:\nLets make a function to speed up the DAG making process. We’ll use this a lot\n\n\nCode\n# Lets make a function to speed up the DAG making process. We'll use this a lot\n\ngg_simple_dag <- function(d) {\n  \n  d %>% \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(color = met.brewer(\"Hiroshige\")[4]) +\n    geom_dag_text(color = met.brewer(\"Hiroshige\")[7]) +\n    geom_dag_edges() + \n    theme_dag()\n}\n\ndagify( X ~ A + C,\n        C ~ A + B,\n        Y ~ X + C + B,  \n        coords = tibble(name = c(\"A\", \"C\", \"B\", \"X\", \"Y\"),\n                         x = c(1, 2, 3, 1, 3),\n                         y = c(2, 2, 2, 1, 1))) %>% \n  gg_simple_dag()\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#golems",
    "href": "unimelb_statistical_rethinking_2023.html#golems",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Golems",
    "text": "Golems\n\\(~\\)\nStatistical models are akin to Golems - clay robots brought to life by magic to follow specific tasks. The trouble is, they follow tasks extremely literally and are blind to their creators intent, so even those born out of the purest of intentions can cause great harm. Models are another form of robot; if we apply models within the wrong context they will not help us at all.\nEcological models are rarely designed to falsify null-hypotheses. This is because there are many possible non-null models or put another way there are no true null models in many systems. This is problematic because null models underlie how the majority of biologists do statistics!\nA more appropriate question is to ask how multiple process models that we can identify are different.\n\nShould falsify the explanatory model, not the null model. Make predictions and try and falify those. Karl Popper\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#owls",
    "href": "unimelb_statistical_rethinking_2023.html#owls",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Owls",
    "text": "Owls\n\\(~\\)\nSatistical modelling explanations often provide a brief introduction, then leave out many of the important details e.g. step 1: draw two circles, step 2: draw the rest of the fking owl.\nWe shall have a specific workflow for fitting models that we will document\nDrawing the owl, or the scientific workflow can be broken down into 5 steps:\n\nTheoretical estimand: what are you trying to do in your study in the first place?\nSome scientific causal model should be identified: step 1 will be precisely defined in the context of a causal model.\nUse 1 and 2 to build a statistical model.\nSimulate the scientific casual model to validate that the statistical model from step 3 yields the theoretical estimand i.e. check that our stats model works.\nAnalyse the real data. Note that the real data are only introduced now.\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#globe-tossing",
    "href": "unimelb_statistical_rethinking_2023.html#globe-tossing",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Globe tossing",
    "text": "Globe tossing\nTo estimate the proportion of water on planet earth, we collect data by tossing a globe 10 times and record whether our left index finger lands on land or water.\nOur results are shown in the code below\n\n\nCode\nglobe_toss_data <- tibble(toss = c(\"l\", \"w\", \"l\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"w\")) \n\nglobe_toss_data\n\n\n# A tibble: 10 × 1\n   toss \n   <chr>\n 1 l    \n 2 w    \n 3 l    \n 4 l    \n 5 w    \n 6 w    \n 7 w    \n 8 l    \n 9 w    \n10 w    \n\n\nRemember the owl workflow:\n\nDefine generative model of sample.\nDesign estimand.\nUse 1 and 2 to build a statistical model.\nTest (3) using (1).\nAnalyse sample and summarise.\n\n\\(~\\)\nStep 1\nSome true proportion of water, \\(p\\).\nWe can measure this indirectly using:\n\n\\(N\\): the number of globe tosses\n\\(W\\): the number of water observations\n\\(L\\): the number of land observations\n\n\n\nCode\n#Put dag here\n\n#N affects L and W but not the other way around\n\n# P also influences W and L\n\n\nBayesian data analysis\nFor each possible explanation of the data, count all the ways data can happen. Explanations with more ways to produce the data are more plausible (this is entropy).\nToss globe, probability \\(p\\) of observing \\(W\\), \\(1 - p\\) of \\(L\\).\nEach toss is random because it is chaotic (there are forces that could be theoretically measured i.e. velocity, exact starting orientation, but we are not equipped to measure these in real time, so the process appears random).\nEach toss is independent of one another.\nWhat are the relative number of ways we could get the data we actually got, given the process that generates all the possible datasets that could’ve been born from the process?\n\\(~\\)\n\nA 4 sided globe (dice)\nIn Bayesian analysis: enumerate all possible outcomes.\n\n\nCode\n# code land as 0 and water as 1 and create possibility data\n# create the dataframe (tibble)\n\nd <- tibble(p_1 = 0,\n            p_2 = rep(1:0, times = c(1, 3)),\n            p_3 = rep(1:0, times = c(2, 2)),\n            p_4 = rep(1:0, times = c(3, 1)),\n            p_5 = 1)\n\nd %>% \n  gather() %>% \n  mutate(x = rep(1:4, times = 5),\n         possibility = rep(1:5, each = 4)) %>% \n  \n  ggplot(aes(x = x, y = possibility, \n             fill = value %>% as.character())) +\n  geom_point(shape = 21, size = 9) +\n  scale_fill_manual(values = c(\"white\", \"navy\")) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(.75, 4.25),\n                  ylim = c(.75, 5.25)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        text = element_text(size = 18))    \n\n\n\n\n\nThe data: the dice is rolled three times: water, land, water.\nConsider all possible answers and how they would occur.\ne.g. if we hypothesise that 25% of the earth is covered by water, what are all the possible ways to produce our sample?\n\n\nCode\n# create a tibble of all the possibilities per marble draw, with columns position (where to appear on later figures x axis), draw (what number draw is it? and how many for each number? where to appear on figures y axis) and fill (colour of ball for figure)\n\nd <- tibble(position = c((1:4^1) / 4^0, \n                         (1:4^2) / 4^1, \n                         (1:4^3) / 4^2),\n            draw     = rep(1:3, times = c(4^1, 4^2, 4^3)),\n            fill     = rep(c(\"b\", \"w\"), times = c(1, 3)) %>% \n              rep(., times = c(4^0 + 4^1 + 4^2)))\n\n# we wish to make a path diagram, for which we will need connecting lines, create two more tibbles for these\n\nlines_1 <- tibble(x    = rep((1:4), each = 4),\n                  xend = ((1:4^2) / 4),\n                  y    = 1,\n                  yend = 2)\n\nlines_2 <- tibble(x    = rep(((1:4^2) / 4), each = 4),\n                  xend = (1:4^3) / (4^2),\n                  y    = 2,\n                  yend = 3)\n\n# We’ve generated the values for position (i.e., the x-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that.\n\nd <- d %>% \n  mutate(denominator = ifelse(draw == 1, .5,\n                              ifelse(draw == 2, .5 / 4,\n                                     .5 / 4^2))) %>% \n  mutate(position    = position - denominator)\n\nlines_1 <- lines_1 %>% \n  mutate(x    = x - .5,\n         xend = xend - .5 / 4^1)\n\nlines_2 <- lines_2 %>% \n  mutate(x    = x - .5 / 4^1,\n         xend = xend - .5 / 4^2)\n\n# create the plot, using geom_segment to add the lines - note coord_polar() which gives th eplot a globe-like effect. scale_x_continuous and the y equivalent have been used to remove axis lables and titles\n\nd %>% \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data  = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               size  = 1/3) +\n  geom_segment(data  = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               size  = 1/3) +\n  geom_point(aes(fill = fill),\n             shape = 21, size = 4) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +\n  theme_minimal() +\n  theme(panel.grid      = element_blank(),\n        legend.position = \"none\") +\n  coord_polar()\n\n\n\n\n\nPrune the number of possible outcomes down to those that are consistent with the data.\ne.g. there are 3 paths consistent with our dice rolling experiment for the given data and the given hypothesis.\n\n\nCode\nlines_1 <-\n  lines_1 %>% \n  mutate(remain = c(rep(0:1, times = c(1, 3)),\n                    rep(0,   times = 4 * 3)))\n\nlines_2 <-\n  lines_2 %>% \n  mutate(remain = c(rep(0,   times = 4),\n                    rep(1:0, times = c(1, 3)) %>% \n                      rep(., times = 3),\n                    rep(0,   times = 12 * 4)))\n\nd <-\n  d %>% \n  mutate(remain = c(rep(1:0, times = c(1, 3)),\n                    rep(0:1, times = c(1, 3)),\n                    rep(0,   times = 4 * 4),\n                    rep(1:0, times = c(1, 3)) %>% \n                      rep(., times = 3),\n                    rep(0,   times = 12 * 4))) \n\n# finally, the plot:\nd %>% \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data  = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain %>% as.character()),\n               size  = 1/3) +\n  geom_segment(data  = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain %>% as.character()),\n               size  = 1/3) +\n  geom_point(aes(fill = fill, alpha = remain %>% as.character()),\n             shape = 21, size = 4) +\n  # it's the alpha parameter that makes elements semitransparent\n  scale_alpha_manual(values = c(1/10, 1)) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +\n  theme_minimal() +\n  theme(panel.grid      = element_blank(),\n        legend.position = \"none\") +\n  coord_polar()\n\n\n\n\n\nIs 3 ways to produce the sample big or small, to find out, compare with other possibilities.\ne.g. lets make another conjecture - all land or all water - we have a land and water observation so there are zero ways that the all land/water hypotheses are consistent with the data.\n\n\nCode\n# if we make two custom functions, here, it will simplify the code within `mutate()`, below\nn_water <- function(x){\n  rowSums(x == \"W\")\n}\n\nn_land <- function(x){\n  rowSums(x == \"L\")\n}\n\nt <-\n  # for the first four columns, `p_` indexes position\n  tibble(p_1 = rep(c(\"L\", \"W\"), times = c(1, 4)),\n         p_2 = rep(c(\"L\", \"W\"), times = c(2, 3)),\n         p_3 = rep(c(\"L\", \"W\"), times = c(3, 2)),\n         p_4 = rep(c(\"L\", \"W\"), times = c(4, 1))) %>% \n  mutate(`roll 1: water`  = n_water(.),\n         `roll 2: land` = n_land(.),\n         `roll 3: water`  = n_water(.)) %>% \n  mutate(`ways to produce` = `roll 1: water` * `roll 2: land` * `roll 3: water`)\n\nt %>% \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_1\np_2\np_3\np_4\nroll 1: water\nroll 2: land\nroll 3: water\nways to produce\n\n\n\n\nL\nL\nL\nL\n0\n4\n0\n0\n\n\nW\nL\nL\nL\n1\n3\n1\n3\n\n\nW\nW\nL\nL\n2\n2\n2\n8\n\n\nW\nW\nW\nL\n3\n1\n3\n9\n\n\nW\nW\nW\nW\n4\n0\n4\n0\n\n\n\n\n\nUnglamorous basis of applied probability: Things that can happen more ways are more plausible.\nThis is Bayesian inference. Given a set of assumptions (hypotheses) the number of ways the numbers could have occurred accoridng to those assumptions (hypotheses) is the posterior probability distribution.\nBut we don’t really have enough evidence to be confident in a prediction. So lets roll the dice again. In bayes world a process called Bayesian updating exists, that saves you from running the draws again, in favour of just updating previous counts (or data). Bayesian updating is simple multiplication e.g. we roll another water, for the 3 water hypothesis there are 3 paths for this to occur so we multiply 9 x 3, resulting in 27 possible paths consistent with the new dataset.\nEventually though, the garden gets really big. This is where your computer comes in and it starts to make more sense to work with probabilities rather than counts.\n\n\nCode\nW <- sum(globe_toss_data == \"w\")\nL <- sum(globe_toss_data == \"l\")\np <- c(0, 0.25, 0.5, 0.75, 1) # proportions W\nways <- sapply(p, function(q) (q*4)^W * ((1 - q)*4)^L)\nprob <- ways/sum(ways)\n\nposterior <- cbind(p, ways, prob) %>% \n  as_tibble() %>% \n  mutate(p = as.character(p))\n\nposterior %>% \nggplot(aes(x = p, y = prob)) +\n  geom_col() +\n  labs(x = \"proportion water\", y = \"probability\") +\n  theme_minimal()\n\n\n\n\n\n\\(~\\)\n\n\nTest before you est\n\nCode a generative simulation\nCode an estimator\nTest (2) with (1)\n\nBuild a simulation\n\n\nCode\nsim_globe <- function(p = 0.7, N =9) {\n  sample(c(\"W\", \"L\"), size = N, prob = c(p, 1-p), replace = TRUE)\n}\n\n# W and L are the possible observations\n\n# N is number of tosses\n\n# prob is the probabilities of water and land occurring\n\n\nThe simulation does this:\n\n\nCode\nsim_globe()\n\n\n[1] \"W\" \"W\" \"L\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\n\nNow lets test it on extreme settings\ne.g. all water\n\n\nCode\nsim_globe(p=1, N = 11)\n\n\n [1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\n\nLooks good\nWe can also test how close the proportion of water produced in the simulation is to the specified p\n\n\nCode\nsum(sim_globe(p=0.5, N = 1e4) == \"W\") / 1e4\n\n\n[1] 0.5011\n\n\nAlso looks good.\nSo based upon our generative model:\nWays for \\(p\\) to produce $W, L = (4p)^W * (4 - 4p)^L $\n\n\nCode\n# function to compute posterior distirbution\n\ncompute_posterior <- function(the_sample, poss = c(0, 0.25, 0.5, 0.75, 1)) {\n  W <- sum(the_sample == \"W\") # the number of W observed\n  L <- sum(the_sample == \"L\") # the number of L observed\n  ways <- sapply(poss, function(q) (q*4)^W * ((1-q)*4)^L)\n  post <- ways/sum(ways)\n  #bars <- sapply(post, function(q) make_bar(q))\n  tibble(poss, ways, post = round(post, 3))#, bars)\n}\n\n\nWe can then simulate the experiment many times with sim_globe\n\n\nCode\ncompute_posterior(sim_globe())\n\n\n# A tibble: 5 × 3\n   poss  ways  post\n  <dbl> <dbl> <dbl>\n1  0        0 0    \n2  0.25    81 0.097\n3  0.5    512 0.612\n4  0.75   243 0.291\n5  1        0 0    \n\n\nThis allows us to check that our model is doing what we think it is.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#an-infinite-number-of-possibilties",
    "href": "unimelb_statistical_rethinking_2023.html#an-infinite-number-of-possibilties",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "An infinite number of possibilties",
    "text": "An infinite number of possibilties\nGlobes aren’t dice. There are an infinite number of possible proportions of the earth covered in water.\nTo get actual infinity we can turn to math.\nThe relative number of ways that nay value of \\(p\\) could produce any sample of W and L observations. This is a well know distribution called the binomial distribution\n\\[p^W(1-p)^L \\]\nThe only trick is normalising to make it a probability. A little calculus is needed (this produces the beta distirbution:\n\\[Pr(W, L|P) = \\frac{(W + L)!}{W!L!}p^{W}(1 - p)^{L}\\]\nNote the normalising constant \\(\\frac{(W + L)!}{W!L!}\\)\nWe can use the binomial sampling formula to give us the number of paths through the garden of forking data for this particular problem. That is given some value of P (akin to some number of blue marbles in the bag), the number of ways to see W and L can be worked out using the expression above.\nWe can use R to calculate this, assuming 6 globe tosses that landed on water out of 10 possible tosses and that P = 0.7:\n\n\nCode\ndbinom(6, 10, 0.7)\n\n\n[1] 0.2001209\n\n\nThis is the relative number of ways that 6 out of 10 can happen given a value of 0.7 for p. For this to be meaningful, we need to work out a probability value for many other values of P. This gives our probability distribution.\nLets plot how bayesian updating works, as we add observations\n\n\nCode\n# add the cumulative number of trials and successes for water to the dataframe.\n\nglobe_toss_data <- globe_toss_data %>% mutate(n_trials = 1:10, \n                                              n_success = cumsum(toss == \"w\"))\n\n# Struggling to follow this code, awesome figure produced though\n\nsequence_length <- 50\n\nglobe_toss_data %>%\n  expand(nesting(n_trials, toss, n_success),\n         p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>%\n  group_by(p_water) %>%\n  mutate(lagged_n_trials = lag(n_trials, k = 1),\n         lagged_n_success = lag(n_success, k =1)) %>%\n  ungroup() %>%\n  mutate(prior = ifelse(n_trials == 1, .5,\n                        dbinom(x = lagged_n_success,\n                               size = lagged_n_trials,\n                               prob = p_water)),\n         likelihood = dbinom(x = n_success,\n                             size = n_trials,\n                             prob = p_water),\n         strip = str_c(\"n = \", n_trials)\n         ) %>%\n  # the next three lines allow us to normalize the prior and the likelihood, \n  # putting them both in a probability metric\n  group_by(n_trials) %>%\n  mutate(prior = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) %>%\n  # plot time\n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior), linetype = 2) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = c(0, 0.5, 1)) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~n_trials, scales = \"free_y\")\n\n\n\n\n\nPosterior is continually updated as data points are added.\n\nEach posterior is simply a multiplication of a set of diagonal lines, like that shown in the first panel. Whether the slope of the line is pos or neg depends on whether the observation is land or water.\nEvery posterior is a prior for the next observation, but the data order doesn’t make a difference, in that the posterior will always be the same for a given dataset. BUT this assumption can be violated if each observation is not independent.\nSample size is embodied within the shape of the posterior. Already been accounted for.\n\nSome big things to grasp about bayes\n\nNo minimum sample size, because we have something called a prior. Note that estimation sucks with small samples, but that’s good! The model doesn’t draw too much from too little.\nShape of distribution embodies sample size\nThere are no point estimates e.g. means or medians. Everything is a distribution. You will never need to bootstrap again.\nThere is no one true interval - they just communicate the shape of the posterior distribution. No magical number e.g. 95%.\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#from-posterior-to-prediction-brms-time",
    "href": "unimelb_statistical_rethinking_2023.html#from-posterior-to-prediction-brms-time",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "From posterior to prediction (brms time)",
    "text": "From posterior to prediction (brms time)\nHere is a nice point to introduce brms the main modelling package that I use.\nLet’s fit a globe tossing model, where we observe 24 water observations from 36 tosses.\n\n\nCode\nb1.1 <-\n  brm(data = list(w = 24), \n      family = binomial(link = \"identity\"),\n      w | trials(36) ~ 0 + Intercept,\n      #prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      seed = 2,\n      file = \"fits/b02.01\")\n\nb1.1\n\n\n Family: binomial \n  Links: mu = identity \nFormula: w | trials(36) ~ 0 + Intercept \n   Data: list(w = 24) (Number of observations: 1) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.66      0.08     0.51     0.80 1.00     1681     1947\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere’s a lot going on in that output. For now, focus on the ‘Intercept’ line. The intercept of a typical regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial.\nTo use the posterior, we can sample from it using the as_draws_df function.\n\n\nCode\nas_draws_df(b1.1) %>% \n  mutate(n = \"n = 36\") %>%\n  \n  ggplot(aes(x = b_Intercept)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion water\", limits = c(0, 1)) +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16)) +\n  facet_wrap(~ n)\n\n\n\n\n\nAnother way to do this is to use the fitted function\n\n\nCode\nf <-\n  fitted(b1.1, \n         summary = F,\n         scale = \"linear\") %>% \n  data.frame() %>% \n  set_names(\"proportion\") %>% \n  as_tibble()\n\nf\n\n\n# A tibble: 4,000 × 1\n   proportion\n        <dbl>\n 1      0.705\n 2      0.562\n 3      0.584\n 4      0.667\n 5      0.694\n 6      0.706\n 7      0.654\n 8      0.684\n 9      0.682\n10      0.658\n# … with 3,990 more rows\n\n\nPlot the distribution\n\n\nCode\nf %>% \n  ggplot(aes(x = proportion)) +\n  geom_density(fill = \"grey50\", color = \"grey50\") +\n  annotate(geom = \"text\", x = .08, y = 2.5,\n           label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                     breaks = c(0, .5, 1),\n                     limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nStrikingly similar (well, exactly the same).\nWe can use this distribution of probabilities to predict histograms of water counts.\n\n\nCode\n# the simulation\nset.seed(3) # so we get the same result every time\n\nf <-\n  f %>% \n  mutate(w = rbinom(n(), size = 36,  prob = proportion))\n\n# the plot\nf %>% \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", size = 1/10) +\n  scale_x_continuous(\"number of water samples\", breaks = 0:40 * 4) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 450)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(8, 36)) +\n  theme_minimal()\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#what-are-linear-models",
    "href": "unimelb_statistical_rethinking_2023.html#what-are-linear-models",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "What are linear models",
    "text": "What are linear models\n\\(~\\)\n\nSimple statistical golems that model the mean and the variance of a variable. That’s it.\nThe mean is sum weighted sum of other variables. As those variables change, the mean and variance changes.\nAnovas, Ancovas, t-tests are all linear models.\n\nThe normal distribution\n\nCounts up all the ways the observations can happen given a set of assumptions.\nGiven some mean and variance the normal distribution gives you the relative number of ways the data can appear.\n\nThe normal distribution is the norm because:\n\nIt is very common in nature.\nIt’s easy to calculate\nVery conservative assumptions (spreads probability out more than any other distribution, reducing the risk of mistake, at the expense of accuracy)\n\nTo understand why the normal distribution is so common consider a soccer pitch mid-line, and individuals flipping coins that dictate whether they should step to the left or the right.\nMany individuals remain close to the line, while a few move towards the extremes. Simply, there are more ways to get a difference of 0 than there is any other result. This creates a bell curve and summarises processes to mean and variance. The path of one individual is shaded black in the figure below.\n\n\nCode\n# lets simulate this scenario\n\nset.seed(4)\n\npos <-\n  replicate(100, runif(16, -1, 1)) %>% # this is the sim\n  as_tibble() %>%\n  rbind(0, .) %>% # add a row of zeros above simulation results\n  mutate(step = 0:16) %>% # creates a step column\n  gather(key, value, -step) %>% # convert data to long format\n  mutate(person = rep(1:100, each = 17)) %>% # person IDs added\n  # the next lines allow us to make cumulative sums within each person\n  group_by(person) %>%\n  mutate(position = cumsum(value)) %>%\n  ungroup() # allows more data manipulation\n\nggplot(data = pos,\n       aes(x = step, y = position, group = person)) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_line(aes(colour = person < 2, alpha = person < 2)) +\n  scale_colour_manual(values = c(\"skyblue3\", \"black\")) +\n  scale_alpha_manual(values = c(1/5, 1)) +\n  scale_x_continuous(\"step number\", breaks = c(0, 4, 8, 12, 16)) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nThe density plot for this scenario at step 16:\n\n\nCode\n# first find the sd\n\nsd <-\n  pos %>%\n  filter(step == 16) %>%\n           summarise(sd = sd(position))\n\npos %>%\n  filter(step == 16) %>%\n  ggplot(aes(x = position)) +\n  stat_function(fun = dnorm,\n                args = list(mean = 0, sd = 2.18),\n                linetype = 2) +\n  geom_density(colour = \"transparent\", fill = \"dodgerblue\", alpha = 0.5) +\n  coord_cartesian(xlim = c(-6, 6)) +\n  labs(title = \"16 steps\",\n       y = \"density\")\n\n\n\n\n\nWhy normal? The generative perspective:\n\nNature makes bell curves whenever it adds things together.\nThis dampens fluctuations - large fluctuations are cancelled out by small fluctuations.\nSymmetry arises from the addition process.\n\nInferential perspective:\n\nIf all you know are the mean and the variance, then the least surprising distribution is gaussian.\n\n\nNote: if you just want mean or variance, a variable does not have to be normally distributed for the normal distribution to be useful\n\n\nOh and go with the FLOW. This is hard, you won’t understand everything, but keep flowing forward\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#weekly-owl-reminder",
    "href": "unimelb_statistical_rethinking_2023.html#weekly-owl-reminder",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Weekly owl reminder",
    "text": "Weekly owl reminder\n\nState a clear question\nSketch your causal assumptions\nUse the sketch to define generative model\nUse generative model to build estimator\nProfit (real model)\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#time-for-some-data-kalahari-heights",
    "href": "unimelb_statistical_rethinking_2023.html#time-for-some-data-kalahari-heights",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Time for some data: Kalahari heights",
    "text": "Time for some data: Kalahari heights\n\\(~\\)\nTo get us started with linear regression, lets load in the Kalahari dataset from McElreath’s rethinking package.\n\n\nCode\nlibrary(rethinking)\ndata(Howell1)\nKalahari_data <- as_tibble(Howell1)\n\n\nNow lets detach rethinking, we need to do this so brms always works\n\n\nCode\nrm(Howell1)\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\n\nRight lets have a quick look at the data\n\n\nCode\nKalahari_data\n\n\n# A tibble: 544 × 4\n   height weight   age  male\n    <dbl>  <dbl> <dbl> <int>\n 1   152.   47.8    63     1\n 2   140.   36.5    63     0\n 3   137.   31.9    65     0\n 4   157.   53.0    41     1\n 5   145.   41.3    51     0\n 6   164.   63.0    35     1\n 7   149.   38.2    32     0\n 8   169.   55.5    27     1\n 9   148.   34.9    19     0\n10   165.   54.5    54     1\n# … with 534 more rows\n\n\nAnd we can check out how each variable is distributed like so:\n\n\nCode\nKalahari_data %>% \n  pivot_longer(everything()) %>% \n  mutate(name = factor(name, levels = c(\"height\", \"weight\", \"age\", \"male\"))) %>% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10) +\n  facet_wrap(~ name, scales = \"free\", ncol = 1)\n\n\n\n\n\nThe owl\n1. In this lecture we are going to focus on describing the association between height and weight in adults.\n2. Scientific model: how does height affect weight?\nHeight influences weight, but not the other way around. Height is causal.\n\\(W = f(H)\\) this means that weight is some function of height.\n\n\nCode\ndagify( W ~ H + U,  \n        coords = tibble(name = c(\"H\", \"W\", \"U\"),\n                         x = c(1, 2, 3),\n                         y = c(1, 1, 1))) %>% \n  gg_simple_dag()\n\n\n\n\n\nNote the \\(U\\) in the DAG - an unobserved influence on weight.\n\nBuild a generative model\n\\(W = \\beta H + U\\)\nLets simulate the data\n\n\nCode\nsim_weight <- function(H, b, sd){\n  U <- rnorm(length(H), 0, sd)\n  W <- b*H + U\n  return(W)\n}\n\n\nRun the simulation and plot. We’ll need values for heights, a \\(\\beta\\) value and some value of sd for weight in kgs\n\n\nCode\nsim_data <- tibble(H = runif(200, min = 130, max = 170)) %>% \n  mutate(W = sim_weight(H, b = 0.5, sd = 5))\n\n# plot\n\nsim_data %>% \n  ggplot(aes(x = H, y = W)) +\n   geom_point(color = \"salmon\", shape = 1.5, stroke =2.5, size = 3, alpha = 0.9) +\n  theme_classic() +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme(text = element_text(size = 16))\n\n\n\n\n\nDescribing our model\n\\(W_{i} = \\beta H_{i} + U_i\\) is our equation for expected weight\n\\(U_{i} = Normal(0,\\sigma)\\) is the Gaussian error with sd \\(\\sigma\\)\n\\(H_{i} =\\) Uniform\\((130, 170)\\) means that all values are equally likely for height between 130-170\n\\(i\\) is an index and here represents individuals in the dataset\n= indicates a deterministic relationship\n~ indicates that something is “distributed as”\n\\(~\\)\n\n\nBuild estimator\nA linear model:\n\\(E(W_i|H_i) = \\alpha + \\beta H_{i}\\)\n\\(\\alpha\\) = the intercept\n\\(\\beta\\) = the slope\nOur estimator:\n\\(W_{i}\\) ~ \\(Normal(u_{i},\\sigma)\\)\n\\(u_{i}\\) ~ \\(\\alpha + \\beta H_{i}\\)\nIn words: \\(W\\) is distributed normally with mean that is a linear function of H\n\n\nPriors\nWe can specify these to be very helpful. We simply want to design priors to stop the model hallucinating impossible outcomes e.g. negative weights.\nPriors should express scientific knowledge, but softly. This is because the real process in nature is different to what we imagined and there needs to be room for this.\nSome basic things we know about weight:\n\nWhen height is zero, weight should be zero.\n\n\n\\(\\alpha\\) ~ Normal(0, 10) will achieve this\n\n\nWeight increases with height in humans on average. So \\(\\beta\\) should be positive\nWeight in kgs is less than height in cms, so \\(\\beta\\) should be less than 1\n\n\n\\(\\beta\\) ~ Uniform(0, 1) will achieve this\n\n\n\\(\\sigma\\) must be positive\n\n\n\\(\\sigma\\) ~ Uniform(0, 10)\n\nLets plot these priors\n\n\nCode\nn_lines <- 50\n\ntibble(n = 1:n_lines,\n       a = rnorm(n_lines, 0, 10),\n       b = runif(n_lines, 0, 1)) %>% \n  expand(nesting(n, a, b), height = 130:170) %>% \n  mutate(weight = a + b * (height)) %>%\n  \n  # plot\n  ggplot(aes(x = height, y = weight, group = n)) +\n  geom_line(alpha = 1/1.5, linewidth = 1, colour = met.brewer(\"Hiroshige\")[2]) +\n  scale_x_continuous(expand = c(0, 0)) +\n  coord_cartesian(ylim = c(0, 100),\n                  xlim = c(130, 170)) +\n  theme_classic()\n\n\n\n\n\nWoah, ok the slope looks ok, but the intercept is wild.\nThis is because we set a very high sd value for \\(\\alpha\\)\nWe can fix this, but for a problem like this, the data will overwhelm the prior.\n\\(~\\)\n\n\nBack to the owl: brms time\nLets validate our model\nLet simulate again with our sim_weight() function\n\n\nCode\nsim_data_100 <- \n  tibble(H = runif(100, min = 130, max = 170)) %>% \n  mutate(W = sim_weight(H, b = 0.5, sd = 5))\n\n\nFit the model\n\n\nCode\nweight_synthetic_model <-\n  brm(W ~ 1 + H,\n      family = gaussian,\n      data = sim_data_100,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(uniform(0, 1), class = b, lb = 0, ub = 1),\n                prior(uniform(0, 10), class = sigma, lb = 0, ub = 10)),\n      chains = 4, cores = 4, iter = 6000, warmup = 2000, seed = 1,\n      file = \"fits/weight_synthetic_model\")\n\n\nGet the model summary\n\n\nCode\nweight_synthetic_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: W ~ 1 + H \n   Data: sim_data_100 (Number of observations: 100) \n  Draws: 4 chains, each with iter = 6000; warmup = 2000; thin = 1;\n         total post-warmup draws = 16000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    12.28      6.51    -0.70    25.06 1.00    15740    11165\nH             0.42      0.04     0.34     0.51 1.00    15754    11351\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.91      0.35     4.28     5.67 1.00    14738    10608\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRight so, H is close to 0.5 and sigma is very close to 4.91. These aren’t perfect because we have a smallish sample and relatively wild priors.\n\\(~\\)\nStep 5. Profit (with the real data)\nTime to fit the actual model\n\n\nCode\nKalahari_adults <-\n  Kalahari_data %>% \n  filter(age > 18)\n\nweight_kalahari_model <-\n  brm(weight ~ 1 + height,\n      family = gaussian,\n      data = Kalahari_adults,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(uniform(0, 1), class = b, lb = 0, ub = 1),\n                prior(uniform(0, 10), class = sigma, lb = 0, ub = 10)),\n      chains = 4, cores = 4, iter = 6000, warmup = 2000, seed = 1,\n      file = \"fits/weight_kalahari_model\")\n\nweight_kalahari_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: weight ~ 1 + height \n   Data: Kalahari_adults (Number of observations: 346) \n  Draws: 4 chains, each with iter = 6000; warmup = 2000; thin = 1;\n         total post-warmup draws = 16000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -51.66      4.55   -60.54   -42.77 1.00    16472    11740\nheight        0.63      0.03     0.57     0.68 1.00    16478    11912\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     4.27      0.16     3.97     4.60 1.00    15291    11571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\\(~\\)\n\n\nOBEY THE LAW\n\\(~\\)\nLaw 1. Parameters are not independent of one another and cannot always be interpreted independently. They all act simultaneously on predictors (think about this from the context of the fitted() function).\nInstead we can push out posterior predictions from the model and describe/interpret those.\n\nPlot the sample\nPlot the posterior mean\nPlot the uncertainty of the mean\nPlot the uncertainty of predictions\n\n\n\nCode\n# use fitted to get posterior predictions\n\nheight_seq <- \n  tibble(height = 135:180) %>% \n  mutate(height_standard = height - mean(Kalahari_adults$height))\n\n# add uncertainty intervals\n\nmu_summary <-\n  fitted(weight_kalahari_model,\n         newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq)\n\n# add prediction intervals\n\npred_weight <-\n  predict(weight_kalahari_model,\n          newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq)\n\n# make plot \n\nKalahari_adults %>%\n  ggplot(aes(x = height)) +\n   geom_ribbon(data = pred_weight, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"salmon\", alpha = 0.5) +\n  geom_point(aes(y = weight), color = \"salmon\", shape = 1.5, stroke =2, size = 1.5, alpha = 0.9) +\n  geom_smooth(data = mu_summary,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"salmon\", color = \"black\", alpha = 0.7, size = 1/2) +\n  coord_cartesian(xlim = range(Kalahari_adults$height)) +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme_classic() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nThis plot shows 1) the raw data, 2) the predicted relationship between height and weight with 3) 95% uncertainty intervals for the mean and 4) 95% prediction intervals for where data points are predicted to fall within.\nPredict() reports prediction intervals, which are simulations that are the joint consequence of both the mean and sigma, unlike the results of fitted(), which only reflect the mean.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#categories",
    "href": "unimelb_statistical_rethinking_2023.html#categories",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Categories",
    "text": "Categories\nWant to stratify by categories in our data. This means fitting a separate regression line for each category.\nLets return to the Kalahari data. Now we’ll add in a categorical variable - the sex of the individual.\n\n\nCode\nKalahari_adults %>%\n  ggplot(aes(x = height, colour = as.factor(male))) +\n  geom_point(aes(y = weight), shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +\n  coord_cartesian(xlim = range(Kalahari_adults$height)) +\n  scale_colour_manual(values = c(\"salmon\", \"darkcyan\")) +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\nThink scientifically first:\nHow are height, sex and weight causally associated?\nHow are height, sex and weight statistically related?\nLets build a DAG:\n\nFirst, we know that height causally effects weight. You can change your own weight without changing your height. However, as you get taller there is more of you, thus it is reasonable to expect height to affect weight.\nWe also know that sex influences height, but it is silly to say that height influences sex.\nThird, if there is any influence we expect sex to influence weight not the other way around. Therefore weight may be influenced by both height and sex.\n\nLets draw this DAG\n\n\nCode\ndagify(W ~ S + H,\n       H ~ S,\n       labels = c(\"W\" = \"Weight\", \n                  \"H\" = \"Height\",\n                  \"S\" = \"Sex\")) %>% \n  gg_simple_dag()\n\n\n\n\n\nThis is a mediation graph. Note that effects do not stop at one variable, instead they continue on through the path. In that way sex has both a direct and indirect effect on weight. The indirect effect may be through height, which is ‘contaminated’ by sex.\nStatistically:\n\\(H = f_{H}(S)\\)\n\\(W = f_{W}(H, S)\\)\nFollowing our workflow lets simulate a more complex dataset, that this time includes separate sexes.\nTo keep in line with the Kalahari data we’ll code females = 1 and males = 2\n\n\nCode\nsim_HW <- function(S, b, a){\n  N <- length(S)\n  H <- if_else(S == 1, 150, 160) + rnorm(N, 0, 5)\n  W <- a[S] + b[S]*H + rnorm(N, 0, 5)\n  tibble(S, H, W)\n}\n\n\nGive it some data and run sim_HW()\n\n\nCode\nS <- rethinking::rbern(100) + 1\n\n(synthetic_data <- sim_HW(S, b = c(0.5, 0.6), a = c (0, 0)))\n\n\n# A tibble: 100 × 3\n       S     H     W\n   <dbl> <dbl> <dbl>\n 1     2  161.  98.8\n 2     1  156.  81.0\n 3     1  148.  65.0\n 4     2  161.  98.2\n 5     1  148.  80.3\n 6     1  156.  75.2\n 7     1  142.  71.9\n 8     2  155.  94.0\n 9     2  162.  95.8\n10     2  156.  95.1\n# … with 90 more rows\n\n\nDefine the questions we’re going to ask:\nDifferent questions lead to a need for different stats models.\nQ: Causal effect of H on W?\nQ: Causal effect of S on W?\nQ: Direct effect of S on W?\nEach require different components of the DAG.\nDrawing the categorical OWL:\nThere are several ways to code categorical variables\n\nDummy or indicator variables\n\n\nSeries of 0 1 variables that stand in for categories\n\n\nIndex variables\n\n\nAssign an index value to each category\nBetter for specifying priors\nExtend effortlessly to multi-level models\nWhat we will use\n\n\\(~\\)\nQ: What is the causal effect of S on W?\n\\(~\\)\nUsing index variables\nEstimating average weight:\n\\(W_{i} = Normal(\\mu_{i}, \\sigma)\\)\n\\(\\mu_{i} = \\alpha S[i]\\) where \\(S[i]\\) is the sex of the i-th person\nS = 1 indicates female, S = 2 indicates male\n\\(\\alpha = [\\alpha_{i}, \\alpha_{2}]\\) this means there are two intercepts, one for each sex\nPriors\n\\(\\alpha_{j} = Normal(60, 10)\\)\nAll this says is that there is more than one value of (indicated by the subscript j) and that we want the prior to be the same for each of the values. Values correspond to the categories.\nWe’ll let the sample update the prior and tell us if sexual dimorphism exists\nRight ready to model. Not yet! More simulation. This might seem like overkill but it will help so much to get into this habit.\nWe’ll construct a female and male sample and look at the average difference. We’ll only change sex.\nWe’ll find the difference between the sexes in our simulation (remember we coded a stronger effect of height on male weight than on female weight):\n\n\nCode\n# female sample\n\nS <- rep(1, 100)\n\nsimF <- sim_HW(S, b = c(0.5, 0.6), a = c(0, 0))\n\nS <- rep(2, 100)\n\nsimM <- sim_HW(S, b = c(0.5, 0.6), a = c(0, 0))\n\n# effect of Sex (male - female)\n\nmean(simM$W - simF$W)\n\n\n[1] 19.53385\n\n\nOk a difference of ~21kgs\nNow lets test the estimator.\nNote that we have specified a 0 intercept. In brms this will result in an output that produces a separate intercept for each categorical variable.\n\n\nCode\nsynthetic_kalahari_sex_model <-\n  brm(data = synthetic_data,\n      W ~ 0 + as.factor(S),\n      prior = c(prior(normal(60, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 1,\n      file = \"fits/synthetic_kalahari_sex\")\n\nsynthetic_kalahari_sex_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: W ~ 0 + as.factor(S) \n   Data: synthetic_data (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nas.factorS1    75.69      0.82    74.13    77.31 1.00     4038     2899\nas.factorS2    96.13      0.74    94.69    97.63 1.00     4126     2946\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.50      0.36     4.86     6.29 1.00     4199     2984\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAgain we find a difference between the sexes ~21kgs. Looks like our model tests what we want to test.\nNow lets use the real data\n\n\nCode\n# make the index variable to match McElreath's dataset\n\nKalahari_adults <-\n  Kalahari_adults %>% \n  mutate(Sex = if_else(male == \"1\", 2, 1),\n         Sex = as.factor(Sex))\n\nkalahari_sex_model <-\n  brm(data = Kalahari_adults,\n      weight ~ 0 + Sex,\n      prior = c(prior(normal(60, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 1,\n      file = \"fits/_kalahari_sex_model\")\n\n\nFind the contrasts and plot the average differences between women and men\n\n\nCode\ndraws <- as_draws_df(kalahari_sex_model)\n\nas_draws_df(kalahari_sex_model) %>% \n  mutate(`Mean weight contrast` = b_Sex2 - b_Sex1) %>% \n  rename(Female = b_Sex1,\n         Male = b_Sex2) %>% \n  pivot_longer(cols = c(\"Female\", \"Male\", \"Mean weight contrast\")) %>% \n  \n  ggplot(aes(x = value, y = 0)) +\n    stat_halfeye(.width = 0.95,\n                 normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"Posterior mean weights (kgs)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\nWhat about the posterior predictive distribution, not just the mean?\n\n\nCode\nFemale_dist <- rnorm(1000, draws$b_Sex1, draws$sigma) %>% \n  as_tibble() %>% \n  rename(Female = value)\n\nMale_dist <- rnorm(1000, draws$b_Sex2, draws$sigma) %>% \n  as_tibble() %>% \n  rename(Male = value)\n\nplot_1 <-\n  cbind(Female_dist, Male_dist) %>% \n  pivot_longer(cols = c(\"Female\", \"Male\")) %>% \n    \n  ggplot(aes(x = value, group = name, colour = name, fill = name)) +\n  geom_density(alpha = 0.8) +\n  scale_colour_manual(values = c(\"salmon\", \"darkcyan\")) +\n  scale_fill_manual(values = c(\"salmon\", \"darkcyan\")) +\n  xlab(\"Posterior predicted weights (kgs)\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n \n\nplot_2 <- \ncbind(Female_dist, Male_dist) %>%\n  as_tibble() %>% \n  mutate(predictive_diff = Male - Female) %>% \n  \n  ggplot(aes(x = predictive_diff, colour = \"orange\", fill = \"orange\")) +\n  stat_slab(aes(fill = after_stat(x > 0), colour = after_stat(x > 0)), alpha = 0.8) +\n  xlab(\"Posterior predictive weight difference (kgs)\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\nplot_1 / plot_2\n\n\n\n\n\nThe takeaway here is that there are lots of women that are heavier than men, even though the difference between the means is large.\nWe need to calculate the contrast, or difference between the categories (as we have shown above).\n\nIt is never legitimate to compare overlap in parameters\nWhat you do is compute the distribution of the difference (as shown above)\n\n\\(~\\)\nQ: What is the direct causal effect of S on W?\n\\(~\\)\nNow we must add Height into the model\n\\(W_{i} = Normal(\\mu_{i}, \\sigma)\\)\n\\(\\mu_{i} = \\alpha_{S[i]} + \\beta_{S[i]}(H_{i} - \\overline{H})\\)\nNow we have two intercepts and two slopes!\nCentering\n\nCentering H makes it so that \\(\\alpha\\) is the average weight of a person with average height\nEasy to fit priors for \\(alpha\\)\nLinear regressions build lines that pass through this point (the grand mean)\n\n\n\nCode\nKalahari_adults <-\n  Kalahari_adults %>%\n   mutate(height_standard = height - mean(height))\n\n\nLets model\n\n\nCode\nKalahari_h_S_model <-\n  brm(data = Kalahari_adults,\n      weight ~ 0 + Sex + height_standard,\n      prior = c(#prior(normal(60, 10), class = a),\n                prior(lognormal(0, 1), class = b, lb = 0),\n                prior(exponential(1), class = sigma)),\n      iter = 4000, warmup = 2000, chains = 4, cores = 4,\n      seed = 1,\n      file = \"fits/Kalahari_h_S_model\")\n\n\nI’m not sure how to make McElreath’s plot here - he brilliantly finds the effect of sex at 50 different heights then plots. The best I can do is the overall effect of sex on weight after accounting for sex.\nThe takeaway however, is that nearly all of the causal effect of sex acts through height.\n\n\nCode\ndraws_2 <- as_draws_df(Kalahari_h_S_model) %>% \n  mutate(weight_contrast = b_Sex2 - b_Sex1)\n\nFemale_dist_2 <- rnorm(1000, draws_2$b_Sex1, draws_2$sigma) %>% \n  as_tibble() %>% \n  rename(Female = value)\n\nMale_dist_2 <- rnorm(1000, draws_2$b_Sex2, draws_2$sigma) %>% \n  as_tibble() %>% \n  rename(Male = value)\n\nplot_1 <-\n  cbind(Female_dist_2, Male_dist_2) %>% \n  pivot_longer(cols = c(\"Female\", \"Male\")) %>% \n    \n  ggplot(aes(x = value, group = name, colour = name, fill = name)) +\n  geom_density(alpha = 0.8) +\n  scale_colour_manual(values = c(\"salmon\", \"darkcyan\")) +\n  scale_fill_manual(values = c(\"salmon\", \"darkcyan\")) +\n  xlab(\"Posterior predicted weights (kgs)\\nafter accounting for the effect of height\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n \n\nplot_2 <- \ncbind(Female_dist, Male_dist) %>%\n  as_tibble() %>% \n  mutate(predictive_diff = Male - Female) %>% \n  \n  ggplot(aes(x = predictive_diff, colour = \"orange\", fill = \"orange\")) +\n  geom_density(alpha = 0.8) +\n  xlab(\"Posterior predictive weight difference (kgs)\\nafter accounting for the effect of height\") +\n  ylab(\"Density\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\nplot_1 / plot_2\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#curves-with-lines",
    "href": "unimelb_statistical_rethinking_2023.html#curves-with-lines",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Curves with lines",
    "text": "Curves with lines\n\\(~\\)\nMany causal relationships are obviously not linear.\nLinear models can fit curves quite easily, but be wary that this is geocentric (not mechanistic).\nFor example lets examine the full Kalahari dataset, which includes children.\n\n\nCode\nKalahari_data %>%\n  ggplot(aes(x = height)) +\n  geom_point(aes(y = weight), colour = \"salmon\", shape = 1.5, stroke =2, size = 2.5, alpha = 0.6) +\n  coord_cartesian(xlim = range(Kalahari_data$height)) +\n  labs(x = \"Height (cm)\", y = \"Weight (kgs)\") +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\nTwo popular strategies\n\nPolynomials\nSplines and generalised additive models (nearly always better)\n\n\\(~\\)\n\nPolynomials\n\\(~\\)\n\\(\\mu_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x^2_{i}\\)\nNote the \\(x^{2}\\), you can use higher and higher order terms and the model becomes more flexible.\nIssues\n\nThere is unhelpful asyymetry to polynomials. For example, for a second order term, the model believes the data creates a parabola and it’s very hard to convince it otherwise.\nThe uncertainty at the edges of the data can be huge, making extrapolation and prediction very difficult.\nThere is no local smoothing so any data point can massively change the shape of a curve.\n\nFor a second order term, the model believes the data creates a parabola and it’s very hard to convince it otherwise.\nFit the models with second and third order terms\n\n\nCode\n# First it is worth standardising height\nKalahari_data <-\nKalahari_data %>%\n  mutate(height_s = (height - mean(height)) / sd(height)) %>% \n  mutate(height_s2 = height_s^2,\n         height_s3 = height_s^3)\n\n# fit quadratic model\n\nquadratic_kalahari <- \n  brm(data = Kalahari_data, \n      family = gaussian,\n      weight ~ 1 + height_s + height_s2,\n      prior = c(prior(normal(60, 10), class = Intercept),\n                prior(lognormal(0, 1), class = b, coef = \"height_s\"),\n                prior(normal(0, 1), class = b, coef = \"height_s2\"),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 30000, warmup = 29000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/quadratic_kalahari\")\n\n# get predictions\n\nheight_seq <- \n  tibble(height_s = seq(from = -4, to = 4, length.out = 30)) %>% \n  mutate(height_s2 = height_s^2)\n\nfitd_quad <-\n  fitted(quadratic_kalahari, \n         newdata = height_seq) %>%\n  data.frame() %>%\n  bind_cols(height_seq)\n\npred_quad <-\n  predict(quadratic_kalahari, \n          newdata = height_seq) %>%\n  data.frame() %>%\n  bind_cols(height_seq) \n\np2 <-\n  ggplot(data = Kalahari_data, \n       aes(x = height_s)) +\n  geom_ribbon(data = pred_quad, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_quad,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  geom_point(aes(y = weight),\n             color = \"salmon\", shape = 1.5, stroke =2, size = 2.5, alpha = 0.33) +\n  labs(y = \"weight\",\n       subtitle = \"quadratic\") +\n  coord_cartesian(xlim = range(-4, 4),\n                  ylim = range(0, 100)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n# fit cubic model\n\ncubic_kalahari <- \n  brm(data = Kalahari_data, \n      family = gaussian,\n      weight ~ 1 + height_s + height_s2 + height_s3,\n      prior = c(prior(normal(60, 10), class = Intercept),\n                prior(lognormal(0, 1), class = b, coef = \"height_s\"),\n                prior(normal(0, 1), class = b, coef = \"height_s2\"),\n                prior(normal(0, 1), class = b, coef = \"height_s3\"),\n                prior(uniform(0, 50), class = sigma)),\n      iter = 40000, warmup = 39000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/cubic_kalahari\")\n\n# get predictions\n\nheight_seq <- \n  height_seq %>% \n  mutate(height_s3 = height_s^3)\n\nfitd_cub <-\n  fitted(cubic_kalahari, \n         newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq)\n\npred_cub <-\n  predict(cubic_kalahari, \n          newdata = height_seq) %>%\n  as_tibble() %>%\n  bind_cols(height_seq) \n\np3 <-\n  ggplot(data = Kalahari_data, \n       aes(x = height_s)) +\n  geom_ribbon(data = pred_cub, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_cub,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, size = 1/2) +\n  geom_point(aes(y = weight),\n             color = \"salmon\", shape = 1.5, stroke =2, size = 2.5, alpha = 0.33) +\n  labs(y = \"weight\",\n       subtitle = \"cubic\") +\n  coord_cartesian(xlim = range(-4, 4),\n                  ylim = range(0, 100)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np2 + p3\n\n\n\n\n\nNote our use of the coef argument within our prior statements. Since \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are both parameters of class = b within the brms set-up, we need to use the coef argument in cases when we want their priors to differ.\nAt the left extreme, the model predicts that weight will increase as height decreases. This is because we have told the model the data is curved.\n\\(~\\)\n\n\nSplines\n\\(~\\)\nA big family of functions designed to do local smoothing.\nThis means that only the points within a region determine the shape of the function in that region. In polynomials the whole shape is affected by each data point. Results in splines being far more flexible than polynomials.\nOnce again, they have no mechanistic reality.\nWhat is a spline?\nUsed in drafting for architecture. They create wriggly lines.\nB-splines: regressions on synthetic variables\n\\(\\mu_{i} = \\alpha + w_{1}B_{i},1 + w_{2}B_{i},2 + w_{3}B_{i},3 +\\)\n\\(w\\) = the weight parameter - like a slope, determine the importance of the different variables for the predicted mean. These can overlap somewhat.\n\\(B\\) = the basis function - you make this - only have positive values in narrow regions of x-axis. They turn on weights in isolated regions of the x-axis.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-four-elemental-confounds",
    "href": "unimelb_statistical_rethinking_2023.html#the-four-elemental-confounds",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The four elemental confounds",
    "text": "The four elemental confounds\n\\(~\\)\nConfounds: some feature of the sample and how we use it that misleads us.\nThere are four major confounds:\n\nThe fork\nThe Pipe\nThe Collider\nThe Descendant\n\n\n\nCode\nd1 <- \n  dagify(X ~ Z,\n         Y ~ Z,\n         coords = tibble(name = c(\"X\", \"Y\", \"Z\"),\n                         x = c(1, 3, 2),\n                         y = c(2, 2, 1)))\n\nd2 <- \n  dagify(Z ~ X,\n         Y ~ Z,\n         coords = tibble(name = c(\"X\", \"Y\", \"Z\"),\n                         x = c(1, 3, 2),\n                         y = c(2, 1, 1.5)))\n\nd3 <- \n  dagify(Z ~ X + Y,\n         coords = tibble(name = c(\"X\", \"Y\", \"Z\"),\n                         x = c(1, 3, 2),\n                         y = c(1, 1, 2)))\n\nd4 <- \n  dagify(Z ~ X + Y,\n         D ~ Z,\n         coords = tibble(name = c(\"X\", \"Y\", \"Z\", \"D\"),\n                         x = c(1, 3, 2, 2),\n                         y = c(1, 1, 2, 1.05)))\n\np1 <- gg_simple_dag(d1) + labs(subtitle = \"The Fork\")\np2 <- gg_simple_dag(d2) + labs(subtitle = \"The Pipe\")\np3 <- gg_simple_dag(d3) + labs(subtitle = \"The Collider\")\np4 <- gg_simple_dag(d4) + labs(subtitle = \"The Descendant\")\n\n(p1 | p2 | p3 | p4) &\n  theme(plot.subtitle = element_text(hjust = 0.5)) &\n  plot_annotation(title = \"The four elemental confounds\")\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-fork",
    "href": "unimelb_statistical_rethinking_2023.html#the-fork",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The Fork",
    "text": "The Fork\n\\(~\\)\nX and Y are associated because they share a common cause. However, this association disappears once we account for Z. That means that Y is independent of X once we account for Z.\n\n\nCode\np1\n\n\n\n\n\nNote that once we stratify for Z, the noise affecting X and Y can become independent.\nLet’s simulate:\n\n\nCode\nn <- 1000\n\ntibble(Z = rbernoulli(n, 0.5)) %>% \n  mutate(Z = if_else(Z == \"TRUE\", 1, 0),\n         X = rbernoulli(n, (1 - Z) * 0.1 + Z * 0.9),\n         Y = rbernoulli(n, (1 - Z)* 0.1 + Z * 0.9),\n         X = if_else(X == \"TRUE\", 1, 0),\n         Y = if_else(Y == \"TRUE\", 1, 0)) %>% \n  count(X, Y)\n\n\n# A tibble: 4 × 3\n      X     Y     n\n  <dbl> <dbl> <int>\n1     0     0   425\n2     0     1    97\n3     1     0    77\n4     1     1   401\n\n\nSee how this works? Our simulation specifies no relation between X and Y except through Z, but they are super positively correlated.\nNow for a continuous example\n\n\nCode\nn <- 300\n\nfork_sim_data <- \n  tibble(Z = rbernoulli(n)) %>% \n  mutate(Z = if_else(Z == \"TRUE\", 1, 0),\n         X = rnorm(n, 2*Z - 1),\n         Y = rnorm(n, 2 * Z -1)) \n\nfork_sim_data %>%\n  ggplot(aes(X, Y)) +\n  geom_point(aes(colour = as.factor(Z)), shape = 1.5, stroke =2, size = 2.5, alpha = 0.8) +\n  geom_smooth(aes(colour = as.factor(Z)), method = \"lm\", se = FALSE, linewidth = 1.5) +\n  geom_smooth(colour = \"black\", method = \"lm\", se = FALSE) +\n  scale_colour_manual(values = c(met.brewer(name = \"Hokusai3\")[1], met.brewer(name = \"Hokusai3\")[3])) +\n  theme_classic() +\n  theme(panel.grid = element_blank(),\n        text = element_text(size = 16))\n\n\n\n\n\nA data problem: divorce rate and marriage rate.\n\n\nCode\nWaffle_data %>%\n  ggplot(aes(x = Marriage, y = Divorce)) +\n  stat_smooth(method = \"lm\", fullrange = T, size = 1/2,\n              color = \"firebrick4\", fill = \"firebrick\", alpha = 1/5) +\n  geom_point(size = 3.5, color = \"firebrick4\", alpha = 1/2) +\n  #scale_x_continuous(\"Marriage rate\", limits = c(22, 30)) +\n  ylab(\"Divorce rate\") +\n  xlab(\"Marriage rate\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nThese two variables are correlated but is marriage rate causal of divorce rate?\nHold on, but there are other things we need to consider.\nWhat else could affect divorce? Age at marriage certainly might. Lets plot:\n\n\nCode\nWaffle_data %>%\n  ggplot(aes(x = MedianAgeMarriage, y = Divorce)) +\n  stat_smooth(method = \"lm\", fullrange = T, size = 1/2,\n              color = \"firebrick4\", fill = \"firebrick\", alpha = 1/5) +\n  geom_point(size = 3, color = \"firebrick4\", alpha = 1/2) +\n  scale_x_continuous(\"Median age at marriage\", limits = c(22, 30)) +\n  ylab(\"Divorce rate\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nLets account for this and build the DAG, considering that age of marriage could plausibly lead to more marriage.\nTo estimate the direct effect of marriage, you must break the fork - that is stratify by the common cause - age at marriage.\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"A\", \"M\", \"D\"),\n         x    = c(1, 3, 2),\n         y    = c(2, 2, 1))\n\np1 <-\n  dagify(M ~ A,\n         D ~ A + M,\n         coords = dag_coords) %>%\n  \n  gg_simple_dag()\n\np1\n\n\n\n\n\n\nScientific model\n\nWhat does stratify mean:\n\nIt creates sub-populations - in a linear regression it does this:\n\n\\(D_{i} = Normal(\\mu_{i}, \\sigma)\\)\n\\(\\mu_{i} = \\alpha + \\beta_{M}M_{i} + \\beta_{A}A_{i}\\)\nThat is, make the marriage variable intercept conditional upon age of marriage. To do this simply include it as a term.\nWe can also stratify using interactions. But how we should do it depends on your casual situation.\n\nStatistical model\n\n\\(D_{i} = Normal(\\mu_{i}, \\sigma)\\)\n\\(\\mu_{i} = \\alpha + \\beta_{M}M_{i} + \\beta_{A}A_{i}\\)\n\\(\\alpha = Normal(?, ?)\\)\n\\(\\beta_{M} = Normal(?, ?)\\)\n\\(\\beta_{A} = Normal(?, ?)\\)\n\\(\\sigma = Exponential(?)\\)\nBefore we talk about the exponential prior lets standardise the data (done above but explained here)\nWhy standardise?\nTo standardise subtract the mean and divide each value by the standard deviation. Variables become Z scores - values represent SDs from the mean. 0 is the mean.\nThis makes computation more efficient.\nPriors are easier to choose because we have some understanding about effect size.\nLets do some prior simulation, first with weak priors:\n\\(\\alpha = Normal(0, 10)\\)\n\\(\\beta_{M} = Normal(0, 10)\\)\n\\(\\beta_{A} = Normal(0, 10)\\)\n\\(\\sigma = Exponential(1)\\)\nThese priors are so broad they are essentially flat. This is a bad idea. The reason is because almost all the probability mass is for insanely strong pos or neg relationships.\nThe plot below shows the stupidly steep slopes these priors predict.\n\n\nCode\n# fit a model with bad priors\n\nWaffle_model_bad_priors <- \n  brm(data = Waffle_data, \n      family = gaussian,\n      d ~ 1 + a,\n      prior = c(prior(normal(0, 10), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      sample_prior = T,\n      file = \"fits/Waffle_model_bad_priors\")\n\nset.seed(5)\n\nprior_draws(Waffle_model_bad_priors) %>% \n  slice_sample(n = 50) %>% \n  rownames_to_column(\"draw\") %>% \n  expand(nesting(draw, Intercept, b),\n         a = c(-2, 2)) %>% \n  mutate(d = Intercept + b * a) %>% \n  \n  ggplot(aes(x = a, y = d)) +\n  geom_line(aes(group = draw),\n            color = \"firebrick\", alpha = .4) +\n  labs(x = \"Median age marriage (std)\",\n       y = \"Divorce rate (std)\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\nBetter priors\n\\(\\alpha = Normal(0, 0.2)\\)\n\\(\\beta_{M} = Normal(0, 0.5)\\)\n\\(\\beta_{A} = Normal(0, 0.5)\\)\n\\(\\sigma = Exponential(1)\\)\nRe-sample\n\n\nCode\nMarriage_age_model <- \n  brm(data = Waffle_data, \n      family = gaussian,\n      d ~ 1 + a,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      sample_prior = T,\n      file = \"fits/Marriage_age_model\")\n\nMarriage_age_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: d ~ 1 + a \n   Data: Waffle_data (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.10    -0.20     0.20 1.00     3537     2678\na            -0.57      0.12    -0.79    -0.34 1.00     3801     2753\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.82      0.09     0.68     1.01 1.00     2735     2675\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLets have a look at that plot again with our updated priors\n\n\nCode\nprior_draws(Marriage_age_model) %>% \n  slice_sample(n = 50) %>% \n  rownames_to_column(\"draw\") %>% \n  expand(nesting(draw, Intercept, b),\n         a = c(-2, 2)) %>% \n  mutate(d = Intercept + b * a) %>% \n  \n  ggplot(aes(x = a, y = d)) +\n  geom_line(aes(group = draw),\n            color = \"firebrick\", alpha = .4) +\n  labs(x = \"Median age marriage (std)\",\n       y = \"Divorce rate (std)\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\nMuch better!\nLets test for the causal effect of marriage, while closing the fork\n\n\nCode\nmarriage_model <- \n  brm(data = Waffle_data, \n      family = gaussian,\n      d ~ 1 + m + a,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/marriage_model\")\n\nmarriage_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: d ~ 1 + m + a \n   Data: Waffle_data (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.10    -0.20     0.20 1.00     3514     2688\nm            -0.06      0.16    -0.38     0.26 1.00     2519     2660\na            -0.61      0.16    -0.93    -0.28 1.00     2626     2584\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.09     0.68     1.02 1.00     3296     2324\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOk the exponential distribution:\n\nConstrained to be positive\nOnly info is average displacement from 0, hence the one value, also called the rate.\nA value of 1 means roughly 1 SD from 0\nGreat for sigma\n\nLets look at the coefficients:\n\n\nCode\nmcmc_plot(marriage_model)\n\n\n\n\n\nThere is a very small effect of marriage, but we aren’t sure if this very small effect is positive or negative. Essentially no causal effect on divorce.\nAge of marriage has a large negative effect though.\n\\(~\\)\nGold standard: simulating an intervention from the data\n\n\nCode\npost <- as_draws_df(marriage_model)\n\nn <- 4000\nA <- sample(Waffle_data$a, size = n, replace = T)\n\n# simulate divorce for M = 0 \nbind_cols(\npost %>% \n  mutate(m_0 = rnorm(n, b_Intercept + b_m * 0 + b_a * A, sigma)) %>% \n  select(m_0),\n\n# now 1 sd above the mean\n\npost %>% \n  mutate(m_1 = rnorm(n, b_Intercept + b_m * 1 + b_a * A, sigma)) %>% \n  select(m_1)\n) %>% \n  mutate(diff = m_1 - m_0) %>% \n  \n  ggplot(aes(x = diff)) +\n  geom_density(fill = met.brewer(\"Hokusai2\")[1]) +\n    labs(x = \"Effect of 1 sd increase in M on D\") +\n  #coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-pipe",
    "href": "unimelb_statistical_rethinking_2023.html#the-pipe",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The Pipe",
    "text": "The Pipe\n\\(~\\)\nStructurally different to the fork, but statistically handled similarly.\nIn this case Z is a mediator variable. That is, the influence of X on Y is transmitted through Z.\n\n\nCode\np2\n\n\n\n\n\nSimulate to understand. Look carefully, we don’t code any effect of X on Y or vice-versa.\n\n\nCode\nn <- 1000\n\ntibble(X = rbernoulli(n, 0.5)) %>% \n  mutate(X = if_else(X == \"TRUE\", 1, 0),\n         Z = rbernoulli(n, (1 - X) * 0.1 + X * 0.9),\n         Y = rbernoulli(n, (1 - Z)* 0.1 + Z * 0.9),\n         Z = if_else(X == \"TRUE\", 1, 0),\n         Y = if_else(Y == \"TRUE\", 1, 0)) %>% \n  count(X, Y)\n\n\n# A tibble: 4 × 3\n      X     Y     n\n  <dbl> <dbl> <int>\n1     0     0   431\n2     0     1    90\n3     1     0   111\n4     1     1   368\n\n\nYet there appears to be an effect! This is because X effects Z which affects Y.\nAn example: a plant growth experiment\nCreate the data\n\n\nCode\n# how many plants would you like?\nn <- 100\n\n# h0 is the starting height\n# treatment is an antifungal\n# fungus is fungus presence\n# h1 is end of experiment plant height\n\nset.seed(71)\n\nplant_experiment <- \n  tibble(h0        = rnorm(n, mean = 10, sd = 2), \n         treatment = rep(0:1, each = n / 2),\n         fungus    = rbinom(n, size = 1, prob = .5 - treatment * 0.4),\n         h1        = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)) %>% \n  mutate(treatment = as.factor(treatment),\n         fungus = as.factor(fungus))\n\n\nThere are 100 plants troubled by fungal growth. Half the plants receive anti-fungal treatments. We measure growth and presence of fungus.\n\nThe estimand is the effect of anti-fungal treatment on growth.\nScientific model:\n\n\n\nCode\n# define our coordinates\ndag_coords <-\n  tibble(name = c(\"H0\", \"T\", \"F\", \"H1\"),\n         x    = c(1, 5, 4, 3),\n         y    = c(2, 2, 1.5, 1))\n\n# save our DAG\ndag <-\n  dagify(F ~ T,\n         H1 ~ H0 + F,\n         coords = dag_coords)\n\n# plot \ndag %>%\n gg_simple_dag()\n\n\n\n\n\nThis is a harder problem than it first appears\nThe correct stats analysis here is to ignore the fungal status. This is because this will block the pipe! We remove the effect of the desired part of the experiment.\nSee the model output\n\n\nCode\nfungal_model <- \n  brm(data = plant_experiment, \n      family = gaussian,\n      h1 ~ 0 + h0 + treatment + fungus,\n      prior = c(prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/fungal_model\")\n\nfungal_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: h1 ~ 0 + h0 + treatment + fungus \n   Data: plant_experiment (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nh0             1.34      0.03     1.28     1.41 1.00     1371     1559\ntreatment0     1.00      0.36     0.28     1.68 1.00     1468     2101\ntreatment1     1.46      0.37     0.71     2.16 1.00     1481     2258\nfungus1       -1.73      0.29    -2.28    -1.15 1.00     2674     2344\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.33      0.11     1.13     1.57 1.00     1784     2131\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe treatment works through fungal growth, so if we account for fungal growth, of course we will find no effect. So the message is not to stratify by an effect of the treatment you’re interested in.\nTo prove this, lets fit the more logical model:\n\n\nCode\nfungal_model_causal <- \n  brm(data = plant_experiment, \n      family = gaussian,\n      h1 ~ 0 + h0 + treatment,\n      prior = c(prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/fungal_model_causal\")\n\nfungal_model_causal\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: h1 ~ 0 + h0 + treatment \n   Data: plant_experiment (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nh0             1.34      0.04     1.27     1.41 1.00     1293     1603\ntreatment0    -0.01      0.37    -0.75     0.69 1.00     1454     1903\ntreatment1     1.04      0.38     0.30     1.79 1.00     1338     1618\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.72      0.12     1.49     1.98 1.00     2100     2128\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYou should be very wary of including consequences of your treatment other than the desired outcome variable because of the pipe.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-collider",
    "href": "unimelb_statistical_rethinking_2023.html#the-collider",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The Collider",
    "text": "The Collider\n\\(~\\)\n\n\nCode\np3\n\n\n\n\n\nX and Y are not associated but both influence Z.\nIf you stratify by Z, then X and Y become associated! How??\nIf we learn the value of Z we have to learn something about X and Y. Still confused?\nAn example: Selection bias in grants\n\nThere are 200 grant applications\nRated on trustworthiness and newsworthiness\nThese are uncorrelated variables\n\nLoad in the data and plot the relationship\n\n\nCode\nset.seed(1914)\nn <- 200  # number of grant proposals\np <- 0.1  # proportion to select\n\ngrants <-\n  # uncorrelated newsworthiness and trustworthiness\n  tibble(newsworthiness  = rnorm(n, mean = 0, sd = 1),\n         trustworthiness = rnorm(n, mean = 0, sd = 1)) %>% \n  # total_score\n  mutate(total_score = newsworthiness + trustworthiness) %>% \n  # select top 10% of combined scores\n  mutate(selected = ifelse(total_score >= quantile(total_score, 1 - p), TRUE, FALSE))\n\n\n\n\nCode\n# we'll need this for the annotation\ntext <-\n  tibble(newsworthiness  = c(2, 1), \n         trustworthiness = c(2.25, -2.5),\n         selected        = c(TRUE, FALSE),\n         label           = c(\"selected\", \"rejected\"))\n\ngrants %>% \n  ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +\n  geom_point(aes(shape = selected), alpha = 3/4, stroke = 1.4) +\n  geom_text(data = text,\n            aes(label = label)) +\n  geom_smooth(data = . %>% filter(selected == TRUE),\n              method = \"lm\", fullrange = T,\n              color = \"lightblue\", se = F, size = 1/2) +\n  scale_color_manual(values = c(\"black\", \"lightblue\")) +\n  scale_shape_manual(values = c(1, 19)) +\n  scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +\n  coord_cartesian(ylim = range(grants$trustworthiness)) +\n  theme(legend.position = \"none\") +\n  theme_minimal()\n\n\n\n\n\nAfter selection there is a strong negative correlation.\nOk, so due to selection the only grants that make it are either high in newsworthiness or trustworthiness (makes sense). From a random distribution few grants will be high in both (this is much rarer than being high in one of the categories), The result is a subset of grants that are good at one thing but mainly bad at the other, which creates the appearance of a negative correlation across the entire sample. The key is to not subset the data (don’t stratify)\nExample 2: selection bias in restaurants\n\nRestaurants are successful because they have good food or because they’re in a good location.\nOnly the bad restaurants in really good locations can survive.\nCreates a negative relationship between food quality and location.\n\nExample 3: selection bias in actors\n\nActors can be successful because they are skilled or because they are good looking.\nReally bad actors can survive if they are attractive - they end up being the only less skilled actors that can survive.\nSelection creates a negative relationship between looks and skill.\n\n\\(~\\)\nStats example: endogenous colliders\nIf you condition on a collider, you create phantom non-causal associations.\nExample: Does age influence happiness?\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"H\", \"M\", \"A\"),\n         x    = 1:3,\n         y    = 1)\n\ndagify(M ~ H + A,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\n\nThe estimand: Age affects happiness\n\nThis is possibly confounded by marital status.\nSuppose age has no effect on happiness but both affect marital status.\nSo the collider scenario here is that of the married people, only a very advanced age can overcome unhapiness or a very happy dispoistion can overcome a very young age = a neg relationship\nLets sim this to visualise how\n\n\nCode\nmarriage_sim <- rethinking::sim_happiness(seed = 1977, N_years = 1000)\n\nmarriage_sim %>% \n  mutate(married = factor(married,\n                          labels = c(\"unmarried\", \"married\"))) %>% \n  \n  ggplot(aes(x = age, y = happiness, color = married)) +\n  geom_point(size = 1.75) +\n  scale_color_manual(NULL, values = c(\"grey85\", \"forestgreen\")) +\n  scale_x_continuous(expand = c(.015, .015)) +\n  theme(panel.grid = element_blank()) +\n  theme_minimal()\n\n\n\n\n\nIf the visualisation isn’t enough to convince you, lets fit the models.\nFirst with the collider\n\n\nCode\nmarriage_sim_2 <-\n  marriage_sim %>% \n  mutate(mid = factor(married + 1, labels = c(\"single\", \"married\"))) %>% \n  # only inlcude those 18 and above\n  filter(age > 17) %>% \n  # create a, a standarised variable where 18 = 0 and 65 = 1\n  mutate(a = (age - 18) / (65 - 18))\n\nmarriage_happiness_model <- \n  brm(data = marriage_sim_2, \n      family = gaussian,\n      happiness ~ 0 + mid + a,\n      prior = c(prior(normal(0, 1), class = b, coef = midmarried),\n                prior(normal(0, 1), class = b, coef = midsingle),\n                prior(normal(0, 2), class = b, coef = a),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 6,\n      file = \"fits/marriage_happiness_model\")\n\nmarriage_happiness_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: happiness ~ 0 + mid + a \n   Data: marriage_sim_2 (Number of observations: 960) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmidsingle     -0.24      0.06    -0.36    -0.11 1.00     1732     2058\nmidmarried     1.26      0.08     1.09     1.42 1.00     1758     1943\na             -0.75      0.11    -0.96    -0.53 1.00     1508     2084\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.99      0.02     0.95     1.04 1.00     2479     2033\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs we expected, age appears to be negatively associated with happiness.\nNow remove the collider\n\n\nCode\nmarriage_happiness_model_causal <- \n  brm(data = marriage_sim_2, \n      family = gaussian,\n      happiness ~ 0 + a,\n      prior = c(prior(normal(0, 2), class = b, coef = a),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 6,\n      file = \"fits/marriage_happiness_model_causal\")\n\nmarriage_happiness_model_causal\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: happiness ~ 0 + a \n   Data: marriage_sim_2 (Number of observations: 960) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na    -0.00      0.07    -0.13     0.14 1.00     3281     2571\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.21      0.03     1.16     1.27 1.00     3192     2242\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAge has literally no effect on happiness now, as we specified in the simulation.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-descendant",
    "href": "unimelb_statistical_rethinking_2023.html#the-descendant",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The Descendant",
    "text": "The Descendant\n\\(~\\)\n\n\nCode\nd4 <- \n  dagify(Z ~ X + Y,\n         D ~ Z,\n         coords = tibble(name = c(\"X\", \"Y\", \"Z\", \"D\"),\n                                x = c(1, 3, 2, 2),\n                                  y = c(1, 1, 2, 1.05))\n         )\n\np4 <- gg_simple_dag(d4) + labs(subtitle = \"The Descendant\")\n\np4\n\n\n\n\n\nThe variable A is the descendant of the Z variable.\nThe consequence is if we condition (stratify) by A it will have the same effect as conditioning by Z. While A does not affect X or Y, it is related to Z so it has a weaker (depending on the A -> association strength) but tangible effect on X/Y.\nThink about how common descendants are. Any proxy is a descendant!\nE.g. all fitness measures are proxies and therefore descendants of true fitness.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#unobserved-confounds",
    "href": "unimelb_statistical_rethinking_2023.html#unobserved-confounds",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Unobserved confounds",
    "text": "Unobserved confounds\n\\(~\\)\nSuppose we are interested in the direct effect of grandparents education on their grandchildrens education.\n\nThey will have direct and indirect effects on these kids\n\nEstimand: Direct effect of grandparents G on children C.\nBut parent education is obviously an issue here.\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"G\", \"P\", \"C\", \"U\"),\n         x    = c(1, 2, 2, 3),\n         y    = c(2, 2, 1, 1.5))\n\ndagify(P ~ G + U,\n       C ~ P + G + U,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nThe parental effect is likely confounded with living conditions. However, this might not be the case for grandparents who live in a different area.\nRemember what our goal is: estimate the direct causal effect of grandparents.\nIf the situation was this:\n\n\nCode\ndagify(P ~ G,\n       C ~ P + G,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nThen we would just have to stratify by P to block the pipe and find the direct effect of G.\nUnfortunately, U exists, which makes P a collider! So this makes P potentially associated with C because U affects them both.\nWe are left with only bad choices. Sometimes this is the way it is.\nWhy does collider bias happen: continuing with the parental example.\nLet’s simulate the data, with G having no causal effect on C.\n\n\nCode\n# how many grandparent-parent-child triads would you like?\nn    <- 200 \n\nb_gp <- 1  # direct effect of G on P\nb_gc <- 0  # direct effect of G on C\nb_pc <- 1  # direct effect of P on C\nb_u  <- 2  # direct effect of U on P and C\n\n# simulate triads\nset.seed(1)\n\nd <-\n  tibble(u = 2 * rbinom(n, size = 1, prob = .5) - 1,\n         g = rnorm(n, mean = 0, sd = 1)) %>% \n  mutate(p = rnorm(n, mean = b_gp * g + b_u * u, sd = 1)) %>% \n  mutate(c = rnorm(n, mean = b_pc * p + b_gc * g + b_u * u, sd = 1))\n\n\nNow we’ll fit the model without U - which closes the pipe but opens the collider.\n\n\nCode\nparent_collider_model <- \n  brm(data = d, \n      family = gaussian,\n      c ~ 0 + Intercept + p + g,\n      prior = c(prior(normal(0, 1), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 6,\n      file = \"fits/parent_collider_model\")\n\nparent_collider_model\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: c ~ 0 + Intercept + p + g \n   Data: d (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.12      0.10    -0.31     0.07 1.00     4158     2829\np             1.79      0.05     1.70     1.87 1.00     3550     3015\ng            -0.84      0.11    -1.04    -0.63 1.00     3927     2951\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.43      0.07     1.30     1.57 1.00     4142     2926\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLets plot what’s happening\n\n\nCode\nd %>% \n  mutate(centile = ifelse(p >= quantile(p, prob = .45) & p <= quantile(p, prob = .60), \"a\", \"b\"),\n         u = factor(u)) %>%\n  \n  ggplot(aes(x = g, y = c)) +\n  geom_point(aes(shape = centile, color = u),\n             size = 2.5, stroke = 1/4) +\n  stat_smooth(data = . %>% filter(centile == \"a\"),\n              method = \"lm\", se = F, size = 1/2, color = \"black\", fullrange = T) +\n  scale_shape_manual(values = c(19, 1)) +\n  scale_color_manual(values = c(\"black\", \"lightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nStratifying by P as the regression line shows creates a negative relationship where nothing causal is happening!\nNow we’ll fit the model with U\n\n\nCode\ngrandparent_total <- \n  update(parent_collider_model,\n         newdata = d,\n         formula = c ~ 0 + Intercept + p + g + u,\n         seed = 6,\n         file = \"fits/grandparent_total\")\n\ngrandparent_total\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: c ~ Intercept + p + g + u - 1 \n   Data: d (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.12      0.07    -0.26     0.03 1.00     3264     2707\np             1.01      0.07     0.88     1.15 1.00     1603     2175\ng            -0.04      0.10    -0.24     0.15 1.00     2016     2141\nu             1.99      0.15     1.70     2.28 1.00     1621     2264\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.04      0.05     0.94     1.14 1.00     2871     2527\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote the change in the effect of G on C. Remember that we coded it to have 0 effect in our simulation!\nIn short: there are two ways for parents to attain their education: from G or from U. This is the classic setup for a collider.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#dag-thinking",
    "href": "unimelb_statistical_rethinking_2023.html#dag-thinking",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "DAG thinking",
    "text": "DAG thinking\n\\(~\\)\nIn an experiment we aim to cut all the causes of the treatment through randomisation.\nBut if we don’t randomise, can we overcome this with statistics? Sometimes.\nFirst some notation:\n\\(P(Y|do(X)) = P(Y|?)\\)\nWhere P(Y) is the distribution of Y\nSo the above means the distribution of Y conditional upon do(X) equals the distribution of Y conditional on some set of information.\ndo(X) means to intervene on X - to set it to some particular value.\nWe can do this with a DAG!\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"U\", \"X\", \"Y\"),\n         x    = c(2, 1, 3),\n         y    = c(2, 1, 1))\n\ndagify(X ~ U,\n       Y ~ X + U,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nIf we don’t know U we can find the distribution of Y, stratified by X and U, averaged over the distribution of U. But how?\nRemember that the causal effect of X on Y is not the coefficient relating X to Y. It is actually the distribution of Y when we change X, averaged over the distributions of the control variables (U in this case). This is a marginal effect."
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#do-calculus",
    "href": "unimelb_statistical_rethinking_2023.html#do-calculus",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Do calculus",
    "text": "Do calculus\nThe back-door criterion\nA way to analyse graphs to find sets of variables to stratify by\nIt can also help us design research.\nFormally it is a rule to find a set of variables to stratify (condition) by to yield \\(P(Y|do(X))\\)\nThe steps:\n\nIdentify all paths connecting the treatment (X) to the outcome (Y).\nOnce you have the paths, focus on the paths that enter X. These are back-door paths (non-causal).\nFind adjustment set (set of variables to stratify by) that closes/blocks all back-door paths."
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#controls",
    "href": "unimelb_statistical_rethinking_2023.html#controls",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Controls",
    "text": "Controls\nControl variable: a variable we introduce to an analysis so that a causal estimate of something else is possible.\nThings you should not do:\n\nAdd everything you’ve measured\nIncluding all variables that aren’t collinear: sometimes both variables play a role in causality\nAnything measured before treatment is fair game to add - not true\n\nTaking some examples from Cinelli, Forney and Pearl 2021: A crash course in good and bad controls\nExample 1\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"u\", \"X\", \"Z\", \"v\", \"Y\"),\n         x    = c(1, 1, 2, 3, 3),\n         y    = c(2, 1, 1.6, 2, 1))\n\ndagify(X ~ u,\n       Z ~ u + v,\n       Y ~ v + X,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nWhere u and v are unobserved variables.\nLet’s say that Z denotes friendship status and X and Y the health of two people. u and v are the hobbies of person X and person Y.\nIf we stratify by Z, that is, include it as a control, then we open a back-door path (it is a collider). This is bad for estimating the causal effect of X on Y.\nNote that Z could be a pre-treatment variable (I haven’t been taught this but apparently it’s a thing).\nDo calculus time - list the paths:\n\nX -> Y: front door\nX <- u -> Z <- v -> Y: backdoor. Closed unless you include Z. A bad control.\n\nExample 2\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"X\", \"Z\", \"u\", \"Y\"),\n         x    = c(1, 2, 2.5, 3),\n         y    = c(1, 1, 1.2, 1))\n\ndagify(Z ~ u + X,\n       Y ~ Z + u,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nAnalyse the paths:\nX -> Z -> Y: front\nX -> Z <- u -> Y: still front\nZ is a mediator here, so if the goal to to estimate the effect of X on Y, we should not include Z in the model anyway. There is no back-door path we have to block.\nHowever, it’s worse than that, because u - an unexplained variable - creates a fork. Including Z makes the estimate even worse than it would usually. This should always be noted where things might have a common cause e.g. happiness and lifespan or body size and fecundity.\nAdd sim and plot\n\n\n\nNote that Z would turn out to be significant. This is spurious! Don’t do backwards step selection. You need a causal model.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#do-not-touch-the-collider",
    "href": "unimelb_statistical_rethinking_2023.html#do-not-touch-the-collider",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Do not touch the collider",
    "text": "Do not touch the collider\n\\(~\\)\nDAG example\nCase control bias\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"X\", \"Z\", \"Y\"),\n         x    = c(1, 2, 2),\n         y    = c(1, 2, 1))\n\ndagify(Z ~ Y,\n       Y ~ X,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nOk, so this is a simple pipe. But if we want to know the causal effect of X on Y then we run into an issue if we control for Z.\nImagine that X = education, Y = occupation and Z = Income. If we control for income, then this removes variation in Y, leaving less for education to explain - the effect size is underestimated. Now Z is not a cause of Y, but the model has no way of knowing that so it just measures the covariance. We are required to make this distinction.\nA common bad control. When you apply selection on the outcome - this ruins scientific influence.\nE.g. if you want to know the effect of education on occupation then you should not stratify by income because it narrows the range of cases you compare at each level.\nSim this one as well code at 58.34\n\\(~\\)\nPrecision parasite\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"X\", \"Z\", \"Y\"),\n         x    = c(1, 1, 2),\n         y    = c(1, 2, 1))\n\ndagify(X ~ Z,\n       Y ~ X,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nWhile Z affects X it is not a back-door path because it does not connect to Y (except via the front door). However, it is still not good to condition on Z.\nThe parasite does not change the mean estimate, but it creates an estimate with higher variance. We lose precision.\nAnother sim\n\\(~\\)\nBias amplification\nA bad version of the precision parasite.\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"X\", \"Z\", \"Y\", \"u\"),\n         x    = c(1, 1, 2, 1.5),\n         y    = c(1, 2, 1, 1.75))\n\ndagify(X ~ Z + u,\n       Y ~ X + u,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nRemember u is an unmeasured variable. We can’t get an unbiased causal inference in this scenario.\nIf we add Z, the world implodes. We get a really biased estimate, with an inflated effect size.\nWHY?\nCovariation between X and Y requires variation in their causes. Variation is removed by conditioning - we are accounting for this association then finding the variance explained by the remaining variables. So stratifying on X removes variation in X, which gives more weight to be explained by the confound U = stronger false relationship!\nHard to get my head around but wow.\nLets plot to help understanding.\nWe’ll sim a situation like the above. Z is a binomial variable that causally affects X. X has no effect on Y. However, u affects both X and Y.\n\n\nCode\nn <- 1000\nZ <- rethinking::rbern(n)\nu <- rnorm(n)\nX <- rnorm(n, 7*Z + u)\nY <- rnorm(n, 0*X + u)\n\ntibble(Z = as.factor(Z),\n       u = u,\n       X = X,\n       Y = Y) %>% \n  \n  ggplot(aes(x = X, y = Y, colour = Z)) +\n  geom_point(aes(shape = Z), stroke =2, size = 1.2, alpha = 0.5) +\n  geom_smooth(\n    method = \"lm\", fullrange = T,\n    color = \"black\", se = F, size = 1) +\n  geom_smooth(data = . %>% filter(Z == \"1\"),\n              method = \"lm\", fullrange = T,\n              color = \"blue\", se = F, size = 1) +\n  geom_smooth(data = . %>% filter(Z == \"0\"),\n              method = \"lm\", fullrange = T,\n              color = \"red\", se = F, size = 1) +\n  scale_shape_manual(values = c(1, 19)) \n\n\n\n\n\nThere is less variation in each ‘stratification’!\n\\(~\\)\nThe bias is amplified,\nWhy?\n\nX can’t vary unless its causes vary\nIf Z doesn’t vary then X can’t vary. Stratifying by this removes variance in X and in that way Y. This means a larger proportion of the variation in X comes from the confounding fork u.\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#the-table-2-fallacy",
    "href": "unimelb_statistical_rethinking_2023.html#the-table-2-fallacy",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "The Table 2 fallacy",
    "text": "The Table 2 fallacy\n\\(~\\)\nIn many fields the typical Table 2 in a manuscript will contain model coefficients.\nThe thing is, not all the coefficients are causal effects and there is no information that tells you whether they are.\nRemember you have designed your model to identify the causal effect of X on Y. It is dangerous to interpret the effects of other variables that you have not designed the model for. The other variables are there to block back-door paths.\n\n\nCode\ndag_coords <-\n  tibble(name = c(\"S\", \"A\", \"X\", \"Y\"),\n         x    = c(1, 1, 2, 3),\n         y    = c(3, 1, 2, 2))\n\ndagify(S ~ A,\n       X ~ S + A,\n       Y ~ S + A + X,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nS = smoking\nA = age\nX = HIV\nY = stroke\nWe want to know the effect of X on Y - the effect of HIV on stroke risk.\nThings to note:\n\nAge has many effects, including some quite indirect ones\nWe need to decontaminate X of the effects of smoking and age.\n\nBack-door criterion:\nList paths\n\nX -> Y\nX <- S -> Y\nX <- A -> Y\nX <- A -> S -> Y\n\nWe need to close paths 2-4.\nWe can condition by A and S to effectively do this.\nSo we could fit a model that looks like this:\n\\[Y = X + S + A\\] What if we focus on the effect of S?\n\n\nCode\ndagify(S ~ A,\n       X ~ S + A,\n       Y ~ S + A + X,\n       coords = dag_coords) %>%\n  gg_simple_dag()\n\n\n\n\n\nS is confounded by A, as there is a back-door path. But we condition on A and X so that back-door path is closed.\nBUT\nWhat are we estimating for S? Well we have removed the indirect effect of S that acts through X, as we have closed that pipe. I.e. We are only estimating how smoking directly affects stroke, but not including how smoking acts on stroke through increasing the risk of HIV. This might be small or it might be large, we don’t know. The take home is this coefficient measures something different than that of X.\nNow lets focus on Age.\nIt acts through everything, so we need not include any of the conditional variables to find its total effect on stroke (both direct and indirect). But we have conditioned on S and X. We have closed these pathways. We only get the direct effect, which may be tiny!\nTake-home: don’t interpret all coefficients as unconditional effects!\nHow to report results\nEither\n\nJust report causal coefficients\nGive an explicit interpretation of each coefficient\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#cross-validation",
    "href": "unimelb_statistical_rethinking_2023.html#cross-validation",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Cross Validation",
    "text": "Cross Validation\n\\(~\\)\n\nFor simple models, more parameters improves fit within sample but may reduce accuracy out of sample\nThere is a trade off between flexibility and overfitting\n\nWe can test how effective our linear regression is at prediction using Leave-one-out-cross-validation\nThe steps:\n\nDrop one point\nFit line to remaining points\nPredict dropped point\nRepeat (1) with next point\nScore based on the error between actual point and predicted point of the regression\n\n\n\nCode\nhominin_brain_mass_model <- \n  brm(data = hominin_data, \n      family = gaussian,\n      brain_std ~ 1 + mass_std,\n      prior = c(prior(normal(0.5, 1), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(lognormal(0, 1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 7,\n      file = \"fits/hominin_brain_mass_model\")\n\n\nBayesian Cross-Validation\nAs usual we use distributions in bayes rather than points. The predictive distribution is uncertain, we don’t get a specific point estimate for the prediction. A cross-validation score is a bit harder to get over a distribution, but luckily we can follow the log pointwise predictive density equation. I won’t display it here because we can depend on the software. Just know that it is referred to as \\(lppd_{CV}\\). Essentially this goes point by point like above, finds the posterior distribution, then averages this distribution over the number of samples (I think).\nSome definitions:\nIn sample score: summed deviance of points from the full sample regression\nOut of sample score: summed deviance of points from each out of sample regression\nLet’s fit more complex models\n\n\nCode\n# quadratic\nb7.2 <- \n  update(hominin_brain_mass_model,\n         newdata = hominin_data, \n         formula = brain_std ~ 1 + mass_std + I(mass_std^2),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         file = \"fits/b07.02\")\n\n# cubic\nb7.3 <- \n  update(hominin_brain_mass_model,\n         newdata = hominin_data, \n         formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         control = list(adapt_delta = .9),\n         file = \"fits/b07.03\")\n\n\n# fourth-order\nb7.4 <- \n  update(hominin_brain_mass_model,\n         newdata = hominin_data, \n         formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         control = list(adapt_delta = .995),\n         file = \"fits/b07.04\")\n\n# fifth-order\nb7.5 <- \n  update(hominin_brain_mass_model,\n         newdata = hominin_data, \n         formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5),\n         iter = 2000, warmup = 1000, chains = 4, cores = 4,\n         seed = 7,\n         control = list(adapt_delta = .99999),\n         file = \"fits/b07.05\")\n\n# make a function for plotting\n\nmake_figure7.3 <- function(brms_fit, ylim = range(hominin_data$brain_std)) {\n  \n  \n  # define the new data \n  nd <- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))\n  \n  # simulate and wrangle\n  fitted(brms_fit, newdata = nd, probs = c(.055, .945)) %>% \n    data.frame() %>% \n    bind_cols(nd) %>% \n    \n    # plot!  \n    ggplot(aes(x = mass_std)) +\n    geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n                    color = met.brewer(\"Cassatt1\")[1], size = 1/2, \n                    fill = alpha(met.brewer(\"Cassatt1\")[2], 1/3)) +\n    geom_point(data = hominin_data,\n               aes(y = brain_std),\n               color = met.brewer(\"Cassatt1\")[2],\n               size = 5) +\n    labs(x = \"body mass (std)\",\n         y = \"brain volume (std)\") +\n    coord_cartesian(xlim = c(-1.2, 1.5),\n                    ylim = ylim) +\n    theme_classic() +\n    theme(text = element_text(family = \"Courier\"),\n          panel.background = element_rect(fill = alpha(met.brewer(\"Cassatt1\", 8)[4], 1/4)))\n  \n}\n\np1 <- make_figure7.3(hominin_brain_mass_model)\np2 <- make_figure7.3(b7.2)\np3 <- make_figure7.3(b7.3)\np4 <- make_figure7.3(b7.4, ylim = c(.25, 1.1))\np5 <- make_figure7.3(b7.5, ylim = c(.1, 1.4))\n\n\n((p1 | p2) / (p3 | p4) / (p5)) +\n  plot_annotation(title = \"Figure7.3. Polynomial linear models of increasing\\ndegree for the hominin data.\")\n\n\n\n\n\nSo how does this affect our fit?\nThe in-sample score improves with model complexity, but the out of sample score gets worse (larger)! For the 4th order polynomial model in sample is insanely good, but the fit varies wildly once we remove a point. The take home is that we can build really complex models that fit the observed data really well, but have little to no predictive capacity for new data points. This is the basic trade-off inherent to modelling: over fitting leads to bad predictions.\nBut what if we have more data?\nWith more data we find that there is an intermediate model complexity (the 2nd degree model in this case) that maximises both within and out of sample deviance; that is, it is the best at describing these data. However, this doesn’t mean that it explains them, it’s just best at pure prediction in the absence of intervention. If we want to do this, we need to understand the causal structure.\n\n\n\n\n\n\nFrom McElreath\n\n\n\nThe overfit polynomial models fit the data extremely well, but they suffer for this within-sample accuracy by making nonsensical out-of-sample predictions. In contrast, underfitting produces models that are inaccurate both within and out of sample. They learn too little, failing to recover regular features of the sample. (p. 201)\n\n\n\\(~\\)\nTo explore the distinctions between overfitting and underfitting, we’ll need to refit the models above several times after serially dropping one of the rows in the data. You can filter() by row_number() to drop rows in a tidyverse kind of way. For example, we can drop the second row of hominin_data like this.\n\n\nCode\nhominin_data %>%\n  mutate(row = 1:n()) %>% \n  filter(row_number() != 2)\n\n\n# A tibble: 6 × 6\n  species     brain  mass mass_std brain_std   row\n  <chr>       <dbl> <dbl>    <dbl>     <dbl> <int>\n1 afarensis     438  37     -0.779     0.324     1\n2 habilis       612  34.5   -1.01      0.453     3\n3 boisei        521  41.5   -0.367     0.386     4\n4 rudolfensis   752  55.5    0.917     0.557     5\n5 ergaster      871  61      1.42      0.645     6\n6 sapiens      1350  53.5    0.734     1         7\n\n\nNow we’ll make a function brain_loo_lines() that will refit the model and extract lines information in one step.\n\n\nCode\nnd <- tibble(mass_std = seq(from = -2, to = 2, length.out = 100))\n\nbrain_loo_lines <- function(brms_fit, row, ...) {\n  \n  # refit the model\n  new_fit <- \n    update(brms_fit,\n           newdata = filter(hominin_data, row_number() != row), \n           iter = 2000, warmup = 1000, chains = 4, cores = 4,\n           seed = 7,\n           refresh = 0,\n           ...)\n  \n  # pull the lines values\n  fitted(new_fit, \n         newdata = nd) %>% \n    data.frame() %>% \n    select(Estimate) %>% \n    bind_cols(nd)\n  \n}\n\n\nNow use map() to iterate the function on all LOO dataset versions\n\n\nCode\nhominin_fits <-\n  tibble(row = 1:7) %>% \n  mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = hominin_brain_mass_model, row = .))) %>% \n  unnest(post)\n\nhominin_polynomial_fits <-\n  tibble(row = 1:7) %>% \n  mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = b7.4, \n                                                 row = ., \n                                                 control = list(adapt_delta = .999)))) %>% \n  unnest(post)\n\n\nPlot the results\n\n\nCode\n# left\np1 <-\n  hominin_fits %>%  \n  \n  ggplot(aes(x = mass_std)) +\n  geom_line(aes(y = Estimate, group = row),\n            color = met.brewer(\"Cassatt1\", 8)[2], linewidth = 1/2, alpha = 1/2) +\n  geom_point(data = hominin_data,\n             aes(y = brain_std),\n             color = met.brewer(\"Cassatt1\", 8)[2],\n             size = 5) +\n  labs(x = \"body mass (std)\",\n       y = \"brain volume (std)\") +\n  coord_cartesian(xlim = range(hominin_data$mass_std),\n                  ylim = range(hominin_data$brain_std)) +\n  theme_classic() +\n  theme(text = element_text(family = \"Courier\"),\n        panel.background = element_rect(fill = alpha(met.brewer(\"Cassatt1\", 8)[4], 1/4)))\n\n# right\np2 <-\n  hominin_polynomial_fits %>%  \n  \n  ggplot(aes(x = mass_std, y = Estimate)) +\n  geom_line(aes(group = row),\n            color = met.brewer(\"Cassatt1\", 8)[2], linewidth = 1/2, alpha = 1/2) +\n  geom_point(data = hominin_data,\n             aes(y = brain_std),\n             color = met.brewer(\"Cassatt1\", 8)[2],\n             size = 5) +\n  labs(x = \"body mass (std)\",\n       y = \"brain volume (std)\") +\n  coord_cartesian(xlim = range(hominin_data$mass_std),\n                  ylim = c(-0.1, 1.4)) +\n  theme_classic() +\n  theme(text = element_text(family = \"Courier\"),\n        panel.background = element_rect(fill = alpha(met.brewer(\"Cassatt1\", 8)[4], 1/4)))\n\n# combine\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nFrom McElreath\n\n\n\n“Notice that the straight lines hardly vary, while the curves fly about wildly. This is a general contrast between underfit and overfit models: sensitivity to the exact composition of the sample used to fit the model” (p. 201)."
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#regularisation",
    "href": "unimelb_statistical_rethinking_2023.html#regularisation",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Regularisation",
    "text": "Regularisation\n\\(~\\)\nWe can design models that are better at prediction in their structure.\nWe want to identify regular features in the data, this leads to good predictions\nWe can minimise over fitting with:\nPriors:\n\nSceptical priors have tighter variance, and reduce the flexibility of a model\nOften tighter than we think they need to be\nThese improve predictions\nEssentially, tight priors stop the model overreacting to nonsense within the sample\n\nChoosing the width of a prior\n\nFor causal inference, use science, then go a touch tighter.\nFor pure prediction, we can use cross-validation to tune the prior.\nMany tasks are a mix of both\n\nPrediction penalty\nFor N points, cross-validation requires fitting N models. This quickly becomes computationally expensive, when your dataset gets larger.\nGood news! We can compute the prediction penalty from a single model fit. There are two ways:\n\nImportance sampling (PSIS)\nInformation criteria (WAIC or LOO)\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#predictive-criteria-should-not-be-casued-to-choose-model-structure",
    "href": "unimelb_statistical_rethinking_2023.html#predictive-criteria-should-not-be-casued-to-choose-model-structure",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Predictive criteria should not be casued to choose model structure",
    "text": "Predictive criteria should not be casued to choose model structure\n\nThey often prefer confounds and colliders\n\nExample: return of the plant growth experiment\nThe DAG looks like this:\n\n\nCode\n# define our coordinates\ndag_coords <-\n  tibble(name = c(\"H0\", \"T\", \"F\", \"H1\"),\n         x    = c(1, 5, 4, 3),\n         y    = c(2, 2, 1.5, 1))\n\n# save our DAG\ndag <-\n  dagify(F ~ T,\n         H1 ~ H0 + F,\n         coords = dag_coords)\n\n# plot \ndag %>%\n gg_simple_dag()\n\n\n\n\n\nAnd the data:\n\n\nCode\nhead(plant_experiment)\n\n\n# A tibble: 6 × 4\n     h0 treatment fungus    h1\n  <dbl> <fct>     <fct>  <dbl>\n1  9.14 0         0       14.3\n2  9.11 0         0       15.6\n3  9.04 0         0       14.4\n4 10.8  0         0       15.8\n5  9.16 0         1       11.5\n6  7.63 0         0       11.1\n\n\nNow lets add the information criterion loo (leave one out) to the model object. loo is the PSIS metric McElreath spruiks. Then plot the result.\n\n\nCode\nfungal_model <- add_criterion(fungal_model, criterion = \"loo\")\nfungal_model_causal <- add_criterion(fungal_model_causal, criterion = \"loo\")\n\nloo <- loo_compare(fungal_model, fungal_model_causal, criterion = \"loo\")\n\nprint(loo, simplify = F)\n\n\n                    elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo\nfungal_model           0.0       0.0  -172.6      6.4         3.4    0.4  \nfungal_model_causal  -25.2      10.0  -197.9      7.2         3.0    0.5  \n                    looic  se_looic\nfungal_model         345.3   12.7  \nfungal_model_causal  395.7   14.4  \n\n\n\n\nCode\nloo[, 7:8] %>% \n  data.frame() %>% \n  rownames_to_column(\"model_name\") %>% \n  mutate(model_name = fct_reorder(model_name, looic, .desc = T)) %>% \n  \n  ggplot(aes(x = looic, y = model_name, \n             xmin = looic - se_looic, \n             xmax = looic + se_looic)) +\n  geom_pointrange(color = met.brewer(\"Cassatt1\", 8)[2],\n                  fill = met.brewer(\"Cassatt1\", 8)[1], shape = 21) +\n  labs(x = \"PSIS-LOO\", y = NULL) +\n  theme(axis.ticks.y = element_blank()) +\n  theme_classic() +\n  theme(text = element_text(family = \"Courier\"),\n        panel.background = element_rect(fill = alpha(met.brewer(\"Cassatt1\", 8)[4], 1/4)))\n\n\n\n\n\nFungus status is actually a better predictor of plant growth than anti-fungal treatment. Makes sense, the treatment is essentially an imperfect proxy for fungal status. For prediction this is better, but it’s terrible for determining the effect of the treatment. See that your preferred model depends entirely upon your aim.\n\n\nCode\np1 <-\n  plant_experiment %>% \n  mutate(growth = h1 - h0) %>% \n  ggplot(aes(x = fungus, y = growth, colour = treatment)) +\n  geom_jitter(size = 3, shape = 1.5, stroke =2, alpha = 0.75) +\n  labs(x = \"Presence of fungus\",\n       y = \"Plant growth\") + \n  theme_classic()\n\np2 <-\n  plant_experiment %>% \n  mutate(growth = h1 - h0) %>% \n  ggplot(aes(x = treatment, y = growth, colour = fungus)) +\n  geom_jitter(size = 3, shape = 1.5, stroke =2, alpha = 0.75) +\n  labs(x = \"Presence of fungus\",\n       y = \"Plant growth\") +\n  theme_classic()\n\n\np1 / p2\n\n\n\n\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#importance-sampling",
    "href": "unimelb_statistical_rethinking_2023.html#importance-sampling",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Importance sampling",
    "text": "Importance sampling\n\\(~\\)\nUse a single posterior distribution for N points to sample for each posterior for N-1 points.\nKey idea: Point with low probability has a strong influence on posterior distribution. Highly probable points on the other hand do not individually affect the posterior distribution much, because there are other points in close proximity conveying very similar information.\nCan use pointwise probabilities to re-weight samples from the posterior.\nBUT, importance sampling tends to be unreliable.\nThe best current one: Pareto-smoothed importance sampling\n\nMore stable (lower variance)\nUseful diagnostics (it will tell you when it’s bad aka has high variance)\nIdentifies important (high leverage) points (outliers)\n\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#information-criteria",
    "href": "unimelb_statistical_rethinking_2023.html#information-criteria",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Information criteria",
    "text": "Information criteria\n\\(~\\)\nAkaike information criteria: AIC\nThe first form. For flat priors and large samples:\n\\(AIC = (-2) * lppd + 2k\\)\nWhere -2 is a scaling term, lppd is the log pointwise predictive density and 2k is a penalty term for complexity, where k is the number of parameters.\nIf we want to use good priors, we can’t use it.\n\\(~\\)\nWidely applicable information criterion: WAIC\n\\(WAIC(y, \\Theta) = -2(lppd - \\sum var_{\\Theta} log p(y_{i}|\\Theta))\\)\nThis is a more complex formula but is has some similarities to the AIC formula.\n\\(\\sum var_{\\Theta} log p(y_{i}|\\Theta)\\) is the penalty term now.\n\\(WAIC\\) provides a very similar result to PSIS, but without the diagnostics.\n\\(~\\)"
  },
  {
    "objectID": "unimelb_statistical_rethinking_2023.html#outliers-and-robust-regression",
    "href": "unimelb_statistical_rethinking_2023.html#outliers-and-robust-regression",
    "title": "Statistical rethinking 2023 (brms examples)",
    "section": "Outliers and robust regression",
    "text": "Outliers and robust regression\n\\(~\\)\nSome points are more important for the posterior distribution than others.\nOutliers are observations in the tails of predictive distributions\nIf a model has outliers, this indicates the model doesn’t expect enough variation. This can lead to poor, damaging predictions.\nSay we want to predict hurricane strength. We really don’t want to disregard extreme hurricanes, as this would be disastrous.\nDon’t drop outliers, it’s the models fault, not the data’s. But how do we deal with them?\n\nIt’s the model that’s wrong, not the data\nFirst, quantify influence of each point - we can do this with cross validation\nSecond, use a mixture model with fatter tails i.e. the student t distribution.\n\n\\(~\\)\nRobust regression\nWe’re back at the divorce rate example, where Idaho and Maine are outliers. Idaho has the lowest median marriage age and a very low divorce rate, while Maine has a very high divorce rate, but an average age of marriage.\n\n\nCode\nb5.3 <- \n  brm(data = Waffle_data, \n      family = gaussian,\n      d ~ 1 + m + a,\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.03\")\n\nb5.3 <- add_criterion(b5.3, criterion = \"loo\")\n\nb5.3 <- add_criterion(b5.3, \"waic\", file = \"fits/b05.03\")\n\nWaffle_data %>% \n  mutate(outlier = if_else(Location %in% c(\"Idaho\", \"Maine\"), \"YES\", \"NO\")) %>% \n  ggplot(aes(x = a, y = d, colour = outlier)) +\n  geom_point(shape = 1.5, stroke = 2, size = 3) +\n  scale_colour_manual(values = c(met.brewer(\"Cassatt1\")[2], met.brewer(\"Cassatt1\")[6])) +\n  geom_text_repel(aes(label = Location),\n                  data = Waffle_data %>% filter(Location == c(\"Idaho\", \"Maine\")), size = 5, colour = \"black\", family = \"Courier\", seed = 438, box.padding = 0.5) +\n  labs(x = \"Age at marriage (std)\",\n       y = \"Divorce rate (std)\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nUsing PSIS we can find which data points are outliers\n\n\nCode\nloo(b5.3)\n\n\n\nComputed from 4000 by 50 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo    -63.8  6.4\np_loo         4.7  1.9\nlooic       127.7 12.8\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     49    98.0%   730       \n (0.5, 0.7]   (ok)        1     2.0%   187       \n   (0.7, 1]   (bad)       0     0.0%   <NA>      \n   (1, Inf)   (very bad)  0     0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nOk so we have one point that is having a large effect on the posterior. Which is it?\n\n\nCode\nloo(b5.3) %>% \n  pareto_k_ids(threshold = 0.5)\n\n\n[1] 13\n\n\nPoint 13, this means row 13 in the tibble. Lets find out which location that corresponds to.\n\n\nCode\nWaffle_data %>% \n  as_tibble() %>% \n  slice(13) %>% \n  select(Location:Loc)\n\n\n# A tibble: 1 × 2\n  Location Loc  \n  <fct>    <fct>\n1 Idaho    ID   \n\n\nWe can quantify the influence using the PSIS k statistic or the WAIC penalty term (the effective number of parameters)\nLet’s plot them both\n\n\nCode\ntibble(pareto_k = b5.3$criteria$loo$diagnostics$pareto_k,\n       p_waic   = b5.3$criteria$waic$pointwise[, \"p_waic\"],\n       Loc      = pull(Waffle_data, Loc)) %>% \n  \n  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n  geom_vline(xintercept = .5, linetype = 2, color = \"black\", alpha = 1/2) +\n  geom_point(aes(shape = Loc == \"ID\"), size = 3, stroke = 2, alpha = 0.8) +\n  geom_text(data = . %>% filter(p_waic > 0.5),\n            aes(x = pareto_k - 0.03, label = Loc),\n            hjust = 1) +\n  scale_color_manual(values = met.brewer(\"Cassatt1\")[c(2, 5)]) +\n  scale_shape_manual(values = c(1, 19)) +\n  labs(subtitle = \"Gaussian model (b5.3)\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nWe can combat outliers with mixture models\n\nUse multiple gaussian distributions with different variances: produces the student t distribution\nWe have a distribution with thicker tails, meaning that it expects more extreme points than your standard gaussian model\n\n\n\nCode\ntibble(x = seq(from = -6, to = 6, by = 0.01)) %>% \n  mutate(Gaussian    = dnorm(x),\n         `Student-t` = dstudent_t(df = 2, x)) %>% \n  pivot_longer(-x,\n               names_to = \"likelihood\",\n               values_to = \"density\") %>% \n  mutate(`minus log density` = -log(density)) %>% \n  pivot_longer(contains(\"density\")) %>% \n  \n  ggplot(aes(x = x, y = value, group = likelihood, color = likelihood)) +\n  geom_line() +\n  scale_color_manual(values = c(met.brewer(\"Cassatt1\")[2], \"black\")) +\n  ylim(0, NA) +\n  labs(x = \"value\", y = NULL) +\n  theme(strip.background = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\") +\n  theme_classic()\n\n\n\n\n\nLet’s try it out and retest those PSIS, WAIC values.\nThis is an easy model to fit in brms\n\n\nCode\nb5.3t <- \n  brm(data = Waffle_data, \n      family = student,\n      bf(d ~ 1 + m + a, nu = 2),\n      prior = c(prior(normal(0, 0.2), class = Intercept),\n                prior(normal(0, 0.5), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 5,\n      file = \"fits/b05.03t\")\n\nb5.3t <- add_criterion(b5.3t, criterion = c(\"loo\", \"waic\"))\n\n# plot\n\ntibble(pareto_k = b5.3t$criteria$loo$diagnostics$pareto_k,\n       p_waic   = b5.3t$criteria$waic$pointwise[, \"p_waic\"],\n       Loc      = pull(Waffle_data, Loc)) %>% \n  \n  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n  geom_point(aes(shape = Loc == \"ID\"), size = 3, stroke = 2) +\n  geom_text(data = . %>% filter(Loc %in% c(\"ID\", \"ME\")),\n            aes(x = pareto_k - 0.01, label = Loc),\n            hjust = 1) +\n  scale_color_manual(values = met.brewer(\"Cassatt1\")[c(2, 5)]) +\n  scale_shape_manual(values = c(1, 19)) +\n  labs(subtitle = \"Student-t model (b5.3t)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n\n\n\n\nAnd a final comparison\n\n\nCode\nbind_rows(posterior_samples(b5.3),\n          posterior_samples(b5.3t)) %>% \n  mutate(fit = rep(c(\"Gaussian (b5.3)\", \"Student-t (b5.3t)\"), each = n() / 2)) %>% \n  pivot_longer(b_Intercept:sigma) %>% \n  mutate(name = factor(name,\n                       levels = c(\"b_Intercept\", \"b_a\", \"b_m\", \"sigma\"),\n                       labels = c(\"alpha\", \"beta[a]\", \"beta[m]\", \"sigma\"))) %>% \n  \n  ggplot(aes(x = value, y = fit, color = fit)) +\n  stat_pointinterval(.width = .95, size = 1) +\n  scale_color_manual(values = c(met.brewer(\"Cassatt1\")[1], \"black\")) +\n  labs(x = \"posterior\", y = NULL) +\n  theme_classic() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        legend.position = \"none\",\n        strip.background = element_rect(fill = alpha(met.brewer(\"Cassatt1\")[2], 1/4), color = \"transparent\"),\n        strip.text = element_text(size = 12)) +\n  facet_wrap(~ name, ncol = 1, labeller = label_parsed)\n\n\n\n\n\nOverall, the coefficients are very similar between the two models.\nRobust regression summary\n\nLots of unobserved heterogeneity that comes from a mixture of Gaussian distributions\nThicker tails makes the model less surprised/influenced by extreme observations\nDegrees of freedom determines how much weight to put in the tails. No good way of doing this other than trial and error…\nStudent-t regression is a nice default (perhaps better than Gaussian)"
  }
]